<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<!DOCTYPE html>
<head>
<!-- Favicon for various operating systems -->
<link rel="apple-touch-icon" sizes="180x180" href="./favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="./favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="./favicon/favicon-16x16.png">
<!-- <link rel="manifest" href="./favicon/site.webmanifest"> -->
<link rel="mask-icon" href="./favicon/safari-pinned-tab.svg" color="#603cba">
<link rel="shortcut icon" href="./favicon/favicon.ico">
<meta name="msapplication-TileColor" content="#603cba">
<meta name="msapplication-config" content="./favicon/browserconfig.xml">
<meta name="theme-color" content="#382121">
</head>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">danStich</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../../index.html">Home</a>
</li>
<li>
  <a href="../../teaching.html">Teaching</a>
</li>
<li>
  <a href="../../research.html">Research</a>
</li>
<li>
  <a href="../../cv.html">Curriculum vitae</a>
</li>
<li>
  <a href="../../courseWebsites.html">Course websites</a>
</li>
<li>
  <a href="../../contact.html">Contact</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<style>
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}

.column {
    float: left;
    padding: 15px;
}

.clearfix::after {
    content: "";
    clear: both;
    display: table;
}

.content {
    width: 75%;
}

</style>
<p><br></p>
<div id="glm-count-models" class="section level1">
<h1>GLM: count models</h1>
<p><br></p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p><br></p>
<p>This week in class, we introduced GLM formulations of count models using either Poisson or negative binomial error distributions. Along with these ideas, we discussed concepts relating to transformations for exploratory variables and some basic model diagnostics for the GLM in these situations. During lab this week, we will continue to expand on these ideas, and we will reinforce concepts related to model selection and validation that we have talked about in other sections.</p>
<p>For our purposes this week, we will continue to delve further into count models that use Poisson and negative binomial error distributions. We will make predictions from these models, talk about standardized effects, and continue to learn more about writing methods and results for these approaches. We will work with the same models we formulated under the frequentist mode of interest earlier this week, but now we will cast these analyses in a Bayesian framework.</p>
<p><br></p>
</div>
<div id="the-data" class="section level2">
<h2>The data</h2>
<p><br></p>
<p>We are going to continue working with the crabs data this week in lab. In case you need a refresher on those data, here it is:</p>
<p>These data represent the number of satellite male crabs per female (rows) horseshoe crab in relation to a number of characteristics of the females, including their color, spine condition, carapace width, and mass (g).</p>
<p>The full citation for the paper that this data set is based on: H. J. Brockmann. 1996. Satellite male groups in horseshoe crabs, <em>Limulus polyphemus</em>. Ethology 102:1-21. <a href="doi:10.1111/j.1439-0310.1996.tb01099.x" class="uri">doi:10.1111/j.1439-0310.1996.tb01099.x</a></p>
<p>Read in the data, and recall that we want to be working with <code>color</code>, and <code>spine</code> as categorical variables since these are both classification schemes and not actually numeric data.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Read in the data</span>
  crabs =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;http://employees.oneonta.edu/stichds/data/crabs.csv&#39;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>)

<span class="co"># Take a look at the first few rows of the dataframe</span>
  <span class="kw">head</span>(crabs, <span class="dv">10</span>)</code></pre></div>
<pre><code>##    color spine width mass satellites
## 1      2     3  28.3 3.05          8
## 2      3     3  26.0 2.60          4
## 3      3     3  25.6 2.15          0
## 4      4     2  21.0 1.85          0
## 5      2     3  29.0 3.00          1
## 6      1     2  25.0 2.30          3
## 7      4     3  26.2 1.30          0
## 8      2     3  24.9 2.10          0
## 9      2     1  25.7 2.00          8
## 10     2     3  27.5 3.15          6</code></pre>
<p><br></p>
<p>Did you make sure that the variables <code>color</code> and <code>spine</code> were stored as either factors or character variables? If not, go ahead and change their data type now.</p>
<p><br></p>
<p><br></p>
<p>Don’t forget to make sure you did it correctly:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Have a look at the data structure</span>
  <span class="kw">str</span>(crabs)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    173 obs. of  5 variables:
##  $ color     : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 2 3 3 4 2 1 4 2 2 2 ...
##  $ spine     : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 3 3 3 2 3 2 3 3 1 3 ...
##  $ width     : num  28.3 26 25.6 21 29 25 26.2 24.9 25.7 27.5 ...
##  $ mass      : num  3.05 2.6 2.15 1.85 3 2.3 1.3 2.1 2 3.15 ...
##  $ satellites: int  8 4 0 0 1 3 0 0 8 6 ...</code></pre>
<p><br></p>
</div>
<div id="a-worked-example" class="section level2">
<h2>A worked example</h2>
<p><br></p>
<p>Let’s get started! Our first challenge here is to fit a count model describing the variability in satellite males as some function of female characteristics. Let’s start by building a model that uses only <code>color</code> as an explanatory variable. We can liken this to doing and ANOVA to test the effects of <code>color</code> on <code>satellites</code>. So, if you are following along in your text book, we are somewhere between the models presented in chapters 13 and those presented in chapter 15.</p>
<p>We can formulate this model in a pretty straightforward way by using the same conventions that we have used during the past couple of weeks for Bayesian models written in the BUGS language. Here, we will estimate separate intercepts for each of the color variants in the data set:</p>
<p>If we wanted to write out the likelihood and the model by hand, it might look something like what follows. We are assuming that the i<sup>th</sup> response (number of satellites) within each j<sup>th</sup> level of the variable <code>color</code> is drawn from a Poisson distribution with a mean and variance of <span class="math inline">\(\lambda\)</span>. We start by writing the likelihood for the i<sup>th</sup> value of <span class="math inline">\(N\)</span> with respect to the i<sup>th</sup> value of <span class="math inline">\(\lambda\)</span>:</p>
<p><span class="math display">\[N_i = Poisson(\lambda_{i})\]</span></p>
<p>We can then specify the linear predictor on the link scale (log in this case, because we are modeling count data) to specify the model. In this case, we don’t specify any betas because we are estimating group-specific intercepts.</p>
<p><span class="math display">\[log(\lambda_{i}) = \alpha_{i_j}\]</span></p>
<p>This approach accomplishes the same thing as dummy coding color and writing <span class="math inline">\(log(\lambda_i) = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + \beta_3 \cdot x_3\)</span>, but the former easier to write out in notation and in the BUGS language. It also makes it easier to communicate means. If we want relative effect sizes, we can just calculate them by doing simple algebra with the posteriors.</p>
<p>And, finally, we assume that the i<sup>th</sup> value of is drawn from a normal distribution on the link scale, dependent upon the color (j) of the female crab in the i<sup>th</sup> observation. We specify the prior on alpha for each j<sup>th</sup> color as a diffuse, normal distribution with a mean (<span class="math inline">\(\mu\)</span>) of zero and a variance (<span class="math inline">\(\sigma^2\)</span>) of 100 (in BUGS language <span class="math inline">\(\tau = 0.01\)</span>).</p>
<p><span class="math display">\[\alpha_{j} = Normal(0, 0.01)\]</span></p>
<blockquote>
<p>Specify* the model for JAGS</p>
</blockquote>
<p>Here, we add in a few calculations so we can assess model fit (see Chapter 13.5-13.5.1 in Kery [2010] for explanations). Essentially, what we are going to do here is calculate the predicted and fitted residuals so we can make (1) a residual plot and (2) a predicted vs fitted plot of sorts.</p>
<p>Note that we put this code inside the same loop as the likelihood because it is also looping over each observation in the data. This decision has nothing to do with statistics and everything to do with programming. A sinlge, loop containing multiple tasks in JAGS is evaluated faster than multiple smaller loops containing only a single task. We could just as easily write the code for fit assessment in a separate loop that iterates over operations. It would just be a little slower to run.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modelstring=<span class="st">&quot;                 </span>
<span class="st">  model {                       </span>

<span class="st">    # Likelihood and residuals</span>
<span class="st">      for(i in 1:nobs){  </span>
<span class="st">        # Likelihood</span>
<span class="st">          N[i] ~ dpois(lambda[i])</span>
<span class="st">          log(lambda[i]) &lt;- alpha[color[i]] + beta[color[i]]*mass[i]</span>
<span class="st">          </span>
<span class="st">        # Calculations for assessing model fit</span>
<span class="st">          # Pearson residuals (aka standardized residuals)</span>
<span class="st">            pResid[i] &lt;- N[i] - lambda[i]/sqrt(lambda[i])</span>

<span class="st">          # Make a new set of observations </span>
<span class="st">          # that conform perfectly to the posterior distribution</span>
<span class="st">            N.new[i] ~ dpois(lambda[i])</span>

<span class="st">          # Pearson residuals for the data that conform perfectly</span>
<span class="st">            pResidN[i] &lt;- N.new[i] - lambda[i]/sqrt(lambda[i])</span>

<span class="st">          # Calculate squared residuals for model fit and predictive fit</span>
<span class="st">            D[i] &lt;- pow(pResid[i], 2)</span>
<span class="st">            Dnew[i] &lt;- pow(pResidN[i], 2)</span>
<span class="st">      }</span>

<span class="st">    # Priors</span>
<span class="st">      for(j in 1:ncolors){</span>
<span class="st">        alpha[j] ~ dnorm(0, 0.01)</span>
<span class="st">        beta[j] ~ dnorm(0, 0.01) </span>
<span class="st">      }</span>

<span class="st">    # Discrepency measures</span>
<span class="st">      fitted &lt;- sum(D[])</span>
<span class="st">      predicted &lt;- sum(Dnew[])</span>
<span class="st">  }                             </span>
<span class="st">    </span>
<span class="st">&quot;</span>

<span class="kw">writeLines</span>(modelstring,<span class="dt">con=</span><span class="st">&quot;crabsModel.txt&quot;</span>)</code></pre></div>
<p><br></p>
<p>Model <em>specification</em> is the declaration of the mathematical formula that we use to relate our expalanatory variables to the response.</p>
<blockquote>
<p>Create a list to hold the data for the model</p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Package the data in a list</span>
  crabs.data =<span class="st"> </span><span class="kw">list</span>(
    <span class="dt">nobs =</span> <span class="kw">nrow</span>(crabs),
    <span class="dt">N =</span> crabs<span class="op">$</span>satellites,
    <span class="dt">color =</span> crabs<span class="op">$</span>color,
    <span class="dt">ncolors =</span> <span class="kw">length</span>(<span class="kw">unique</span>(crabs<span class="op">$</span>color)),
    <span class="dt">mass=</span>crabs<span class="op">$</span>mass
  )</code></pre></div>
<blockquote>
<p>Tell JAGS which parameters we wish to monitor</p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Parameters monitored</span>
  parameters &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;alpha&#39;</span>, <span class="st">&#39;beta&#39;</span>, <span class="st">&#39;pResid&#39;</span>, <span class="st">&#39;fitted&#39;</span>, <span class="st">&#39;predicted&#39;</span>)</code></pre></div>
<blockquote>
<p>Define a function that creates a list of initial values for each parameter</p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Initial values</span>
  inits &lt;-<span class="st"> </span><span class="cf">function</span>(){<span class="kw">list</span>(
    <span class="dt">alpha=</span><span class="kw">rnorm</span>(<span class="kw">length</span>(<span class="kw">unique</span>(crabs<span class="op">$</span>color)),<span class="dv">0</span>,<span class="dv">1</span>),
    <span class="dt">beta=</span><span class="kw">rnorm</span>(<span class="kw">length</span>(<span class="kw">unique</span>(crabs<span class="op">$</span>color)),<span class="dv">0</span>,<span class="dv">1</span>)    
    )}</code></pre></div>
<blockquote>
<p>Define objects to hold MCMC settings for model calibration</p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># MCMC settings</span>
  ni &lt;-<span class="st"> </span><span class="dv">7500</span>       <span class="co"># Number of draws from posterior (for each chain)</span>
  nt &lt;-<span class="st"> </span><span class="dv">3</span>          <span class="co"># Thinning rate</span>
  nb &lt;-<span class="st"> </span><span class="dv">2500</span>       <span class="co"># Number of draws to discard as burn-in</span>
  nc &lt;-<span class="st"> </span><span class="dv">3</span>          <span class="co"># Number of chains</span></code></pre></div>
<blockquote>
<p>Fit (calibrate) the model using JAGS</p>
</blockquote>
<p><em>Calibration</em> is fitting the model to actual data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Call jags and run the model</span>
  crabs.model &lt;-<span class="st"> </span><span class="kw">jags</span>(crabs.data, <span class="dt">inits=</span>inits, parameters, <span class="st">&quot;crabsModel.txt&quot;</span>,
    <span class="dt">n.chains =</span> nc, <span class="dt">n.thin =</span> nt, <span class="dt">n.iter =</span> ni, <span class="dt">n.burnin =</span> nb,
    <span class="dt">working.directory =</span> <span class="kw">getwd</span>())</code></pre></div>
<blockquote>
<p>Print the model summary</p>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print a summary of the model</span>
  <span class="kw">print</span>(crabs.model, <span class="dt">digits =</span> <span class="dv">2</span>) </code></pre></div>
<blockquote>
<p>Making sense of the results</p>
</blockquote>
<p>Unfortunately, storing the residuals also means that we get the print out of the estimates in the model summary…But, we can summarize the model nicely by just printing out the components we really care about. Here, I select only the columns for mean, sd, 95% CRI, Rhat, and n.eff (columns 1-3, and 7-9 in the summary table) for our <span class="math inline">\(\alpha\)</span> parameters and the deviance estimate (rows 1-5 of the summary table).</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get the relevant information from the model summary</span>
  modSummary =<span class="st"> </span>crabs.model<span class="op">$</span>BUGSoutput<span class="op">$</span>summary[<span class="dv">1</span><span class="op">:</span><span class="dv">9</span>, <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">7</span><span class="op">:</span><span class="dv">9</span>)]

<span class="co"># Print the model summary</span>
  <span class="kw">print</span>(modSummary, <span class="dt">digits=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>##             mean     sd    2.5%   97.5% Rhat n.eff
## alpha[1]   1.916  1.018  -0.077   3.856  1.0  3700
## alpha[2]  -0.053  0.226  -0.492   0.376  1.0   930
## alpha[3]  -1.312  0.469  -2.232  -0.389  1.0  1400
## alpha[4]  -1.470  0.733  -2.943  -0.066  1.0  4800
## beta[1]   -0.205  0.387  -0.961   0.551  1.0  3900
## beta[2]    0.474  0.079   0.322   0.623  1.0   870
## beta[3]    0.875  0.180   0.519   1.221  1.0  2000
## beta[4]    0.960  0.304   0.358   1.567  1.0  5000
## deviance 905.030 30.860 898.554 913.676  1.1  5000</code></pre>
<p><br></p>
<p>And, if we need it, we can also get the DIC like this:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print out DIC value</span>
  crabs.model<span class="op">$</span>BUGSoutput<span class="op">$</span>DIC</code></pre></div>
<pre><code>## [1] 1381.331</code></pre>
<blockquote>
<p>Model diagnostics</p>
</blockquote>
<p>At a quick glance, the summary table seems to indicate that our chains converged nicely and it looks like we got a good number of samples from our posteriors. I’m not making traceplots 178 parameters, but knock yourself out by all means.</p>
<p>Now that we have our wits back about us, let’s actually take a look at some residual plots!</p>
<p>Make a plot of the residuals.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(crabs.model<span class="op">$</span>BUGSoutput<span class="op">$</span>mean<span class="op">$</span>pResid, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">ylab=</span><span class="st">&#39;Pearson residuals&#39;</span>, <span class="dt">xlab =</span> <span class="st">&#39;Observation&#39;</span>, <span class="dt">pch=</span><span class="dv">21</span>, <span class="dt">bg=</span><span class="st">&#39;gray87&#39;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&#39;red&#39;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</code></pre></div>
<p><img src="hw09_glm_counts_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p><br></p>
<p>Uh, oh…this isn’t looking good! As you can see, most of our residuals are above the zero line here, and even though we are not working with a normally distributed response, the residuals should still have a mean of zero and an approximately normal distribution because we are using a link function that <em>should</em> make them normal.</p>
<p>We can keep plowing through this and take a look at our discrepancy measure to see whether or not the model provides an adequate fit:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(crabs.model<span class="op">$</span>BUGSoutput<span class="op">$</span>sims.list<span class="op">$</span>fitted,
     crabs.model<span class="op">$</span>BUGSoutput<span class="op">$</span>sims.list<span class="op">$</span>predicted,
     <span class="dt">xlab=</span><span class="st">&#39;Discrepancy measure for observed&#39;</span>,
     <span class="dt">ylab=</span><span class="st">&#39;Discrepancy measure for predicted&#39;</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="st">&#39;black&#39;</span>)</code></pre></div>
<p><img src="hw09_glm_counts_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p><br></p>
<p>Also not looking great (see Ch. 13.5.1 for explanation). We can find out pretty quickly, but the key here is that we are looking for a [Bayesian p-] value of about 0.50 (see Ch. 8.4.2 in Kery [2010]).</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(crabs.model<span class="op">$</span>BUGSoutput<span class="op">$</span>sims.list<span class="op">$</span>fitted<span class="op">&gt;</span>
<span class="st">     </span>crabs.model<span class="op">$</span>BUGSoutput<span class="op">$</span>sims.list<span class="op">$</span>predicted)</code></pre></div>
<pre><code>## [1] 0.9994001</code></pre>
<p><br></p>
<p>Crap. We are in big trouble here.</p>
<p>Okay, well it’s actually not that bad…We just need to look at the assumptions that we’ve made (and that we would have known were erroneous if we’d done our exploratory analysis!). The fact is, our data are over-dispersed (the variance is bigger than the mean), so these data are not actually Poisson.</p>
<p><br></p>
</div>
<div id="accounting-for-overdispersion" class="section level2">
<h2>Accounting for overdispersion</h2>
<p><br></p>
<p>We basically have two options here. We could go full-on negative binomial, or we could just address the overdispersion directly by adding a parameter to the model! Let’s do the latter. Basically, the only change here is that we are going to add a parameter, called <span class="math inline">\(\epsilon\)</span> to our model so we can incorporate the overdispersion directly.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modelstring=<span class="st">&quot;                 </span>
<span class="st">  model {                       </span>
<span class="st">    # Likelihood</span>
<span class="st">      for(i in 1:nobs){          </span>
<span class="st">        N[i] ~ dpois(lambda[i])   </span>
<span class="st">        log(lambda[i]) &lt;- alpha[color[i]] + beta[color[i]]*mass[i] + eps[i]</span>
<span class="st">        eps[i] ~ dnorm(0, tau)</span>

<span class="st">        # Calculations for assessing model fit</span>
<span class="st">          pResid[i] &lt;- N[i] - lambda[i]/sqrt(lambda[i])</span>
<span class="st">          N.new[i] ~ dpois(lambda[i])</span>
<span class="st">          pResidN[i] &lt;- N.new[i] - lambda[i]/sqrt(lambda[i])</span>
<span class="st">          D[i] &lt;- pow(pResid[i], 2)</span>
<span class="st">          Dnew[i] &lt;- pow(pResidN[i], 2)</span>
<span class="st">      }                         </span>

<span class="st">    # Priors</span>
<span class="st">        sigma ~ dunif(0, 10)</span>
<span class="st">        tau &lt;- 1/(sigma * sigma)</span>

<span class="st">      for(i in 1:ncolors){</span>
<span class="st">        alpha[i] ~ dnorm(0,0.001) </span>
<span class="st">        beta[i] ~ dnorm(0,0.001)  </span>

<span class="st">      }</span>

<span class="st">    # Sums of squared residuals</span>
<span class="st">      fitted &lt;- sum(D[])</span>
<span class="st">      predicted &lt;- sum(Dnew[])</span>
<span class="st">  }                             </span>
<span class="st">&quot;</span>                               
<span class="kw">writeLines</span>(modelstring,<span class="dt">con=</span><span class="st">&quot;crabsMod2.txt&quot;</span>)

<span class="co"># Package the data in a list</span>
  crabs.data =<span class="st"> </span><span class="kw">list</span>(
    <span class="dt">nobs =</span> <span class="kw">nrow</span>(crabs),
    <span class="dt">N =</span> crabs<span class="op">$</span>satellites,
    <span class="dt">color =</span> crabs<span class="op">$</span>color,
    <span class="dt">ncolors =</span> <span class="kw">length</span>(<span class="kw">unique</span>(crabs<span class="op">$</span>color)),
    <span class="dt">mass=</span>crabs<span class="op">$</span>mass
  )

<span class="co"># Parameters monitored</span>
  parameters &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;alpha&#39;</span>, <span class="st">&#39;beta&#39;</span>, <span class="st">&#39;eps&#39;</span>, <span class="st">&#39;pResid&#39;</span>, <span class="st">&#39;fitted&#39;</span>, <span class="st">&#39;predicted&#39;</span>)

<span class="co"># Initial values</span>
  inits &lt;-<span class="st"> </span><span class="cf">function</span>(){<span class="kw">list</span>(
    <span class="dt">alpha=</span><span class="kw">rnorm</span>(<span class="kw">length</span>(<span class="kw">unique</span>(crabs<span class="op">$</span>color)),<span class="dv">0</span>,<span class="dv">1</span>),
    <span class="dt">beta=</span><span class="kw">rnorm</span>(<span class="kw">length</span>(<span class="kw">unique</span>(crabs<span class="op">$</span>color)),<span class="dv">0</span>,<span class="dv">1</span>),
    <span class="dt">sigma=</span><span class="kw">runif</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">10</span>)
    )}

<span class="co"># MCMC settings</span>
  ni &lt;-<span class="st"> </span><span class="dv">7500</span>      <span class="co"># Number of draws from posterior (for each chain)</span>
  nt &lt;-<span class="st"> </span><span class="dv">3</span>          <span class="co"># Thinning rate</span>
  nb &lt;-<span class="st"> </span><span class="dv">2500</span>       <span class="co"># Number of draws to discard as burn-in</span>
  nc &lt;-<span class="st"> </span><span class="dv">3</span>          <span class="co"># Number of chains</span>

<span class="co"># Call jags and run the model</span>
  crabs.model2 &lt;-<span class="st"> </span><span class="kw">jags</span>(crabs.data, <span class="dt">inits=</span>inits, parameters, <span class="st">&quot;crabsMod2.txt&quot;</span>,
    <span class="dt">n.chains =</span> nc, <span class="dt">n.thin =</span> nt, <span class="dt">n.iter =</span> ni, <span class="dt">n.burnin =</span> nb,
    <span class="dt">working.directory =</span> <span class="kw">getwd</span>())</code></pre></div>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 173
##    Unobserved stochastic nodes: 355
##    Total graph size: 4289
## 
## Initializing model
## 
## 
  |                                                        
  |                                                  |   0%
  |                                                        
  |+++                                               |   6%
  |                                                        
  |++++++                                            |  12%
  |                                                        
  |+++++++++                                         |  18%
  |                                                        
  |++++++++++++                                      |  24%
  |                                                        
  |+++++++++++++++                                   |  30%
  |                                                        
  |++++++++++++++++++                                |  36%
  |                                                        
  |+++++++++++++++++++++                             |  42%
  |                                                        
  |++++++++++++++++++++++++                          |  48%
  |                                                        
  |+++++++++++++++++++++++++++                       |  54%
  |                                                        
  |++++++++++++++++++++++++++++++                    |  60%
  |                                                        
  |+++++++++++++++++++++++++++++++++                 |  66%
  |                                                        
  |++++++++++++++++++++++++++++++++++++              |  72%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++           |  78%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++        |  84%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++++++++  |  96%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
## 
  |                                                        
  |                                                  |   0%
  |                                                        
  |**                                                |   3%
  |                                                        
  |***                                               |   6%
  |                                                        
  |****                                              |   9%
  |                                                        
  |******                                            |  12%
  |                                                        
  |********                                          |  15%
  |                                                        
  |*********                                         |  18%
  |                                                        
  |**********                                        |  21%
  |                                                        
  |************                                      |  24%
  |                                                        
  |**************                                    |  27%
  |                                                        
  |***************                                   |  30%
  |                                                        
  |****************                                  |  33%
  |                                                        
  |******************                                |  36%
  |                                                        
  |********************                              |  39%
  |                                                        
  |*********************                             |  42%
  |                                                        
  |**********************                            |  45%
  |                                                        
  |************************                          |  48%
  |                                                        
  |**************************                        |  51%
  |                                                        
  |***************************                       |  54%
  |                                                        
  |****************************                      |  57%
  |                                                        
  |******************************                    |  60%
  |                                                        
  |********************************                  |  63%
  |                                                        
  |*********************************                 |  66%
  |                                                        
  |**********************************                |  69%
  |                                                        
  |************************************              |  72%
  |                                                        
  |**************************************            |  75%
  |                                                        
  |***************************************           |  78%
  |                                                        
  |****************************************          |  81%
  |                                                        
  |******************************************        |  84%
  |                                                        
  |********************************************      |  87%
  |                                                        
  |*********************************************     |  90%
  |                                                        
  |**********************************************    |  93%
  |                                                        
  |************************************************  |  96%
  |                                                        
  |**************************************************|  99%
  |                                                        
  |**************************************************| 100%</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print an abbreviated summary of the model</span>
<span class="co"># Get the relevant information from the model summary</span>
  modSummary =<span class="st"> </span>crabs.model2<span class="op">$</span>BUGSoutput<span class="op">$</span>summary[<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">9</span>), <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dv">7</span><span class="op">:</span><span class="dv">9</span>)]

<span class="co"># Print the model summary</span>
  <span class="kw">print</span>(modSummary, <span class="dt">digits=</span><span class="dv">2</span>) </code></pre></div>
<pre><code>##            mean    sd   2.5%   97.5% Rhat n.eff
## alpha[1]   1.35  2.45  -3.49   6.192    1  2000
## alpha[2]  -1.19  0.61  -2.43  -0.024    1  5000
## alpha[3]  -3.36  1.06  -5.54  -1.333    1  1600
## alpha[4]  -1.70  1.55  -4.87   1.319    1   800
## beta[1]   -0.11  0.92  -1.94   1.703    1  2500
## beta[2]    0.75  0.22   0.32   1.206    1  5000
## beta[3]    1.51  0.42   0.71   2.351    1  1600
## beta[4]    0.71  0.69  -0.64   2.080    1   890
## deviance 551.96 19.82 514.96 592.121    1  1000</code></pre>
<p><br></p>
<p>We can see that our values of our parameter estimates have changed a bit here, but how does that translate to model fit?</p>
<p>Go ahead and calculate the Bayesian p-value again. It looks like it is a little closer to 0.50, but maybe not good enough to pass the straight-face test…</p>
<p><br></p>
<pre><code>## [1] 0.2157568</code></pre>
<p><br></p>
<p>So, why is this? It’s because we don’t just have a little overdispersion…we have quite a bit. Let’s have a look at our predictions versus observed number of satellites for each color. From these plots, we can see that our measure of central tendancy is still ballpark, it’s just that our variance is still grossly under-estimated.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>){
<span class="kw">plot</span>(<span class="kw">density</span>(crabs<span class="op">$</span>satellites[crabs<span class="op">$</span>color<span class="op">==</span>i]), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>,
     <span class="dt">bty=</span><span class="st">&#39;l&#39;</span>, <span class="dt">main=</span><span class="st">&#39;&#39;</span>, <span class="dt">xlab =</span> <span class="st">&#39;Number of satellites&#39;</span>)
<span class="kw">par</span>(<span class="dt">new=</span><span class="ot">TRUE</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(<span class="kw">exp</span>(crabs.model2<span class="op">$</span>BUGSoutput<span class="op">$</span>sims.list<span class="op">$</span>alpha[,i] <span class="op">+</span>
<span class="st">         </span>crabs.model2<span class="op">$</span>BUGSoutput<span class="op">$</span>sims.list<span class="op">$</span>beta[,i]<span class="op">*</span><span class="kw">mean</span>(crabs<span class="op">$</span>satellites[crabs<span class="op">$</span>color<span class="op">==</span>i]))),
     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dt">main=</span><span class="st">&#39;&#39;</span>, <span class="dt">xlab=</span><span class="st">&#39;&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;&#39;</span>, <span class="dt">col=</span><span class="st">&#39;red&#39;</span>, <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>,
     <span class="dt">bty=</span><span class="st">&#39;l&#39;</span>)
}</code></pre></div>
<p><img src="hw09_glm_counts_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p><br></p>
</div>
<div id="shifting-gears" class="section level2">
<h2>Shifting gears</h2>
<p><br></p>
<p>This example is probably a good case for switching over to the negative binomial distribution. The code to demonstrate this is below. This is the formulation you will work with for the rest of lab.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modelstring=<span class="st">&quot;                 </span>
<span class="st">  model {                       </span>
<span class="st">    # Likelihood</span>
<span class="st">      for(i in 1:nobs){    </span>
<span class="st">        N[i] ~ dnegbin(p[i], r[color[i]]) </span>
<span class="st">        logit(p[i]) &lt;- alpha[color[i]] + beta[color[i]]*mass[i]</span>
<span class="st">      }</span>

<span class="st">    # Priors</span>
<span class="st">      for(i in 1:ncolors){</span>
<span class="st">        alpha[i] ~ dnorm(0,0.0001) </span>
<span class="st">        beta[i] ~ dnorm(0,0.0001)</span>
<span class="st">        r[i] ~ dgamma(0.0001, 0.0001)T(0,3)</span>
<span class="st">      }</span>
<span class="st">  }                             </span>
<span class="st">&quot;</span>                               
<span class="kw">writeLines</span>(modelstring,<span class="dt">con=</span><span class="st">&quot;crabsMod3.txt&quot;</span>)

<span class="co"># Package the data in a list</span>
  crabs.data =<span class="st"> </span><span class="kw">list</span>(
    <span class="dt">nobs =</span> <span class="kw">nrow</span>(crabs),
    <span class="dt">N =</span> crabs<span class="op">$</span>satellites,
    <span class="dt">color =</span> crabs<span class="op">$</span>color,
    <span class="dt">ncolors =</span> <span class="kw">length</span>(<span class="kw">unique</span>(crabs<span class="op">$</span>color)),
    <span class="dt">mass=</span>crabs<span class="op">$</span>mass
  )

<span class="co"># Parameters monitored</span>
  parameters &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;alpha&#39;</span>, <span class="st">&#39;beta&#39;</span>, <span class="st">&#39;r&#39;</span>)

<span class="co"># Initial values</span>
  inits &lt;-<span class="st"> </span><span class="cf">function</span>(){<span class="kw">list</span>(
    <span class="dt">alpha=</span><span class="kw">rnorm</span>(<span class="kw">length</span>(<span class="kw">unique</span>(crabs<span class="op">$</span>color)),<span class="dv">0</span>,<span class="dv">1</span>),
    <span class="dt">beta=</span><span class="kw">rnorm</span>(<span class="kw">length</span>(<span class="kw">unique</span>(crabs<span class="op">$</span>color)),<span class="dv">0</span>,<span class="dv">1</span>),
    <span class="dt">r=</span><span class="kw">runif</span>(<span class="kw">length</span>(<span class="kw">unique</span>(crabs<span class="op">$</span>color)),<span class="dv">0</span>,<span class="dv">1</span>)
    )}

<span class="co"># MCMC settings</span>
  ni &lt;-<span class="st"> </span><span class="dv">4500</span>       <span class="co"># Number of draws from posterior (for each chain)</span>
  nt &lt;-<span class="st"> </span><span class="dv">10</span>         <span class="co"># Thinning rate</span>
  nb &lt;-<span class="st"> </span><span class="dv">500</span>        <span class="co"># Number of draws to discard as burn-in</span>
  nc &lt;-<span class="st"> </span><span class="dv">3</span>          <span class="co"># Number of chains</span>

<span class="co"># Call jags and run the model</span>
  crabs.model3 &lt;-<span class="st"> </span><span class="kw">jags</span>(crabs.data, <span class="dt">inits=</span>inits, parameters, <span class="st">&quot;crabsMod3.txt&quot;</span>,
    <span class="dt">n.chains =</span> nc, <span class="dt">n.thin =</span> nt, <span class="dt">n.iter =</span> ni, <span class="dt">n.burnin =</span> nb,
    <span class="dt">working.directory =</span> <span class="kw">getwd</span>())</code></pre></div>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 173
##    Unobserved stochastic nodes: 12
##    Total graph size: 865
## 
## Initializing model
## 
## 
  |                                                        
  |                                                  |   0%
  |                                                        
  |+++++++++                                         |  18%
  |                                                        
  |++++++++++++++++++                                |  36%
  |                                                        
  |+++++++++++++++++++++++++++                       |  54%
  |                                                        
  |++++++++++++++++++++++++++++++++++++              |  72%
  |                                                        
  |+++++++++++++++++++++++++++++++++++++++++++++     |  90%
  |                                                        
  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%
## 
  |                                                        
  |                                                  |   0%
  |                                                        
  |*                                                 |   2%
  |                                                        
  |**                                                |   4%
  |                                                        
  |***                                               |   7%
  |                                                        
  |****                                              |   9%
  |                                                        
  |******                                            |  11%
  |                                                        
  |*******                                           |  14%
  |                                                        
  |********                                          |  16%
  |                                                        
  |*********                                         |  18%
  |                                                        
  |**********                                        |  20%
  |                                                        
  |***********                                       |  22%
  |                                                        
  |************                                      |  25%
  |                                                        
  |**************                                    |  27%
  |                                                        
  |***************                                   |  29%
  |                                                        
  |****************                                  |  32%
  |                                                        
  |*****************                                 |  34%
  |                                                        
  |******************                                |  36%
  |                                                        
  |*******************                               |  38%
  |                                                        
  |********************                              |  40%
  |                                                        
  |*********************                             |  43%
  |                                                        
  |**********************                            |  45%
  |                                                        
  |************************                          |  47%
  |                                                        
  |*************************                         |  50%
  |                                                        
  |**************************                        |  52%
  |                                                        
  |***************************                       |  54%
  |                                                        
  |****************************                      |  56%
  |                                                        
  |*****************************                     |  58%
  |                                                        
  |******************************                    |  61%
  |                                                        
  |********************************                  |  63%
  |                                                        
  |*********************************                 |  65%
  |                                                        
  |**********************************                |  68%
  |                                                        
  |***********************************               |  70%
  |                                                        
  |************************************              |  72%
  |                                                        
  |*************************************             |  74%
  |                                                        
  |**************************************            |  76%
  |                                                        
  |***************************************           |  79%
  |                                                        
  |****************************************          |  81%
  |                                                        
  |******************************************        |  83%
  |                                                        
  |*******************************************       |  86%
  |                                                        
  |********************************************      |  88%
  |                                                        
  |*********************************************     |  90%
  |                                                        
  |**********************************************    |  92%
  |                                                        
  |***********************************************   |  94%
  |                                                        
  |************************************************  |  97%
  |                                                        
  |**************************************************|  99%
  |                                                        
  |**************************************************| 100%</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print the model summary</span>
  <span class="kw">print</span>(crabs.model3, <span class="dt">digits=</span><span class="dv">2</span>) </code></pre></div>
<pre><code>## Inference for Bugs model at &quot;crabsMod3.txt&quot;, fit using jags,
##  3 chains, each with 4500 iterations (first 500 discarded), n.thin = 10
##  n.sims = 1200 iterations saved
##          mu.vect sd.vect   2.5%    25%    50%    75%  97.5% Rhat n.eff
## alpha[1]   -1.91    2.35  -7.40  -3.21  -1.77  -0.36   2.24 1.00   900
## alpha[2]    0.70    0.54  -0.40   0.34   0.70   1.05   1.75 1.01   800
## alpha[3]    1.66    1.01  -0.37   1.01   1.70   2.36   3.53 1.01   190
## alpha[4]   -0.37    3.47  -7.69  -2.39  -0.25   1.85   6.11 1.01  1200
## beta[1]     0.19    0.87  -1.63  -0.35   0.21   0.69   2.00 1.01   400
## beta[2]    -0.62    0.19  -0.99  -0.74  -0.62  -0.50  -0.28 1.01  1200
## beta[3]    -1.04    0.39  -1.86  -1.28  -1.03  -0.78  -0.33 1.01   210
## beta[4]    -1.28    1.62  -4.83  -2.23  -1.15  -0.23   1.61 1.00  1200
## r[1]        1.25    0.68   0.29   0.73   1.11   1.67   2.80 1.01   380
## r[2]        1.33    0.34   0.81   1.09   1.28   1.52   2.20 1.00  1200
## r[3]        1.04    0.45   0.42   0.71   0.97   1.29   2.16 1.00   800
## r[4]        0.15    0.08   0.04   0.09   0.13   0.19   0.35 1.00  1200
## deviance  741.98    5.11 733.67 738.04 741.48 745.03 753.34 1.00  1200
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 13.1 and DIC = 755.0
## DIC is an estimate of expected predictive error (lower deviance is better).</code></pre>
<p><br></p>
<p>If we want to see how the estimates from this model compare to the observed data, we can plot the posterior densities for r over the top of histograms for each color. All in all this is looking pretty darn good.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))  
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(<span class="kw">unique</span>(crabs<span class="op">$</span>color))){  
<span class="kw">hist</span>(crabs<span class="op">$</span>satellites[crabs<span class="op">$</span>color<span class="op">==</span>i], <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>,
     <span class="dt">bty=</span><span class="st">&#39;l&#39;</span>, <span class="dt">main=</span><span class="st">&#39;&#39;</span>, <span class="dt">xlab =</span> <span class="st">&#39;Number of satellites&#39;</span>, <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>)  
<span class="kw">par</span>(<span class="dt">new=</span><span class="ot">TRUE</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(<span class="kw">exp</span>(crabs.model3<span class="op">$</span>BUGSoutput<span class="op">$</span>sims.list<span class="op">$</span>r[ ,i])),
     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">20</span>), <span class="dt">main=</span><span class="st">&#39;&#39;</span>, <span class="dt">xlab=</span><span class="st">&#39;&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;&#39;</span>, <span class="dt">col=</span><span class="st">&#39;red&#39;</span>, <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">bty=</span><span class="st">&#39;l&#39;</span>)
}</code></pre></div>
<p><img src="hw09_glm_counts_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p><br></p>
</div>
<div id="your-mission" class="section level2">
<h2>Your mission</h2>
<p><br></p>
<p>Just as in the past couple of weeks, your assignment for this lab is to write a <em>brief</em> methods section and a <em>brief</em> results section for the exercises you have been working on. <strong>You may use either the maximum likelihood estimation procedure</strong> covered in lecture this week, <strong>or the Bayesian approach</strong> detailed in the lab exercise. Just make sure you pick one or the other and stick to it in your analysis and your writing.</p>
<p>In addition to these tasks, I would like you to <strong>add one additional explanatory variable</strong> to the model. I would like you to add <code>width</code> as a continuous covariate.</p>
<p>Then, so we can compare the relative effect sizes of <code>width</code> and <code>mass</code>, I want you to standardize the covariates prior to including them in the model.</p>
<p>Remember that this is the same as calculating a whole bunch of z-scores. You can achieve this in R like this (for example):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Standardize mass variable:</span>
  crabs<span class="op">$</span>smass =<span class="st"> </span><span class="kw">as.vector</span>(<span class="kw">scale</span>(crabs<span class="op">$</span>mass))</code></pre></div>
<p><br></p>
<p>For your results, you can present covariates on the standardized scale, and back-transform based on our discussions earlier this week to show effects on the real scale of the variable.</p>
<p><br></p>
<p><br></p>
<p><br></p>
</div>
</div>

<!DOCTYPE html>
<p>Copyright &copy; 2017 Dan Stich. All rights reserved.</p>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
