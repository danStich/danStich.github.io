<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>07_introToBayes.utf8.md</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<!DOCTYPE html>
<head>
<!-- Favicon for various operating systems -->
<link rel="apple-touch-icon" sizes="180x180" href="./favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="./favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="./favicon/favicon-16x16.png">
<!-- <link rel="manifest" href="./favicon/site.webmanifest"> -->
<link rel="mask-icon" href="./favicon/safari-pinned-tab.svg" color="#603cba">
<link rel="shortcut icon" href="./favicon/favicon.ico">
<meta name="msapplication-TileColor" content="#603cba">
<meta name="msapplication-config" content="./favicon/browserconfig.xml">
<meta name="theme-color" content="#382121">
</head>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { background-color: #f8f8f8; }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="..\..\styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">BIOL 678</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Teaching
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../../teaching.html">Teaching home</a>
    </li>
    <li>
      <a href="../../classes/BIOL217/index.html">BIOL 217</a>
    </li>
    <li>
      <a href="../../classes/BIOL678/index.html">BIOL 678</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../research.html">Research</a>
</li>
<li>
  <a href="../../studentProjects.html">Current Students</a>
</li>
<li>
  <a href="../../publications.html">Publications</a>
</li>
<li>
  <a href="../../cv.html">CV</a>
</li>
<li>
  <a href="../../contact.html">Contact</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/danStich">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<div id="introduction-to-bayesian-inference" class="section level1">
<h1>Introduction to Bayesian inference</h1>
<p><img id="multi" src="../../images/chickens.jpg" alt=""></p>
<h2 id="multi">
Introduction
</h2>
<p>This week we are going to introduce a couple of new concepts including Bayesian inference and the generalized linear model. My rationale for introducing these concepts together is 1) both of these tools rely heavily upon knowledge of probability distributions that we began to build earlier this semester, and 2) we are beginning to move into the realm where we will become more reliant upon methods of estimation other than ordinary least squares (OLS), which is what we have been using for the past two or three weeks for fitting linear models (ANOVA, linear regression, and ANCOVA).</p>
<p>During the next several weeks, I am hoping that we can dive in to the basic underpinnings of the Bayesian framework for scientific inference, along with maximum likelihood estimation as we move through more complex extensions of linear models. This framework has been around for a long time, but only recently has it become really broadly applicable to common analytical problems. There are some fundamental (and philosophical) differences between the use of maximum likelihood estimation (aka “frequentist”) methods and the application of Bayes theorem for answering statistical questions. I am hoping we can touch on some of these during our discussions and show the real, practical strengths of maximum likelihood and Bayesian inference that might actually make you want to use one or the other for certain applications.</p>
<p>For better or worse, there is no way we can possibly do a comprehensive treatment of Bayesian statistics within the context of a survey-style, applied statistics course such as this. We can, however, set you up with some basic tools so you can apply Bayesian inference to commonly encountered situations (t-tests, lm, GLM, LMM, and GLMM) that should allow you to explore these concepts on your own in the future. To achieve this, I would like for us to cover some Bayesian analogs to some of the frequentist tests that we have considered so far this semester. For this reason, we will explore maximum likelihood and Bayesian methods side by side while learning new techniques during the next couple of weeks.</p>
<div id="intro-to-bayes-theorem" class="section level2">
<h2>Intro to Bayes Theorem</h2>
<p>Bayes Theorem provides the mathematical framework through which Bayesian inference is applied to quantitative questions. The theorem, in the most basic sense, helps us understand the probability of some event given conditions we suspect are related to that event. This elegant theorem was first derived by Reverend Thomas Bayes in the 1700s. The equation was later reworked by Pierre-Simon Laplace to yield the modern version that we use today:</p>
<p><span class="math display">\[P(A|B) = \frac{P(B|A)\cdot P(A)}{P(B)}\]</span></p>
<p>which is read “the probability of A given B is equal to the probability of B given A times the probability of A, divided by the probability of B”.</p>
<p>A common example application of this theorem that may be of interest to biology students is the probability of having cancer at a given age (thanks Wikipedia!). The example goes something like this:</p>
<p>Suppose we want to know the probability that an individual of age 50 has cancer. We might only have information about the marginal probability of their having cancer given that they are a human (let’s say 1%), and the probability of their being 50 years old given population distribution of ages (let’s just say 3% for the sake of demonstration). To calculate the conditional probability that an individual who is 50 years old has cancer, we would need one more piece of information: the probability that a person with cancer is 50 years old. The calculation is relatively straightforward if we know this number exactly, and we can derive an exact conditional probability. For this example, let’s start by assuming the probability that people who have cancer are 50 years old is 2%. Now we can calculate the conditional probability that a person who is 50 years old has cancer:</p>
<p>Start with the theorem:</p>
<p><span class="math display">\[P(Cancer|Age 50) = \frac{P(Age 50|Cancer)\cdot P(Cancer)}{P(Age 50)}\]</span></p>
<p>Through substitution we get: <span class="math display">\[P(Cancer|Age 50) = \frac{(0.02\cdot 0.01)}{0.03}\]</span></p>
<p>And now we can solve for the conditional probability: <span class="math display">\[P(Cancer|Age 50) = 0.00667\]</span></p>
<p>Hopefully, you can see from this example and earlier learning about rules of probability why this is such an important theorem in statistical probability theory. In fact, this is one of the reasons for the recent resurgence in the use of Bayes Theorem in applied Bayesian inference in biological and ecological statistics during the past couple of decades. But, if it’s so useful, then why has it only been heavily used recently?</p>
<p>It will quickly become obvious to you that the answer is “computers”. To demonstrate this, let’s consider a slightly more complex example.</p>
<p>Now, let’s assume that we don’t actually have exact information about the probability that individuals who have cancer are age 50. Let’s instead assume that we only have a rough idea about that probability, and that we can put a loose distribution around it. Now, there is no longer an exact mathematical solution for Bayes Theorem but rather an infinite number of potential solutions. If we know the distribution, then we can discretize the distribution and find a finite number of solutions to the theorem that would allow us to describe the probability of our event of interest most of the the time. However, there are cases for which this problem becomes intractable without the use of computers, even when the distribution is known. You can imagine that this becomes considerably more complex if the form of the distribution is not known with certainty.</p>
<p>For the sake of demonstration, let’s examine how this procedure changes if we have some uncertainty in one of our probabilities on the right-hand side of the equation:</p>
<p>Looking back at our example, we had: <span class="math display">\[P(Cancer|Age 50) = \frac{P(Age 50|Cancer)\cdot P(Cancer)}{P(Age 50)}\]</span></p>
<p>And, by substitution: <span class="math display">\[P(Cancer|Age 50) = \frac{0.02\cdot 0.01}{0.03}\]</span></p>
<p>Let’s now assume that the proportion of people with cancer who are also 50 years old is now an unknown quantity that is drawn from a beta distribution that can be described by parameters <span class="math inline">\(\alpha\)</span> = 200, and <span class="math inline">\(\beta\)</span> = 10,000:</p>
<p><span class="math display">\[P(Cancer|Age 50) = \frac{Beta(200, 10000) \cdot 0.01}{0.03}\]</span></p>
<p>Realistically, this is a <em>much</em> tighter distribution than we would use for a situation like this, but we’ll get to that during coming weeks. The point is that even a little uncertainty makes the process more complicated than if exact values are <em>known</em>.</p>
<p>We can make this distribution in R:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" title="1"><span class="co"># Simulate 10000 random values for p(Cancer|50)</span></a>
<a class="sourceLine" id="cb1-2" title="2">  p_<span class="dv">50</span>_cancer =<span class="st"> </span><span class="kw">rbeta</span>(<span class="fl">1e4</span>, <span class="dv">200</span>, <span class="fl">1e4</span>)</a></code></pre></div>
<p>We can also look at this distribution:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" title="1"><span class="co"># Make a histogram of p(Age50 | Cancer)</span></a>
<a class="sourceLine" id="cb2-2" title="2">  <span class="kw">hist</span>(p_<span class="dv">50</span>_cancer,</a>
<a class="sourceLine" id="cb2-3" title="3">       <span class="dt">col=</span><span class="st">&quot;gray87&quot;</span>,</a>
<a class="sourceLine" id="cb2-4" title="4">       <span class="dt">main=</span><span class="st">&#39;&#39;</span>,</a>
<a class="sourceLine" id="cb2-5" title="5">       <span class="dt">xlab =</span> <span class="st">&#39;P(50 | Cancer)&#39;</span></a>
<a class="sourceLine" id="cb2-6" title="6">       )</a></code></pre></div>
<p><img src="07_introToBayes_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Now, we have a working probability distribution for the probability of being age 50 given that one has cancer. We can plug this into Bayes Theorem and construct a probability distribution to solve for the inverse: the probability of having cancer given that the patient is age 50 <span class="math inline">\((P(Cancer|Age 50))\)</span>. Let’s do it in R.</p>
<p>First, let’s define our other marginal probabilities as variables in R.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" title="1">p_cancer &lt;-<span class="st"> </span><span class="fl">0.01</span> <span class="co"># Marginal probability of having cancer</span></a>
<a class="sourceLine" id="cb3-2" title="2">p_<span class="dv">50</span> &lt;-<span class="st"> </span><span class="fl">0.03</span>     <span class="co"># Marginal probability of being age 50</span></a></code></pre></div>
<p>Now we can solve the theorem for our finite number of observations:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" title="1">p_cancer_<span class="dv">50</span> &lt;-<span class="st"> </span>(p_<span class="dv">50</span>_cancer<span class="op">*</span>p_cancer)<span class="op">/</span>p_<span class="dv">50</span></a></code></pre></div>
<p>We can calculate descriptive statistics for the conditional probability so that we can describe the distribution.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" title="1"><span class="co"># Using a mean and standard deviation</span></a>
<a class="sourceLine" id="cb5-2" title="2">  <span class="kw">mean</span>(p_cancer_<span class="dv">50</span>)</a>
<a class="sourceLine" id="cb5-3" title="3">[<span class="dv">1</span>] <span class="fl">0.006531673</span></a>
<a class="sourceLine" id="cb5-4" title="4">  <span class="kw">sd</span>(p_cancer_<span class="dv">50</span>)</a>
<a class="sourceLine" id="cb5-5" title="5">[<span class="dv">1</span>] <span class="fl">0.0004564573</span></a></code></pre></div>
<p>We can calculate quantiles (95% CRI). Note that in Bayesian inference, we are going to call these ‘credible intervals’ (CRI) or ‘high density intervals’ (HDI) depending on assumptions related to normality, but they are functionally the same thing as ‘confidence intervals’.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" title="1"><span class="kw">quantile</span>(p_cancer_<span class="dv">50</span>, <span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.50</span>, <span class="fl">0.975</span>))</a>
<a class="sourceLine" id="cb6-2" title="2">       <span class="fl">2.5</span><span class="op">%         50%</span><span class="st">       </span><span class="fl">97.5</span>% </a>
<a class="sourceLine" id="cb6-3" title="3"><span class="fl">0.005674685</span> <span class="fl">0.006520993</span> <span class="fl">0.007460190</span> </a></code></pre></div>
<p>We can also look at the actual distribution:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" title="1"><span class="kw">hist</span>(p_cancer_<span class="dv">50</span>,</a>
<a class="sourceLine" id="cb7-2" title="2">     <span class="dt">col=</span><span class="st">&quot;gray87&quot;</span>,</a>
<a class="sourceLine" id="cb7-3" title="3">     <span class="dt">main=</span><span class="st">&#39;&#39;</span>,</a>
<a class="sourceLine" id="cb7-4" title="4">     <span class="dt">xlab =</span> <span class="st">&#39;P(Cancer | 50)&#39;</span></a>
<a class="sourceLine" id="cb7-5" title="5">     )</a></code></pre></div>
<p><img src="07_introToBayes_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Finally, let’s say now that there is uncertainty in all of our probabilities of interest:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" title="1"><span class="co"># First, let&#39;s define each of our</span></a>
<a class="sourceLine" id="cb8-2" title="2"><span class="co"># probabilities as variables in R</span></a>
<a class="sourceLine" id="cb8-3" title="3">  p_<span class="dv">50</span>_cancer &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="fl">1e4</span>, <span class="dv">20</span>, <span class="fl">1e3</span>)</a>
<a class="sourceLine" id="cb8-4" title="4">  p_cancer &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="fl">1e4</span>, <span class="dv">10</span>, <span class="fl">1e3</span>)</a>
<a class="sourceLine" id="cb8-5" title="5">  p_<span class="dv">50</span> &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="fl">1e4</span>, <span class="dv">30</span>, <span class="fl">1e3</span>)</a></code></pre></div>
<p>We can continue as before. We solve the theorem for our finite number of observations:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" title="1">  p_cancer_<span class="dv">50</span> &lt;-<span class="st"> </span>(p_<span class="dv">50</span>_cancer<span class="op">*</span>p_cancer)<span class="op">/</span>p_<span class="dv">50</span></a></code></pre></div>
<p>We calculate descriptive statistics for the conditional probability to describe the probability using a mean and standard deviation.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" title="1">  <span class="kw">mean</span>(p_cancer_<span class="dv">50</span>)</a>
<a class="sourceLine" id="cb10-2" title="2">[<span class="dv">1</span>] <span class="fl">0.006882778</span></a>
<a class="sourceLine" id="cb10-3" title="3">  <span class="kw">sd</span>(p_cancer_<span class="dv">50</span>)</a>
<a class="sourceLine" id="cb10-4" title="4">[<span class="dv">1</span>] <span class="fl">0.003034055</span></a></code></pre></div>
<p>Quantiles (95% CRI and median):</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" title="1"><span class="kw">quantile</span>(p_cancer_<span class="dv">50</span>, <span class="dt">probs=</span><span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.50</span>, <span class="fl">0.975</span>))</a>
<a class="sourceLine" id="cb11-2" title="2">       <span class="fl">2.5</span><span class="op">%         50%</span><span class="st">       </span><span class="fl">97.5</span>% </a>
<a class="sourceLine" id="cb11-3" title="3"><span class="fl">0.002598754</span> <span class="fl">0.006353177</span> <span class="fl">0.014370260</span> </a></code></pre></div>
<p>And, have a look at our new distribution:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" title="1"><span class="kw">hist</span>(p_cancer_<span class="dv">50</span>,</a>
<a class="sourceLine" id="cb12-2" title="2">     <span class="dt">col=</span><span class="st">&quot;gray87&quot;</span>,</a>
<a class="sourceLine" id="cb12-3" title="3">     <span class="dt">main=</span><span class="st">&#39;&#39;</span>,</a>
<a class="sourceLine" id="cb12-4" title="4">     <span class="dt">xlab =</span> <span class="st">&#39;P(Cancer | 50)&#39;</span></a>
<a class="sourceLine" id="cb12-5" title="5">     )</a></code></pre></div>
<p><img src="07_introToBayes_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Now you can see why computers are starting to matter, and we are not even doing Bayesian inference yet. Why is that?</p>
<p>Because we still haven’t collected any data!!! All that we have done here is state some basic mathematical representations of our beliefs about certain conditional and marginal probabilities of specific events. Those beliefs may be useful representations, or they may be way off!</p>
<p>This mathematical formulation of our ‘beliefs’ is known as the prior distribution for the probability of the event of interest. Want to know more about the prior distribution you say? How convenient…</p>
</div>
<div id="the-prior" class="section level2">
<h2>The prior</h2>
<p>The prior distribution, in simple terms, is the information that we have at our disposal <em>prior</em> to collecting any further data. Those data might come in the form of hard numbers collected through a pilot laboratory or field study, or it might come from some logical process based on deductive reasoning. We will discuss the fact that the latter form of knowledge is probably more more appropriate when applied to how we form our prior on the probability of the event under investigation.</p>
<p>One of the really attractive aspects of Bayesian inference is that we have the ability to incorporate information from prior experiences into our statistical models. The advantage of this is that we can start off with some information, and then collect new information to update our beliefs. Why would we want to do this? Glad you ask:</p>
<p><strong>1. Improved inference</strong> The use of an informed prior allows us to improve the precision of our parameter estimates by narrowing the scope of credible values for our estimates. A strong prior can keep our estimates within a certain range of realistic values, for example.</p>
<p><strong>2. Adaptive research</strong> Incorporation of information from previous studies allows us to continually update our scientific beliefs in an iterative way. If we have data from a similar study, or a previous year of study, then we can use that to inform inference moving forward to obtain more accurate and precise estimates of the parameters of interest.</p>
<p><strong>3. Hypothesis testing</strong> We can use specific formulations of the prior distribution to test specific hypotheses about the probability of the event of interest. For example, if we suspect that the probability of a patient surviving an operation is strongly related to the age (or some other pre-existing condition) of the patient, then you could test different formulations of the prior and see which one results in a better model fit to your data.</p>
<p><strong>4. Incorporation of uncertainty</strong> If there is a lot of uncertainty in the event of interest, we can set a very “weak” or “diffuse” prior. When the prior is extremely diffuse (e.g. a uniform or “flat” prior), then Bayesian inference will yield results that are essentially identical to the results we expect to get from maximum likelihood estimation. The only noticeable difference may be increased precision in some cases depending on the estimator that we use.</p>
<p>So let’s go through a couple examples of what a prior distribution actually looks like.</p>
<div id="the-hospital-example" class="section level3">
<h3>The hospital example</h3>
<p>For this example, let’s assume that we are interested in the survival of a hospital patient. Survival will be denoted as a ‘success’, or 1, and mortality as a ‘failure’, or ‘0’. In this sense, we are dealing with a binomial outcome. But, remember, we can always represent binomial outcomes on the probability scale…right?</p>
<p>In this case, let’s say that we are assuming <em>a priori</em> that survival might be due to random chance, or that it might be influenced by some factor of interest (we’ll use “hospital” in the example below).</p>
<p>There are multiple approaches that we could take to formulating a prior distribution for this case.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" title="1"><span class="co"># A uniform distribution that indicates we</span></a>
<a class="sourceLine" id="cb13-2" title="2"><span class="co"># have no knowledge about how survival</span></a>
<a class="sourceLine" id="cb13-3" title="3"><span class="co"># varies between hospitals</span></a>
<a class="sourceLine" id="cb13-4" title="4">  p_hosp_flat &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="fl">1e4</span>, <span class="dv">0</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb13-5" title="5"></a>
<a class="sourceLine" id="cb13-6" title="6"><span class="co"># A diffuse prior that indicates we think</span></a>
<a class="sourceLine" id="cb13-7" title="7"><span class="co"># survival is the same between hospitals but</span></a>
<a class="sourceLine" id="cb13-8" title="8"><span class="co"># we don&#39;t want to make too strong a statement</span></a>
<a class="sourceLine" id="cb13-9" title="9">  p_hosp_dif &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="fl">1e4</span>, <span class="dv">5</span>, <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb13-10" title="10">  </a>
<a class="sourceLine" id="cb13-11" title="11"><span class="co"># A peaked (strong) prior that indicates we </span></a>
<a class="sourceLine" id="cb13-12" title="12"><span class="co"># are relatively certain ahead of time that</span></a>
<a class="sourceLine" id="cb13-13" title="13"><span class="co"># survival is the same in both hospitals</span></a>
<a class="sourceLine" id="cb13-14" title="14">  p_hosp_str &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="fl">1e4</span>, <span class="dv">500</span>, <span class="dv">500</span>)</a>
<a class="sourceLine" id="cb13-15" title="15">  </a>
<a class="sourceLine" id="cb13-16" title="16"><span class="co"># A strong prior that indicates we think</span></a>
<a class="sourceLine" id="cb13-17" title="17"><span class="co"># survival is substantially different</span></a>
<a class="sourceLine" id="cb13-18" title="18"><span class="co"># between hospitals</span></a>
<a class="sourceLine" id="cb13-19" title="19">  p_hosp_bi &lt;-<span class="st"> </span><span class="kw">rbeta</span>(<span class="fl">1e4</span>, <span class="fl">.5</span>, <span class="fl">.5</span>)</a></code></pre></div>
<p>We can look at these to compare them.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" title="1">  <span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb14-2" title="2">  <span class="kw">hist</span>(p_hosp_flat, <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>, <span class="dt">main=</span><span class="st">&#39;&#39;</span>)</a>
<a class="sourceLine" id="cb14-3" title="3">  <span class="kw">hist</span>(p_hosp_dif, <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>, <span class="dt">main=</span><span class="st">&#39;&#39;</span>)</a>
<a class="sourceLine" id="cb14-4" title="4">  <span class="kw">hist</span>(p_hosp_str, <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>, <span class="dt">main=</span><span class="st">&#39;&#39;</span>)</a>
<a class="sourceLine" id="cb14-5" title="5">  <span class="kw">hist</span>(p_hosp_bi, <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>, <span class="dt">main=</span><span class="st">&#39;&#39;</span>)</a></code></pre></div>
<p><img src="07_introToBayes_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>You can see how different each of these priors is from one another. Hopefully, you are also starting to think about the different kinds of hypotheses that we might test with these different priors. In this case, the issue that we are always trying to address is whether or not survival is due only to random chance. This could be likened to asking whether or not a coin that we toss is a fair coin, or if it has some bias (say for example that it is more likely to land heads up because it is heavier on one side).</p>
<p>Now that we have a prior distribution for our event of interest, we can go out into the world and collect some data about that event. We will then use those data to formulate a ‘posterior’ distribution that reflects some combination of our prior distribution and the data that we have collected. This process is commonly referred to as ‘updating’ our prior beliefs about the event of interest, and is the foundation that underlies Bayesian inference. How we get from the prior to a posterior is wholly dependent on the tools we use to obtain the solution to Bayes theorem, but most often this occurs throught the use of Markov-chain Monte Carlo simulation. This approach allows us to work through Bayes theorem one set of values at a time to obtain a heuristic, simulation-based approach to solving for conditional probabilities. We will discuss this in some (but not too much!) detail as we move forward.</p>
<p>Before moving on, it is important to note that our prior beliefs can potentially have a strong influence on the posterior distribution. This has been the subject of much controversy in the application of Bayesian inference to modern scientific study. Our goal in using prior information should not be to dominate the posterior with our prior beliefs in biological and ecological studies. It should be to support improved inference through the inclusion of relevant information, and can be extremely helpful for situations in which data are somewhat deficient. Ultimately, we want our data to dominate the form of the posterior distributions that result from our analyses. If this is not the case, then we need to be explicit about this and should almost always attempt to evaluate the “sensitivity” of our posterior distribution(s) to the prior(s) we have chosen. This is a field of ongoing development in specific disciplines, and I encourage you to seek out the relevant literature on the matter if you intend to use Bayesian inference in your own research.</p>
</div>
</div>
<div id="the-posterior" class="section level2">
<h2>The posterior</h2>
<p>Estimation of the posterior predictive distribution is really the hallmark of Bayesian inference, and is the crux of any applied analysis that uses this framework to test hypotheses in biology and ecology. The posterior predictive distribution (more commonly called the ‘posterior’) is the estimated probability distribution of unobserved events conditional on some set of observations related to that event.</p>
<p>The posterior distribution can be estimated as the product of our prior distribution and the corresponding likelihood by re-arranging Bayes theorem:</p>
<p><span class="math display">\[posterior \propto prior \cdot likelihood\]</span></p>
<p>You’ll recall from our early adventures into probability distributions and the moments of those distributions that every probability distribution that we work with has a ‘likelihood function’. So, if our prior distribution was a beta distribution (let’s say for a binomial response), then we would use the likelihood for the Beta distribution in the theorem above. For a given observation, we could calculate the value of the likelihood for that observation and solve the theorem exactly…sometimes…but not usually in practice. In order to do this, we need to know the form of the posterior ahead of time. There are a relatively limited set of conditions that allow us to know this ahead of time. Namely, we need to know that we are working with a ‘conjugate’ prior. Without getting too far afield, these are prior distributions for which the form of the posterior is defined and known because it is from the same family as the prior. In our example above, the beta distribution is a conjugate prior for the binomial likelihood, so the solution to Bayes theorem is, relatively speaking, trivial compared to other situations. This is the primary reason that computers are needed to implement modern Bayesian inference. Most of the time we do not know the form of the posterior distribution ahead of time, so we use MCMC sampling to approximate the distribution numerically.</p>
<p>In the simplest sense, the posterior distribution is a combination of our prior distribution and our data. So if you remember nothing else in the explanation, remember that.</p>
</div>
<div id="a-worked-example" class="section level2">
<h2>A worked example</h2>
<p>In this section, we will apply Bayes theorem to update our prior beliefs about the probability of some event of interest in order to demonstrate how we can estimate a conditional probability for that event given some data. We will use the example to demonstrate how the prior and our data interact to form the posterior.</p>
<div id="the-data" class="section level3">
<h3>The data</h3>
<p>We start by reading in data.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" title="1"><span class="co"># Read in the data</span></a>
<a class="sourceLine" id="cb15-2" title="2">  birds &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;farmdata.csv&#39;</span>)</a>
<a class="sourceLine" id="cb15-3" title="3"></a>
<a class="sourceLine" id="cb15-4" title="4"><span class="co"># Look at the data structure</span></a>
<a class="sourceLine" id="cb15-5" title="5">  <span class="kw">str</span>(birds)</a>
<a class="sourceLine" id="cb15-6" title="6"></a>
<a class="sourceLine" id="cb15-7" title="7"><span class="co"># It&#39;s a short data set.</span></a>
<a class="sourceLine" id="cb15-8" title="8"><span class="co"># Let&#39;s just print it to the console</span></a>
<a class="sourceLine" id="cb15-9" title="9">  birds</a></code></pre></div>
<pre><code>   hatched fledged species
1       26      21 chicken
2       16      15 chicken
3       49      40 chicken
4        2       2    duck
5       12      12    duck
6        6       6    duck
7       12      10 chicken
8       26      24 chicken
9        8       8    duck
10       2       2   goose</code></pre>
<p>This data set contains information about chick survival from hatch to adulthood for each cohort of birds on my small-scale poultry farm in 2017. The file contains data on the starting number at hatch, the number of chicks fledged, and the species of bird.</p>
<p>We will use the data set to estimate chick survival from hatch to fledge. There are a number of ways we can do this. In practice, we will probably use Gibbs sampling 99% of the time (haha, Bayesian joke). But we may run into situations in which we can estimate the posterior distribution by hand or in which we need an alternative algorithm for flexibility.</p>
<p>Let’s start simple by estimating the mean expected survival of chicks on my farm across all cohorts and species.</p>
</div>
<div id="model-specification" class="section level3">
<h3>Model specification</h3>
<p>First, we load the <code>R2jags</code> package we will use to run the model.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" title="1"><span class="co">#install.packages(&#39;R2jags&#39;) # Uncomment to run</span></a>
<a class="sourceLine" id="cb17-2" title="2"><span class="kw">library</span>(R2jags)</a></code></pre></div>
<p>Now, we specify the model in the ‘BUGS’ language so we can run our analysis. This first model is heavily commented so you can see what is going on here.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" title="1"><span class="co"># Specify the model string.</span></a>
<a class="sourceLine" id="cb18-2" title="2"><span class="co"># The whole model is wrapped in quotes</span></a>
<a class="sourceLine" id="cb18-3" title="3"><span class="co"># to turn it into one long character string </span></a>
<a class="sourceLine" id="cb18-4" title="4"><span class="co"># that can be written to a text file</span></a>
<a class="sourceLine" id="cb18-5" title="5"></a>
<a class="sourceLine" id="cb18-6" title="6">modelString=<span class="st">&quot;                   # Open the model string</span></a>
<a class="sourceLine" id="cb18-7" title="7"><span class="st">  model {                       # Open the model definition</span></a>
<a class="sourceLine" id="cb18-8" title="8"><span class="st">    # Likelihood</span></a>
<a class="sourceLine" id="cb18-9" title="9"><span class="st">      for(i in 1:nobs{          # For each observation</span></a>
<a class="sourceLine" id="cb18-10" title="10"><span class="st">        y[i] ~ dbin(p, N[i])    # Survival is random draw from binomial with</span></a>
<a class="sourceLine" id="cb18-11" title="11"><span class="st">      }                         # probaility of survival &#39;p&#39; and sample size &#39;N&#39;</span></a>
<a class="sourceLine" id="cb18-12" title="12"></a>
<a class="sourceLine" id="cb18-13" title="13"><span class="st">    # Prior on the probability of success, &#39;p&#39; for binomial density used in </span></a>
<a class="sourceLine" id="cb18-14" title="14"><span class="st">    # likelihood. </span></a>
<a class="sourceLine" id="cb18-15" title="15"></a>
<a class="sourceLine" id="cb18-16" title="16"><span class="st">    # NOTE:  Probability of success is drawn from an uninformative beta </span></a>
<a class="sourceLine" id="cb18-17" title="17"><span class="st">    # distribution with parameters &#39;a&#39; and &#39;b&#39;. This results in a uniform </span></a>
<a class="sourceLine" id="cb18-18" title="18"><span class="st">    # distribution between zero and one. Alternatively, we could specify this</span></a>
<a class="sourceLine" id="cb18-19" title="19"><span class="st">    # as: p ~ dunif(0, 1), but the beta distribution allows more flexibility in</span></a>
<a class="sourceLine" id="cb18-20" title="20"><span class="st">    # case we want to change the prior to be more informative. (i.e. the beta</span></a>
<a class="sourceLine" id="cb18-21" title="21"><span class="st">    # can be used to represent most any distribution between zero and one.)</span></a>
<a class="sourceLine" id="cb18-22" title="22"></a>
<a class="sourceLine" id="cb18-23" title="23"><span class="st">      p ~ dbeta(a, b)           # Prior distribution for p    </span></a>
<a class="sourceLine" id="cb18-24" title="24"><span class="st">      a &lt;- 1                    # When a and b = 1, the prior is uniform (0,1)</span></a>
<a class="sourceLine" id="cb18-25" title="25"><span class="st">      b &lt;- 1</span></a>
<a class="sourceLine" id="cb18-26" title="26"><span class="st">  }                             # Close the model definition</span></a>
<a class="sourceLine" id="cb18-27" title="27"><span class="st">&quot;</span>                               <span class="co"># Close the string for the model file</span></a></code></pre></div>
<p>If we cut this down to the bare minimum text, the model specification would look something like this:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" title="1">modelString=<span class="st">&quot;                 </span></a>
<a class="sourceLine" id="cb19-2" title="2"><span class="st">  model {                       </span></a>
<a class="sourceLine" id="cb19-3" title="3"><span class="st">    # Likelihood</span></a>
<a class="sourceLine" id="cb19-4" title="4"><span class="st">      for(i in 1:nobs){          </span></a>
<a class="sourceLine" id="cb19-5" title="5"><span class="st">        y[i] ~ dbin(p, N[i])      </span></a>
<a class="sourceLine" id="cb19-6" title="6"><span class="st">      }                         </span></a>
<a class="sourceLine" id="cb19-7" title="7"></a>
<a class="sourceLine" id="cb19-8" title="8"><span class="st">    # Priors</span></a>
<a class="sourceLine" id="cb19-9" title="9"><span class="st">      p ~ dbeta(a, b)               </span></a>
<a class="sourceLine" id="cb19-10" title="10"><span class="st">      a &lt;- 1                    </span></a>
<a class="sourceLine" id="cb19-11" title="11"><span class="st">      b &lt;- 1</span></a>
<a class="sourceLine" id="cb19-12" title="12"><span class="st">  }                             </span></a>
<a class="sourceLine" id="cb19-13" title="13"><span class="st">&quot;</span>                               </a></code></pre></div>
<p>This saves the model string as an object in R, that we can use later in place of writing and reading files. If you want to see what it looks like, you can do <code>cat(modelString)</code></p>
<p>Now specify a data set for use in JAGS. We package it in a list called <code>chick.data</code> that contains 1) the number of observations (<code>nobs</code>) so we can loop over each row of data in the likelihood, 2) the number of birds that successfully fledged in each cohort (<code>y</code>), and 3) the starting number of birds hatched in each cohort (<code>N</code>).</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" title="1"><span class="co"># Package the data in a list</span></a>
<a class="sourceLine" id="cb20-2" title="2">  chick.data &lt;-<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb20-3" title="3">    <span class="dt">nobs =</span> <span class="kw">nrow</span>(birds),</a>
<a class="sourceLine" id="cb20-4" title="4">    <span class="dt">y =</span> birds<span class="op">$</span>fledged,</a>
<a class="sourceLine" id="cb20-5" title="5">    <span class="dt">N =</span> birds<span class="op">$</span>hatched</a>
<a class="sourceLine" id="cb20-6" title="6">  )</a></code></pre></div>
<p>Tell JAGS which of the model parameters we would like to monitor. This has to be a character vector containing names of parameters from the model specified above. In this case, the only parameter of interest is <code>p</code>, or probability of survival to fledge.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" title="1"><span class="co"># Parameters monitored</span></a>
<a class="sourceLine" id="cb21-2" title="2">  parameters &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;p&quot;</span>)</a></code></pre></div>
<p>Provide initial values for stochastic nodes. We need to provide initial starting values for each parameter modeled. We use a <em>random</em> starting value for each parameter because we don’t want our model estimates to be influenced by the starting value. In this case, we have only one parameter, <code>p</code>, and we just need a number between one and zero, so we can use the <code>runif</code> function to get one.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" title="1"><span class="co"># Function to provide initial values</span></a>
<a class="sourceLine" id="cb22-2" title="2">  inits &lt;-<span class="st"> </span><span class="cf">function</span>(){<span class="kw">list</span>(<span class="dt">p=</span><span class="kw">runif</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>))}</a></code></pre></div>
<p>Provide the MCMC settings for the model run. Here, we tell JAGS that we want to take 33,000 samples from the posterior distribution (number of iterations, <code>ni</code>). We will only keep every third sample (thinning rate, <code>nt</code>), and we will discard the first 3,000 samples (burnin, <code>nb</code>). We will repeat this process 3 times (number of chains, <code>nc</code>) for a total of 30,000 samples from the posterior.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" title="1">  <span class="co"># MCMC settings</span></a>
<a class="sourceLine" id="cb23-2" title="2">    ni &lt;-<span class="st"> </span><span class="dv">33000</span>     <span class="co"># Number of draws from posterior (for each chain)</span></a>
<a class="sourceLine" id="cb23-3" title="3">    nt &lt;-<span class="st"> </span><span class="dv">3</span>         <span class="co"># Thinning rate</span></a>
<a class="sourceLine" id="cb23-4" title="4">    nb &lt;-<span class="st"> </span><span class="dv">3000</span>      <span class="co"># Number of draws to discard as burn-in</span></a>
<a class="sourceLine" id="cb23-5" title="5">    nc &lt;-<span class="st"> </span><span class="dv">3</span>         <span class="co"># Number of chains</span></a></code></pre></div>
<p>Finally, we call JAGS and run the model:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" title="1"><span class="co"># Call jags and run the model</span></a>
<a class="sourceLine" id="cb24-2" title="2">  chick.model &lt;-<span class="st"> </span><span class="kw">jags</span>(chick.data, <span class="dt">inits=</span>inits, parameters,</a>
<a class="sourceLine" id="cb24-3" title="3">                      <span class="dt">model.file=</span><span class="kw">textConnection</span>(modelString),</a>
<a class="sourceLine" id="cb24-4" title="4">                      <span class="dt">n.chains =</span> nc, <span class="dt">n.thin =</span> nt,</a>
<a class="sourceLine" id="cb24-5" title="5">                      <span class="dt">n.iter =</span> ni, <span class="dt">n.burnin =</span> nb,</a>
<a class="sourceLine" id="cb24-6" title="6">                      <span class="dt">working.directory =</span> <span class="kw">getwd</span>()</a>
<a class="sourceLine" id="cb24-7" title="7">                      )</a></code></pre></div>
<p>Sweet, a progress meter! Now we’re running models!</p>
<p>Print a summary of the model.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" title="1"><span class="kw">print</span>(chick.model)</a>
<a class="sourceLine" id="cb25-2" title="2">Inference <span class="cf">for</span> Bugs model at <span class="st">&quot;4&quot;</span>, fit using jags,</a>
<a class="sourceLine" id="cb25-3" title="3"> <span class="dv">3</span> chains, each with <span class="dv">33000</span> <span class="kw">iterations</span> (first <span class="dv">3000</span> discarded), n.thin =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb25-4" title="4"> n.sims =<span class="st"> </span><span class="dv">30000</span> iterations saved</a>
<a class="sourceLine" id="cb25-5" title="5">         mu.vect sd.vect   <span class="fl">2.5</span><span class="op">%    25%</span><span class="st">    </span><span class="dv">50</span><span class="op">%    75%</span><span class="st">  </span><span class="fl">97.5</span>%  Rhat n.eff</a>
<a class="sourceLine" id="cb25-6" title="6">p          <span class="fl">0.876</span>   <span class="fl">0.026</span>  <span class="fl">0.821</span>  <span class="fl">0.859</span>  <span class="fl">0.878</span>  <span class="fl">0.894</span>  <span class="fl">0.922</span> <span class="fl">1.001</span> <span class="dv">27000</span></a>
<a class="sourceLine" id="cb25-7" title="7">deviance  <span class="fl">26.804</span>   <span class="fl">1.443</span> <span class="fl">25.797</span> <span class="fl">25.897</span> <span class="fl">26.250</span> <span class="fl">27.127</span> <span class="fl">30.892</span> <span class="fl">1.001</span> <span class="dv">15000</span></a>
<a class="sourceLine" id="cb25-8" title="8"></a>
<a class="sourceLine" id="cb25-9" title="9">For each parameter, n.eff is a crude measure of effective sample size,</a>
<a class="sourceLine" id="cb25-10" title="10">and Rhat is the potential scale reduction <span class="kw">factor</span> (at convergence, <span class="dt">Rhat=</span><span class="dv">1</span>).</a>
<a class="sourceLine" id="cb25-11" title="11"></a>
<a class="sourceLine" id="cb25-12" title="12">DIC <span class="kw">info</span> (using the rule, <span class="dt">pD =</span> <span class="kw">var</span>(deviance)<span class="op">/</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb25-13" title="13">pD =<span class="st"> </span><span class="fl">1.0</span> and DIC =<span class="st"> </span><span class="fl">27.8</span></a>
<a class="sourceLine" id="cb25-14" title="14">DIC is an estimate of expected predictive <span class="kw">error</span> (lower deviance is better).</a></code></pre></div>
</div>
<div id="results" class="section level3">
<h3>Results</h3>
<p>The model converges quickly, and we can now say that if I hatch another cohort of chicks, I can expect those chicks to survive to fledge with a probability of 0.88 (95% CRI = 0.82 - 0.92).</p>
<p>If I wanted to determine whether or not some level of survival was credible for planning the future of the flock, I could ask in much the same way as we would conduct a one-sample t-test. For example, can I plan on 95% of the chicks in my next brood surviving to fledge, or should I hatch extra eggs if I want the numbers? In this case, I can see that 0.95 is not included in the credible interval, so if I want to have more than some specific number fledge then I should hatch a few extra eggs. Similarly, if I want 10 new fledglings, then I would need to hatch 10/0.88 = 11 chicks (95% CRI = 11 - 12 chicks).</p>
</div>
<div id="model-summary" class="section level3">
<h3>Model summary</h3>
<p>As for the rest of this summary. For now, we will focus on a few specific things:</p>
<p><strong>1.</strong> Do our estimates make logical sense? Not are they right or wrong, but is our estimate crap or not? If the 95% CRI goes from zero to one, then we probably have some issues with estimation because this means that we learned nothing new from the data (crap).</p>
<p><strong>2.</strong> We need to look at the value of <code>Rhat</code>(<span class="math inline">\(\hat{r}\)</span>, the Gelman-Rubin convergence diagnostic or potential scale-reduction factor). This statistic is a diagnostic that can help us determine whether or not the model has even converged on an estimate for our parameter(s) of interest. It assesses the degree of mixing between the Markov chains that we used to come to our estimates. We will examine some graphical diagnostics of mixing below.</p>
<p><strong>3.</strong> We need to pay attention to <code>n.eff</code>. This quantity is the ‘number of effective samples’ that we have taken from the posterior. As discussed in class, the draws we take from the posterior can be auto-correlated to varying degrees depending on the sampler used. <code>n.eff</code> tells us how many independent samples we can actually consider ourselves to have drawn from the posterior. To have have some degree of confidence in our parameter estimates, we want this to be at least several hundred large. More is better, but these models can take a long time to run as they build in complexity so there is a balance to be struck. If we are running long chains and we still are not achieving large <code>n.eff</code>, it is a pretty good indication that we need to increase our thinning rate or (more likely) consider an alternative model parameterization. If you use this inferential framework in your future, you will also want to look at diagnostics like ACF plots, or auto-correlation function plots that can show you how auto-correlation changes with thinning rate. You may also want to check out <a href="https://mc-stan.org/">Stan</a>, a newer program that reduces autocorrelation between samples using a different approach to MCMC estimation. It is what all of the cool kids are doing right now, but it is a little tougher to code in when you are first learning.</p>
</div>
<div id="diagnostic-plots" class="section level3">
<h3>Diagnostic plots</h3>
<p>In this section, we will examine some visual diagnostics to assess the convergence of our parameter estimates and identify any unusual trends in the Markovian walk for the chains that we used. But, it takes a little while to get there.</p>
<p>Just like everything else, the model is an object with named elements:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" title="1"><span class="kw">names</span>(chick.model)</a>
<a class="sourceLine" id="cb26-2" title="2">[<span class="dv">1</span>] <span class="st">&quot;model&quot;</span>              <span class="st">&quot;BUGSoutput&quot;</span>         <span class="st">&quot;parameters.to.save&quot;</span></a>
<a class="sourceLine" id="cb26-3" title="3">[<span class="dv">4</span>] <span class="st">&quot;model.file&quot;</span>         <span class="st">&quot;n.iter&quot;</span>             <span class="st">&quot;DIC&quot;</span>               </a>
<a class="sourceLine" id="cb26-4" title="4"><span class="kw">names</span>(chick.model<span class="op">$</span>BUGSoutput)</a>
<a class="sourceLine" id="cb26-5" title="5"> [<span class="dv">1</span>] <span class="st">&quot;n.chains&quot;</span>        <span class="st">&quot;n.iter&quot;</span>          <span class="st">&quot;n.burnin&quot;</span>       </a>
<a class="sourceLine" id="cb26-6" title="6"> [<span class="dv">4</span>] <span class="st">&quot;n.thin&quot;</span>          <span class="st">&quot;n.keep&quot;</span>          <span class="st">&quot;n.sims&quot;</span>         </a>
<a class="sourceLine" id="cb26-7" title="7"> [<span class="dv">7</span>] <span class="st">&quot;sims.array&quot;</span>      <span class="st">&quot;sims.list&quot;</span>       <span class="st">&quot;sims.matrix&quot;</span>    </a>
<a class="sourceLine" id="cb26-8" title="8">[<span class="dv">10</span>] <span class="st">&quot;summary&quot;</span>         <span class="st">&quot;mean&quot;</span>            <span class="st">&quot;sd&quot;</span>             </a>
<a class="sourceLine" id="cb26-9" title="9">[<span class="dv">13</span>] <span class="st">&quot;median&quot;</span>          <span class="st">&quot;root.short&quot;</span>      <span class="st">&quot;long.short&quot;</span>     </a>
<a class="sourceLine" id="cb26-10" title="10">[<span class="dv">16</span>] <span class="st">&quot;dimension.short&quot;</span> <span class="st">&quot;indexes.short&quot;</span>   <span class="st">&quot;last.values&quot;</span>    </a>
<a class="sourceLine" id="cb26-11" title="11">[<span class="dv">19</span>] <span class="st">&quot;program&quot;</span>         <span class="st">&quot;model.file&quot;</span>      <span class="st">&quot;isDIC&quot;</span>          </a>
<a class="sourceLine" id="cb26-12" title="12">[<span class="dv">22</span>] <span class="st">&quot;DICbyR&quot;</span>          <span class="st">&quot;pD&quot;</span>              <span class="st">&quot;DIC&quot;</span>            </a>
<a class="sourceLine" id="cb26-13" title="13"><span class="kw">names</span>(chick.model<span class="op">$</span>BUGSoutput<span class="op">$</span>sims.list)</a>
<a class="sourceLine" id="cb26-14" title="14">[<span class="dv">1</span>] <span class="st">&quot;deviance&quot;</span> <span class="st">&quot;p&quot;</span>       </a></code></pre></div>
<p>We access the posterior distribution from the MCMC simulations like this. Later we will look at this more closely.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" title="1">post.chick =<span class="st"> </span>chick.model<span class="op">$</span>BUGSoutput<span class="op">$</span>sims.list<span class="op">$</span>p</a></code></pre></div>
<p>We can also access the individual Markov chains.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" title="1">chain1 =<span class="st"> </span>post.chick[<span class="dv">1</span><span class="op">:</span>(<span class="kw">length</span>(post.chick)<span class="op">/</span>nc)]</a>
<a class="sourceLine" id="cb28-2" title="2">chain2 =<span class="st"> </span>post.chick[((<span class="kw">length</span>(post.chick)<span class="op">/</span>nc)<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>((<span class="kw">length</span>(post.chick)<span class="op">/</span>nc)<span class="op">*</span><span class="dv">2</span>)]</a>
<a class="sourceLine" id="cb28-3" title="3">chain3 =<span class="st"> </span>post.chick[((<span class="kw">length</span>(post.chick)<span class="op">/</span>nc)<span class="op">*</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>((<span class="kw">length</span>(post.chick)<span class="op">/</span>nc)<span class="op">*</span><span class="dv">3</span>)]</a></code></pre></div>
<p>The pattern in figure below is exactly what we want to see. You can see the thorough degree of mixing that is evident based on the overlap among the different chains.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" title="1"><span class="kw">plot</span>(chain1,</a>
<a class="sourceLine" id="cb29-2" title="2">     <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,</a>
<a class="sourceLine" id="cb29-3" title="3">     <span class="dt">type=</span><span class="st">&#39;l&#39;</span>,</a>
<a class="sourceLine" id="cb29-4" title="4">     <span class="dt">ylab=</span> <span class="st">&#39;Probability of survival (p)&#39;</span>,</a>
<a class="sourceLine" id="cb29-5" title="5">     <span class="dt">xlab=</span><span class="st">&#39;iteration&#39;</span>)</a>
<a class="sourceLine" id="cb29-6" title="6"><span class="kw">lines</span>(chain2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb29-7" title="7"><span class="kw">lines</span>(chain3, <span class="dt">col=</span><span class="st">&#39;green&#39;</span>)</a></code></pre></div>
<p><img src="07_introToBayes_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>Of course, we can also use the built-in functions in R, but it is nice to know what is going on first!</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" title="1"><span class="kw">traceplot</span>(chick.model)</a></code></pre></div>
<p><img src="07_introToBayes_files/figure-html/unnamed-chunk-32-1.png" width="672" /><img src="07_introToBayes_files/figure-html/unnamed-chunk-32-2.png" width="672" /></p>
<p><strong>NOTE</strong>: You cannot use the <code>traceplot</code> for model objects produced by WinBUGS, but you can easily get the MCMC plots by other methods (see text book).</p>
</div>
<div id="visualizing-the-results" class="section level3">
<h3>Visualizing the results</h3>
<p>Now that we know things converged nicely, let’s have a look at the results.</p>
<p>Start by plotting a histogram of the posterior predictive distribution.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb31-1" title="1"><span class="co"># Plot the histogram</span></a>
<a class="sourceLine" id="cb31-2" title="2">  <span class="kw">hist</span>(post.chick, <span class="dt">main =</span> <span class="st">&#39;&#39;</span>, <span class="dt">xlab =</span> <span class="st">&#39;Probability of survival (p)&#39;</span>,</a>
<a class="sourceLine" id="cb31-3" title="3">       <span class="dt">ylab =</span> <span class="st">&#39;&#39;</span>, <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>)</a>
<a class="sourceLine" id="cb31-4" title="4"></a>
<a class="sourceLine" id="cb31-5" title="5"><span class="co"># Add a vertical line for the mean</span></a>
<a class="sourceLine" id="cb31-6" title="6">  <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">mean</span>(post.chick), <span class="dt">col=</span><span class="st">&#39;red&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb31-7" title="7">  </a>
<a class="sourceLine" id="cb31-8" title="8"><span class="co"># Add vertical lines for the upper and lower limits to the 95% CRI</span></a>
<a class="sourceLine" id="cb31-9" title="9">  <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">quantile</span>(post.chick, <span class="fl">0.025</span>), <span class="dt">col=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb31-10" title="10">  <span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">quantile</span>(post.chick, <span class="fl">0.975</span>), <span class="dt">col=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<p><img src="07_introToBayes_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>Finally, we can plot a density curve of the posterior.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" title="1"><span class="kw">plot</span>(<span class="kw">density</span>(post.chick),</a>
<a class="sourceLine" id="cb32-2" title="2">     <span class="dt">main=</span><span class="st">&#39;&#39;</span>,</a>
<a class="sourceLine" id="cb32-3" title="3">     <span class="dt">xlab =</span> <span class="st">&#39;Probability of survival (p)&#39;</span>,</a>
<a class="sourceLine" id="cb32-4" title="4">     <span class="dt">col=</span><span class="st">&#39;blue&#39;</span>)</a></code></pre></div>
<p><img src="07_introToBayes_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>There you have it: your first Bayesian data analysis. Needless to say (perhaps?) this was a simple example. However, it demonstrates the flexibility of the method.</p>
<div id="okay-just-one-more-example" class="section level3">
<h3>Okay, just <em>one more</em> example…</h3>
<p>We also could have fit the same ‘model of the mean’ using maximum likelihood estimation to solve this <em>generalized linear model</em> (GLM), by using the <code>glm</code> function in R. Below is a quick teaser, but this will be the focus of our discussions for the next few weeks so don’t get worried if it makes little sense to you right now.</p>
<p>Here, we use a binomial error distribution to model the proportion of birds surviving to fledge as a function of the null model (equivelant to estimating the mean marginal probability of survival). We specify the <code>family</code> as <code>binomial</code>, and to account for the different starting sizes for each cohort, we use the variable <code>hatched</code> to “weight” our observations. The idea is that a bigger sample size gets more weight.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb33-1" title="1"><span class="co"># Fit a general linearized model</span></a>
<a class="sourceLine" id="cb33-2" title="2"><span class="co"># and save it to an object named &#39;mod&#39;</span></a>
<a class="sourceLine" id="cb33-3" title="3">  mod =<span class="st"> </span><span class="kw">glm</span>(<span class="kw">I</span>(fledged<span class="op">/</span>hatched)<span class="op">~</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb33-4" title="4">            <span class="dt">data=</span>birds,</a>
<a class="sourceLine" id="cb33-5" title="5">            <span class="dt">family=</span>binomial,</a>
<a class="sourceLine" id="cb33-6" title="6">            <span class="dt">weights=</span>hatched)</a>
<a class="sourceLine" id="cb33-7" title="7"></a>
<a class="sourceLine" id="cb33-8" title="8"><span class="co"># Print the model summary</span></a>
<a class="sourceLine" id="cb33-9" title="9">  <span class="kw">summary</span>(mod)</a>
<a class="sourceLine" id="cb33-10" title="10"></a>
<a class="sourceLine" id="cb33-11" title="11">Call<span class="op">:</span></a>
<a class="sourceLine" id="cb33-12" title="12"><span class="kw">glm</span>(<span class="dt">formula =</span> <span class="kw">I</span>(fledged<span class="op">/</span>hatched) <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">family =</span> binomial, <span class="dt">data =</span> birds, </a>
<a class="sourceLine" id="cb33-13" title="13">    <span class="dt">weights =</span> hatched)</a>
<a class="sourceLine" id="cb33-14" title="14"></a>
<a class="sourceLine" id="cb33-15" title="15">Deviance Residuals<span class="op">:</span><span class="st"> </span></a>
<a class="sourceLine" id="cb33-16" title="16"><span class="st">    </span>Min       1Q   Median       3Q      Max  </a>
<a class="sourceLine" id="cb33-17" title="17"><span class="fl">-1.2974</span>  <span class="fl">-0.1818</span>   <span class="fl">0.7135</span>   <span class="fl">1.1181</span>   <span class="fl">1.7476</span>  </a>
<a class="sourceLine" id="cb33-18" title="18"></a>
<a class="sourceLine" id="cb33-19" title="19">Coefficients<span class="op">:</span></a>
<a class="sourceLine" id="cb33-20" title="20"><span class="st">            </span>Estimate Std. Error z value <span class="kw">Pr</span>(<span class="op">&gt;</span><span class="er">|</span>z<span class="op">|</span>)    </a>
<a class="sourceLine" id="cb33-21" title="21">(Intercept)   <span class="fl">1.9972</span>     <span class="fl">0.2445</span>   <span class="fl">8.169</span> <span class="fl">3.11e-16</span> <span class="op">**</span><span class="er">*</span></a>
<a class="sourceLine" id="cb33-22" title="22"><span class="op">---</span></a>
<a class="sourceLine" id="cb33-23" title="23">Signif. codes<span class="op">:</span><span class="st">  </span><span class="dv">0</span> <span class="st">&#39;***&#39;</span> <span class="fl">0.001</span> <span class="st">&#39;**&#39;</span> <span class="fl">0.01</span> <span class="st">&#39;*&#39;</span> <span class="fl">0.05</span> <span class="st">&#39;.&#39;</span> <span class="fl">0.1</span> <span class="st">&#39; &#39;</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb33-24" title="24"></a>
<a class="sourceLine" id="cb33-25" title="25">(Dispersion parameter <span class="cf">for</span> binomial family taken to be <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb33-26" title="26"></a>
<a class="sourceLine" id="cb33-27" title="27">    Null deviance<span class="op">:</span><span class="st"> </span><span class="fl">11.772</span>  on <span class="dv">9</span>  degrees of freedom</a>
<a class="sourceLine" id="cb33-28" title="28">Residual deviance<span class="op">:</span><span class="st"> </span><span class="fl">11.772</span>  on <span class="dv">9</span>  degrees of freedom</a>
<a class="sourceLine" id="cb33-29" title="29">AIC<span class="op">:</span><span class="st"> </span><span class="fl">27.796</span></a>
<a class="sourceLine" id="cb33-30" title="30"></a>
<a class="sourceLine" id="cb33-31" title="31">Number of Fisher Scoring iterations<span class="op">:</span><span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb33-32" title="32"></a>
<a class="sourceLine" id="cb33-33" title="33"><span class="co"># The coefficients are on the scale</span></a>
<a class="sourceLine" id="cb33-34" title="34"><span class="co"># of the link function (logit scale- haven&#39;t </span></a>
<a class="sourceLine" id="cb33-35" title="35"><span class="co"># discussed yet) so we need to transform</span></a>
<a class="sourceLine" id="cb33-36" title="36"><span class="co"># them to the probability scale.</span></a>
<a class="sourceLine" id="cb33-37" title="37">  <span class="co"># Make a function to invert the logit</span></a>
<a class="sourceLine" id="cb33-38" title="38">    inv.logit =<span class="st"> </span><span class="cf">function</span>(x){</a>
<a class="sourceLine" id="cb33-39" title="39">      <span class="kw">exp</span>(x)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(x))</a>
<a class="sourceLine" id="cb33-40" title="40">    }</a>
<a class="sourceLine" id="cb33-41" title="41">  </a>
<a class="sourceLine" id="cb33-42" title="42"><span class="co"># Transform the mean to the probability scale</span></a>
<a class="sourceLine" id="cb33-43" title="43">  <span class="kw">inv.logit</span>(<span class="kw">summary</span>(mod)<span class="op">$</span>coefficients[<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb33-44" title="44">[<span class="dv">1</span>] <span class="fl">0.8805031</span></a></code></pre></div>
<p>We see here that the mean estimated from this method is essentially identical to that estimated above - proof positive that without prior information these two approaches lead the same estimate of the mean. We will dig into this idea more as we move through the semester.</p>
</div>
</div>
</div>

<!DOCTYPE html>

<br>

<hr>

<p style="color:gray; text-align:center">This work is licensed under a <a href="https://creativecommons.org/licenses/by/4.0/legalcode">Creative Commons Attribution 4.0 International License</a>. Data are provided for educational purposes only unless otherwise noted.</p>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
