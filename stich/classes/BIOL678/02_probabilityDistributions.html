<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<!DOCTYPE html>
<head>
<!-- Favicon for various operating systems -->
<link rel="apple-touch-icon" sizes="180x180" href="./favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="./favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="./favicon/favicon-16x16.png">
<!-- <link rel="manifest" href="./favicon/site.webmanifest"> -->
<link rel="mask-icon" href="./favicon/safari-pinned-tab.svg" color="#603cba">
<link rel="shortcut icon" href="./favicon/favicon.ico">
<meta name="msapplication-TileColor" content="#603cba">
<meta name="msapplication-config" content="./favicon/browserconfig.xml">
<meta name="theme-color" content="#382121">
</head>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">danStich</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../../index.html">Home</a>
</li>
<li>
  <a href="../../teaching.html">Teaching</a>
</li>
<li>
  <a href="../../research.html">Research</a>
</li>
<li>
  <a href="../../cv.html">Curriculum vitae</a>
</li>
<li>
  <a href="../../courseWebsites.html">Course websites</a>
</li>
<li>
  <a href="../../contact.html">Contact</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<style>
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}

.column {
    float: left;
    padding: 15px;
}

.clearfix::after {
    content: "";
    clear: both;
    display: table;
}

.content {
    width: 75%;
}

</style>
<p><br></p>
<div id="working-with-probability-distributions" class="section level1">
<h1>Working with probability distributions</h1>
<p><br></p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p><br></p>
<p>This week we are going to talk about probability and probability distributions as a backdrop for the models that we will be working with during the next several weeks. Probability theory is central to statistical techniques, so it will be important for you to have a pretty firm understanding of this to grab hold of big ideas later on. If you are successful in understanding these concepts, you will have a working body of knowledge that is comparable, or better developed than many leading professionals in our fields.</p>
<p><br></p>
<p>When we talk about probability distributions, we are talking about theprobability that a ‘random variable’ takes on some value. In most cases, there is a higher probability that a variable will take on certain values than others. That probability may be governed by any number of processes and thus may assume a number of different shapes with respect to the likelihood of any given value of our random variable. The differences in the shapes, and the mathematical ‘parameters’ that we use to describe those shapes are called ‘probability distributions’.</p>
<p><br></p>
<p>There was a time when biologists were largely restricted to using models that relied heavily on assumptions of normality in the error structure of random variables becuase of how computationally intensive other methods were. This often led to the use of strictly ‘parametric’ tools like ANOVA and t-tests, or the use of strictly ‘non-parametric’ tools like frequency analyses and rank- order methods. While these can still be useful techniques in our toolboxes, that time has passed, and now we have access to a wide range of tools that allow us to extend simple parametric and non-parametric tools to explicitly incorporate mechanisms that allow us to relax or change distributional assumptions. We will discuss these throughout the course, but we need to look at the underlying distributions that govern our decisions about which of these tools to use. So, this week we’ll look at specific distributions that correspond to many of the ‘semi-parametric’ methods that are commonly applied.</p>
<p><br></p>
<p>We are going to use this tutorial for our discussions about probability and probability distributions throughout the week. Next week, we will use this new information to talk about how we calculate distributional statistics from samples and how those can be used for statistical inference before we begin talking about statistical analyses for the remainder of the course. Hopefully the order of things is starting to make some sense now!</p>
<p><br></p>
</div>
<div id="probability-distributions-in-r" class="section level2">
<h2>Probability distributions in R</h2>
<p><br></p>
<p>R has a number of built-in distribution types, and there are random-number generators associated with most or all of these that will allow us to take random samples from a distribution. This is useful for data simulation, but is also helpful for us to learn about probability distributions and how their parameters affect the shape, spread, scale, location, etc. of those distributions. We will briefly discuss concepts like ‘skew’ because of how they can help us think about the assumptions that we are making (or breaking!) in the models that we use.</p>
<p>For this class, we will focus on one major family of distributions and then zero in on a few distributions within this family that you are guaranteed to encounter in analyses throughout your career.</p>
<p><br></p>
<div id="exponential-family-of-distributions" class="section level3">
<h3>Exponential family of distributions</h3>
<p><br></p>
<p>Most or all of the distributions we will need for this class come from the ‘exponential’ family of distributions.</p>
<p>The exponential family is VERY flexible. It includes most of the probability distributions with which you are familiar, and many more. Just ask this <em>very</em> reliable <a href="https://en.wikipedia.org/wiki/Exponential_family">Wikipedia entry</a>. Let’s face it, you were going there anyway, I just cut out the Google step.</p>
<p><br></p>
<p><br></p>
<p>Take a look at the table at the bottom of this Wikipedia page just to get an idea of how many distributions are included within the exponential! Holy cow! We’re not going to look at all of these in this class- I just want you to be aware that this is a HUGE family of specific distributions.</p>
<blockquote>
<p>Distributions that we’ll focus on this week:</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>Continuous
<ul>
<li>Normal (Gaussian)</li>
<li>Lognormal</li>
<li>Beta</li>
<li>NOTE: We’ll talk about the ‘uniform’ distribution in the last two weeks</li>
</ul></li>
<li>Discrete distributions
<ul>
<li>Bernouli</li>
<li>Binomial</li>
<li>Multinomial</li>
<li>Poisson</li>
<li>Negative binomial</li>
</ul></li>
</ol>
<p><br></p>
</div>
<div id="continuous-probability-distributions" class="section level3">
<h3>Continuous probability distributions</h3>
<blockquote>
<p>The normal distribution</p>
</blockquote>
<p>This is one distribution with which most of you have at least some nodding acquaintance.</p>
<p>The normal is defined by two parameters:</p>
<ol style="list-style-type: decimal">
<li><p>The mean (<span class="math inline">\(\mu\)</span>)</p></li>
<li><p>The variance (<span class="math inline">\(\sigma^2\)</span>)</p></li>
</ol>
<p>Let’s take a look at what the normal distribution looks like. We’ll start with the standard normal (z distribution). The standard normal is a normal distribution with a mean of zero and a variance of 1.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">density</span>(
  <span class="kw">rnorm</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)),
  <span class="dt">main=</span><span class="st">&#39;&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;black&#39;</span>,
  <span class="dt">xlab=</span><span class="kw">expression</span>(theta), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p><br></p>
<p>We can change the parameters of the standard normal to increase or decrease the degree of ‘kurtosis’ or peakedness in our distribution. The blue line shows a distribution with lower kurtosis than the z distribution (this one is called a t-distribution). The red line shows a distribution with greater kurtosis than the z distribution.</p>
<p>For line drawings like these, we can just add them to the existing plot to keep the same x and y scales and axes by using the <code>lines</code> function:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">density</span>(
  <span class="kw">rnorm</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)),
  <span class="dt">main=</span><span class="st">&#39;&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;black&#39;</span>,
  <span class="dt">xlab=</span><span class="kw">expression</span>(theta), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))

<span class="kw">lines</span>(<span class="kw">density</span>(
  <span class="kw">rnorm</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">2</span>)),
  <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&#39;blue&#39;</span>)

<span class="kw">lines</span>(<span class="kw">density</span>(
  <span class="kw">rnorm</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span>.<span class="dv">5</span>)),
  <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)  </code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p><br></p>
<p>We can add a legend to clarify:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">density</span>(
  <span class="kw">rnorm</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)),
  <span class="dt">main=</span><span class="st">&#39;&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;black&#39;</span>,
  <span class="dt">xlab=</span><span class="kw">expression</span>(theta), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))

<span class="kw">lines</span>(<span class="kw">density</span>(
  <span class="kw">rnorm</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">2</span>)),
  <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&#39;blue&#39;</span>)

<span class="kw">lines</span>(<span class="kw">density</span>(
  <span class="kw">rnorm</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span>.<span class="dv">5</span>)),
  <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)  

<span class="kw">legend</span>(<span class="dt">x=</span><span class="dv">1</span>, <span class="co"># x-coordinate for legend</span>
       <span class="dt">y=</span>.<span class="dv">9</span>,<span class="co"># Y-coordinate for legend</span>
       <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&#39;mu=0, sd=1&#39;</span>, <span class="st">&#39;mu=0, sd=2&#39;</span>, <span class="st">&#39;mu=0, sd=.5&#39;</span>), <span class="co"># Names</span>
       <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&#39;black&#39;</span>, <span class="st">&#39;blue&#39;</span>, <span class="st">&#39;red&#39;</span>), <span class="co"># Colors</span>
       <span class="dt">lty=</span><span class="dv">1</span>,  <span class="co"># Line type for legend symbols</span>
       <span class="dt">lwd=</span><span class="dv">2</span>,  <span class="co"># Line width for legend symbols</span>
       <span class="dt">bg=</span><span class="st">&#39;n&#39;</span>, <span class="co"># Legend fill: none</span>
       <span class="dt">bty=</span><span class="st">&#39;n&#39;</span> <span class="co"># Box type for legend: none</span>
) <span class="co"># Close call to legend</span></code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p><br></p>
<blockquote>
<p>The lognormal distribution</p>
</blockquote>
<p>The lognormal distribution is a probability distribution that assumes our random variable is normally distributed on the log scale. This assumption allows us to incorporate ‘skew’ into the normal distribution and change the location and spread of the normal distribution by transforming the parameters (<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>) onto the log scale. This is one of the more common data transformations that you will run into, e.g.: “We used a log-10 transformation to normalize the variable prior to analysis…”</p>
<p>Let’s take a look at how changes to the mean change the location of this distribution:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the density fucntion for a lognormal</span>
<span class="co"># distribution that has a mean of zero and</span>
<span class="co"># a standard deviation of 1.</span>
  <span class="kw">plot</span>(<span class="kw">density</span>(
    <span class="kw">rlnorm</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)),
    <span class="dt">main=</span><span class="st">&#39;&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;black&#39;</span>, 
    <span class="dt">xlab=</span><span class="kw">expression</span>(theta), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))

<span class="co"># Now, we can change the parameters to change the location and spread</span>
  <span class="kw">lines</span>(<span class="kw">density</span>(
    <span class="kw">rlnorm</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">mean=</span><span class="dv">1</span>, <span class="dt">sd=</span><span class="dv">1</span>)),
    <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&#39;blue&#39;</span>) 
  
  <span class="kw">lines</span>(<span class="kw">density</span>(
    <span class="kw">rlnorm</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">mean=</span><span class="dv">2</span>, <span class="dt">sd=</span><span class="dv">1</span>)),
    <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)  

<span class="co"># Add a legend to the plot</span>
  <span class="kw">legend</span>(<span class="dt">x=</span><span class="dv">20</span>, <span class="dt">y=</span>.<span class="dv">9</span>,
         <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&#39;mu=0, sd=1&#39;</span>, <span class="st">&#39;mu=1, sd=1&#39;</span>, <span class="st">&#39;mu=2, sd=1&#39;</span>), 
         <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&#39;black&#39;</span>, <span class="st">&#39;blue&#39;</span>, <span class="st">&#39;red&#39;</span>),
         <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">bg=</span><span class="st">&#39;n&#39;</span>, <span class="dt">bty=</span><span class="st">&#39;n&#39;</span>)</code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p><br></p>
<blockquote>
<p>The beta distribution</p>
</blockquote>
<p>The beta distribution is a probability distribution that is constrained to the interval (0, 1). But, it is incredibly flexible in its parameterization, and as a result is very useful for stochastic simulation of variables on the probability scale, such as survival.</p>
<p>The parameters of the beta distribution are <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, or commonly ‘a’ and ‘b’. Within this distribution, <span class="math inline">\(\alpha\)</span> pushes the distribution to the right (toward 1), and <span class="math inline">\(\beta\)</span> pushes the distribution to the left (toward 0). The relative magnitude of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> determine the location, shape, and spread of the probability distribution for our random variable. When <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are equal, the beta distribution is a t-distribution within the interval (0, 1).</p>
<p>Let’s take a look:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Within the rbeta function in R, &#39;a&#39; is called &#39;shape1&#39; and &#39;b&#39; is called</span>
<span class="co"># &#39;shape2&#39;</span>
  <span class="kw">plot</span>(<span class="kw">density</span>(<span class="kw">rbeta</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">shape1=</span><span class="dv">50</span>, <span class="dt">shape2=</span><span class="dv">50</span>)), <span class="dt">main=</span><span class="st">&#39;&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>,
    <span class="dt">col=</span><span class="st">&#39;black&#39;</span>, <span class="dt">xlab=</span><span class="kw">expression</span>(theta), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">50</span>), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))
<span class="co"># Now, we can change the parameters to change the location and spread</span>
  <span class="kw">lines</span>(<span class="kw">density</span>(<span class="kw">rbeta</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">shape1=</span><span class="dv">50</span>, <span class="dt">shape2=</span><span class="dv">100</span>)), <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&#39;blue&#39;</span>)      
  <span class="kw">lines</span>(<span class="kw">density</span>(<span class="kw">rbeta</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">shape1=</span><span class="dv">500</span>, <span class="dt">shape2=</span><span class="dv">250</span>)), <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col =</span> <span class="st">&#39;red&#39;</span>)  
<span class="co"># Add a legend to the plot</span>
  <span class="kw">legend</span>(<span class="dt">x=</span>.<span class="dv">65</span>, <span class="dt">y=</span><span class="dv">50</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&#39;a=50, b=50&#39;</span>, <span class="st">&#39;a=50, b=100&#39;</span>, <span class="st">&#39;a=500, b=250&#39;</span>), 
         <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&#39;black&#39;</span>, <span class="st">&#39;blue&#39;</span>, <span class="st">&#39;red&#39;</span>), <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">bg=</span><span class="st">&#39;n&#39;</span>, <span class="dt">bty=</span><span class="st">&#39;n&#39;</span>)      </code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p><br></p>
</div>
<div id="discrete-probability-distributions" class="section level3">
<h3>Discrete probability distributions</h3>
<p><br></p>
<p>Discrete probability distributions are useful for situations in which our random variable of interest can only take specific values within the interval of interest. For example, this might include age, counts, pass/fail, or any number of conceivable categories. As a result, these require a slightly different treatment of probability as a discrete, rather than continuous phenomenon.</p>
<blockquote>
<p>The Bernoulli distribution</p>
</blockquote>
<p>The Bernoulli distribution is a special case of the binomial distribution with a single trial (see below for clarification). Bernoulli outcomes are those for which the random variable can take on one of two values: a one or a zero. This distribution is useful for visualizing processes such as coin flips, yes/no responses, live/dead endpoints, and a number of other very interesting phenomena. The Bernoulli distribution has a single parameter: the probability of success, but is also governed by sample size: n.</p>
<p>We can simulate data from a Bernoulli distribution in one of two ways in R</p>
<p>The ‘old-school’ way of doing this was to draw from a binomial with a single ‘trial’. Here we randomly draw a single sample from a binomial with a single trial, and a probability of success of 50%. This can be likened to flipping a fair coin.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rbinom</span>(<span class="dt">n=</span><span class="dv">1</span>, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span>.<span class="dv">5</span>)</code></pre></div>
<pre><code>## [1] 0</code></pre>
<p><br></p>
<p>But now, there is a function in the <code>Rlab</code> package that simplifies this for the specific case of a Bernoulli.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  <span class="co"># First, install the necessary package</span>
    <span class="co">#install.packages(&#39;Rlab&#39;) # Uncomment to install</span>
    <span class="kw">library</span>(Rlab)
  <span class="co"># Now we can take a random sample from a Bernouli</span>
    <span class="co"># Flip the coin once</span>
      <span class="kw">rbern</span>(<span class="dt">n=</span><span class="dv">1</span>, <span class="dt">prob=</span>.<span class="dv">5</span>)</code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="co"># Flip the coin 100 times</span>
      <span class="kw">rbern</span>(<span class="dt">n=</span><span class="dv">100</span>, <span class="dt">prob=</span>.<span class="dv">5</span>)</code></pre></div>
<pre><code>##   [1] 1 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 0
##  [36] 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0
##  [71] 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0</code></pre>
<p><br></p>
<blockquote>
<p>The binomial distribution</p>
</blockquote>
<p>The binomial distribution is pretty similar to the Bernoulli distribution except that it also includes a parameter called ‘size’ which corresponds to a number of trials. In most cases in biology, it will suffice to use the Bernoulli, but for modeling we will want to understand the binomial for things like random stratified designs and nested models that rely on the use of binomial distribution</p>
<p>To sample data from a binomial distribution, we can use <code>rbinom</code>. In this example we tell R that we want 10 samples (<code>n</code>) from a binomial distribution that has 10 trials (<code>size</code>) and a probability of success (<code>prob</code>) of 0.5. This is like having 10 people flip the coin 10 times instead of just one person flipping the coin 100 times.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Take a random draw of 10 samples from a binomial distribution with 10 trials</span>
<span class="co"># and probability of success equal to 0.50</span>
<span class="kw">rbinom</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">size=</span><span class="dv">10</span>, <span class="dt">prob=</span><span class="fl">0.5</span>)</code></pre></div>
<pre><code>##  [1] 6 7 2 3 1 5 4 5 5 6</code></pre>
<p><br></p>
<blockquote>
<p>The multinomial distribution</p>
</blockquote>
<p>The multinomial distribution is a further generalization of the Binomial and Bernoulli distribution. Here, there are one or more possible categorical outcomes, and the probability of each one occuring is specified individually <strong>but all of them must sum to one</strong>. The categories are, in this case, assumed to be a <em>mutually exclusive</em> and <em>exhaustive</em> set of possible outcomes.</p>
<p>We can use the multinomial distribution to randomly sample from categories (imagine our response variable is a categorical variable, like the names of the students in this class). To do this:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># First, we make a vector of names:</span>
  name =<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;Byron&#39;</span>, <span class="st">&#39;Connor&#39;</span>, <span class="st">&#39;Emma E.&#39;</span>, <span class="st">&#39;Emma T&#39;</span>, <span class="st">&#39;Jess&#39;</span>, <span class="st">&#39;Kathleen&#39;</span>,
           <span class="st">&#39;Kelly&#39;</span>, <span class="st">&#39;Melissa&#39;</span>, <span class="st">&#39;Nshyira&#39;</span>,<span class="st">&#39;Ryan&#39;</span>, <span class="st">&#39;Tia&#39;</span>)
<span class="co"># Then, we assign a uniform probability of drawing any given name if they</span>
<span class="co"># can all be drawn with equal frequency:</span>
  probs =<span class="st"> </span><span class="kw">rep</span>(<span class="dt">x=</span><span class="dv">1</span><span class="op">/</span><span class="kw">length</span>(name), <span class="dt">times=</span><span class="kw">length</span>(name))      
  probs      </code></pre></div>
<pre><code>##  [1] 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909
##  [7] 0.09090909 0.09090909 0.09090909 0.09090909 0.09090909</code></pre>
<p><br></p>
<p>Now, we can sample from a multinomial distribution using our objects. Here we are taking 5 samples from the distribution, each time we sample there is only one trial, and we are sampling the 8 probabilities above.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rmultinom</span>(<span class="dt">n=</span><span class="dv">5</span>, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span>probs)</code></pre></div>
<pre><code>##       [,1] [,2] [,3] [,4] [,5]
##  [1,]    0    0    0    0    0
##  [2,]    0    0    0    0    0
##  [3,]    0    0    0    0    0
##  [4,]    0    0    0    1    0
##  [5,]    0    0    0    0    0
##  [6,]    0    0    1    0    1
##  [7,]    0    1    0    0    0
##  [8,]    0    0    0    0    0
##  [9,]    0    0    0    0    0
## [10,]    1    0    0    0    0
## [11,]    0    0    0    0    0</code></pre>
<p><br></p>
<p><strong>WHOA</strong> a matrix??!!! HOLY CRAP, WHAT DOES IT ALL MEAN?</p>
<p>Take a step back and think about this. The rows in this matrix are you and your classmates. If we took one random sample from the multinomial distribution, it would look like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Take a single sample from the list of student names    </span>
  <span class="kw">rmultinom</span>(<span class="dt">n=</span><span class="dv">1</span>, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span>probs)</code></pre></div>
<pre><code>##       [,1]
##  [1,]    0
##  [2,]    0
##  [3,]    0
##  [4,]    0
##  [5,]    0
##  [6,]    0
##  [7,]    0
##  [8,]    1
##  [9,]    0
## [10,]    0
## [11,]    0</code></pre>
<p><br></p>
<p>Here, we pulled a single sample from the distribution, and probability of sampling a given individual was 0.09 (1/11). If it makes it easier, we can put your names next to it:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cbind</span>(name, <span class="kw">rmultinom</span>(<span class="dt">n=</span><span class="dv">1</span>, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span>probs))</code></pre></div>
<pre><code>##       name          
##  [1,] &quot;Byron&quot;    &quot;0&quot;
##  [2,] &quot;Connor&quot;   &quot;0&quot;
##  [3,] &quot;Emma E.&quot;  &quot;0&quot;
##  [4,] &quot;Emma T&quot;   &quot;0&quot;
##  [5,] &quot;Jess&quot;     &quot;0&quot;
##  [6,] &quot;Kathleen&quot; &quot;0&quot;
##  [7,] &quot;Kelly&quot;    &quot;0&quot;
##  [8,] &quot;Melissa&quot;  &quot;0&quot;
##  [9,] &quot;Nshyira&quot;  &quot;1&quot;
## [10,] &quot;Ryan&quot;     &quot;0&quot;
## [11,] &quot;Tia&quot;      &quot;0&quot;</code></pre>
<p><br></p>
<p>Now, if I was calling on you randomly in class, after 10 questions, the spread of people who have participated in class might look like this:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cbind</span>(name, <span class="kw">rmultinom</span>(<span class="dt">n=</span><span class="dv">10</span>, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span>probs))</code></pre></div>
<pre><code>##       name                                              
##  [1,] &quot;Byron&quot;    &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot;
##  [2,] &quot;Connor&quot;   &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot;
##  [3,] &quot;Emma E.&quot;  &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot;
##  [4,] &quot;Emma T&quot;   &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot;
##  [5,] &quot;Jess&quot;     &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot;
##  [6,] &quot;Kathleen&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot;
##  [7,] &quot;Kelly&quot;    &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot;
##  [8,] &quot;Melissa&quot;  &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot;
##  [9,] &quot;Nshyira&quot;  &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot;
## [10,] &quot;Ryan&quot;     &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot;
## [11,] &quot;Tia&quot;      &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot;</code></pre>
<p><br></p>
<p>Taking this one step further, we could just draw a name and stop looking at these ugly (no but really they are awesome!) matrices:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  name[<span class="kw">which</span>(<span class="kw">rmultinom</span>(<span class="dt">n=</span><span class="dv">1</span>, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span>probs)<span class="op">&gt;</span><span class="dv">0</span>)]</code></pre></div>
<pre><code>## [1] &quot;Byron&quot;</code></pre>
<p><br></p>
<p>And now we have a way to randomly select an individual based on a multinomial distribution!</p>
<blockquote>
<p>The Poisson distribution</p>
</blockquote>
<p>The Poisson distribution is used for counts or other integer data. This distribution is widely used (and just as widely misused!) for its ability to account for a large number of biological and ecological processes in the models that we will discuss this semester. The Poisson distribution has a single parameter, <span class="math inline">\(\lambda\)</span>, which is both the mean and the variance of the distribution. So, despite its utility, the distribution is relatively inflexible with respect to shape and spread. <strong>Fun fact</strong>: this distribution was originally worked out by a French mathematician to predict the number of soldiers who were accidentally killed from being kicked by horses.</p>
<p>Take a look at how the distribution changes when we change <span class="math inline">\(\lambda\)</span>, and you will get an idea of how this one works.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(<span class="kw">rpois</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">lambda=</span><span class="dv">100</span>), <span class="dt">main=</span><span class="st">&#39;&#39;</span>)</code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(<span class="kw">rpois</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">lambda=</span><span class="dv">1000</span>), <span class="dt">main=</span><span class="st">&#39;&#39;</span>)    </code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-17-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(<span class="kw">rpois</span>(<span class="dt">n=</span><span class="fl">1e4</span>, <span class="dt">lambda=</span><span class="dv">10000</span>), <span class="dt">main=</span><span class="st">&#39;&#39;</span>)  </code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-17-3.png" width="672" /></p>
<p><br></p>
<blockquote>
<p>The negative binomial distribution</p>
</blockquote>
<p>Okay, this one can be a little difficult to wrap your head around but it’s an important one for us to know about. So, we will spend a little extra time setting this one up to try and be clear. Often, folks start out thinking that they’re going to use a Poisson distribution and they end up collecting with data that do not conform to the relative inflexibility of that single- parameter distribution. For the purpose of this class, we are not going to dive into the mechanics of the negative binomial distribution, but we do need to know what it looks like and why we might need it.</p>
<p>One useful way to conceptualize the negative binomial is ‘how long does it take for some event to occur?’ For example, we might ask how long it takes a fish to start migrating, how long it takes a sea turtle to recover in a rehabilitation center, how long it will take for a terminal patient to expire, or how frequently we see the expression of a gene of interest. These kinds of questions are asked in aptly named ‘time-to-event’ models that rely on the variance structure of the negative binomial. In the context of these kinds of questions, the negative binomial is a discrete probability distribution (and not a continuous distribution) because the ‘time’ components of the distribution is actually a series of independent Bernoulli trials (holy crap!). For example: if we want to know how many days it will take for a turtle to recover, what we are really doing is asking on each day until recovery, ‘Is today the day?’. Then, we flip a coin and find out. So, each day in this example is a Bernoulli trial. Another way to think about this is the number of failures occurring in a sequence before a target number of sucesses is achieved.</p>
<p>For the classical parameterization:</p>
<p>We will start with looking at how many failures are observed before one success in a sequence of Bernoulli trials.</p>
<p>With probability of succes equal to .95, it doesn’t take long and most of the probability mass is on zero, with a couple of stragglers further out.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(<span class="kw">rnbinom</span>(<span class="fl">1e4</span>, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span>.<span class="dv">95</span>))</code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p><br></p>
<p>If we decrease probability of success in each trial to 0.25, we see more failures on average before we reach success. Most of the time, it still takes less than 5 trials to reach a success, but some times it takes much longer.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(<span class="kw">rnbinom</span>(<span class="fl">1e4</span>, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span>.<span class="dv">25</span>))</code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p><br></p>
<p>And, if we increase the number of successes that we use for our criterion, or target, then it spreads the distribution out even further.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="kw">hist</span>(<span class="kw">rnbinom</span>(<span class="fl">1e4</span>, <span class="dt">size=</span><span class="dv">10</span>, <span class="dt">prob=</span>.<span class="dv">25</span>))        </code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p><br></p>
<p>Now, because of it’s properties, the negative binomial is also useful for number of other applications. Specifically, it has been widely used to represent Poisson-like processes in which the mean and variance are not equal (e.g., <em>overdispersion</em>). This has seen a lot of application in the field of ecology, especially for overdispersed count data.</p>
<p>Here, we sample 10k random draws from a distribution with a mean of 10 and an overdispersion parameter of 1. The overdispersion parameter is called ‘size’ because this is an alternative parameterization that is just making use of the relationships between existing parameters of the negative binomial. It’s easy to grasp how the mean changes the location of the distribution.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="kw">hist</span>(<span class="kw">rnbinom</span>(<span class="fl">1e4</span>, <span class="dt">mu=</span><span class="dv">10</span>, <span class="dt">size=</span><span class="dv">1</span>))</code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p><br></p>
<p>But, note how the overdispersion parameter changes things:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="kw">hist</span>(<span class="kw">rnbinom</span>(<span class="fl">1e4</span>, <span class="dt">mu=</span><span class="dv">10</span>, <span class="dt">size=</span>.<span class="dv">1</span>))      </code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="kw">hist</span>(<span class="kw">rnbinom</span>(<span class="fl">1e4</span>, <span class="dt">mu=</span><span class="dv">10</span>, <span class="dt">size=</span><span class="dv">10</span>))</code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-22-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="kw">hist</span>(<span class="kw">rnbinom</span>(<span class="fl">1e4</span>, <span class="dt">mu=</span><span class="dv">10</span>, <span class="dt">size=</span><span class="dv">100</span>))      </code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-22-3.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="kw">hist</span>(<span class="kw">rnbinom</span>(<span class="fl">1e4</span>, <span class="dt">mu=</span><span class="dv">10</span>, <span class="dt">size=</span><span class="dv">1000</span>))  </code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-22-4.png" width="672" /></p>
<p><br></p>
<p>A more intuitive way to work with the negative binomial in R is by using the <code>MASS</code> package. In this parameterization, we use the mean and the dispersion parameter explicitly so it makes more sense:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># MASS comes pre-installed as part of base software</span>
  <span class="kw">library</span>(MASS)
  <span class="kw">hist</span>(<span class="kw">rnegbin</span>(<span class="fl">1e4</span>, <span class="dt">mu=</span><span class="dv">10</span>, <span class="dt">theta =</span> .<span class="dv">1</span>))  </code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  <span class="kw">hist</span>(<span class="kw">rnegbin</span>(<span class="fl">1e4</span>, <span class="dt">mu=</span><span class="dv">10</span>, <span class="dt">theta =</span> <span class="dv">10</span>))       </code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-23-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  <span class="kw">hist</span>(<span class="kw">rnegbin</span>(<span class="fl">1e4</span>, <span class="dt">mu=</span><span class="dv">10</span>, <span class="dt">theta =</span> <span class="dv">100</span>))       </code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-23-3.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  <span class="kw">hist</span>(<span class="kw">rnegbin</span>(<span class="fl">1e4</span>, <span class="dt">mu=</span><span class="dv">10</span>, <span class="dt">theta =</span> <span class="dv">1000</span>)) </code></pre></div>
<p><img src="02_probabilityDistributions_files/figure-html/unnamed-chunk-23-4.png" width="672" /></p>
<p><br></p>
<p><strong>NOTE</strong> that the results are pretty much identical.</p>
<p><br></p>
</div>
</div>
<div id="calculating-sample-statistics-and-distributional-parameters-in-r" class="section level2">
<h2>Calculating sample statistics and distributional parameters in R</h2>
<p><br></p>
<p>In this section, we will learn how to derive the parameters of the normal distribution using a few different methods in R. We will use this opportunity to re-introduce the parameters as ‘moments’ of the distribution so we can talk about what we mean by ‘confidence intervals’. We also will introduce a couple of different methods for calculating ‘moments’ of a distribution. Specifically, we will look at how to derive…</p>
<p><br></p>
<div id="moments-about-the-mean" class="section level3">
<h3>Moments about the mean</h3>
<p><br></p>
<p>Sounds fancy, huh?</p>
<ol style="list-style-type: decimal">
<li>Zeroth moment
<ul>
<li>This is the sum of the total probability of the distribution 1.00, always</li>
</ul></li>
<li>First moment
<ul>
<li>The mean</li>
<li>We will look at a few ways to calculate this</li>
</ul></li>
<li>Second moment +The variance +As with the mean, we will examine a couple of options for calculating</li>
<li>Third moment
<ul>
<li>Skew</li>
<li>We won’t calculate for this class, but we have discussed, and this parameter contributes to the location/spread of the distribution (how far left or right the peak is)</li>
</ul></li>
<li>Fourth moment
<ul>
<li>Kurtosis</li>
<li>Similarly, we won’t cover the calculation, but this is another moment that we have discussed with respect to departure from a z distribution in the normal</li>
</ul></li>
</ol>
<p><br></p>
</div>
<div id="estimating-parameters-of-the-normal-distribution-from-a-sample" class="section level3">
<h3>Estimating parameters of the normal distribution from a sample</h3>
<p><br></p>
<p>The tools demonstrated below can be used for most of the probability distributions that have been implemented in R, and we could go on and on forever about them. But, for the sake of our collective sanity we will walk through the tools available using the normal distribution alone, <em>although I encourage you to explore others as applicable to the work that you are doing!</em></p>
<blockquote>
<p>Method of moments estimator</p>
</blockquote>
<p>See if you can rearrange this in a way that makes sense with how you know to calculate an ‘average’ and a variance!</p>
<p>First, we’ll estimate it by making our own function:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make up a vector of data to test this on</span>
  test_norm =<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">1e3</span>, <span class="dv">0</span>, <span class="dv">1</span>)
  
<span class="co"># Write the function</span>
  norm.mom =<span class="st"> </span><span class="cf">function</span>(x){                     <span class="co"># Define a function by name</span>
    x_bar =<span class="st"> </span>(<span class="dv">1</span><span class="op">/</span><span class="kw">length</span>(x)) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(x)            <span class="co"># Calculate mean</span>
    sigma2 =<span class="st"> </span>(<span class="dv">1</span><span class="op">/</span><span class="kw">length</span>(x)) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>((x<span class="op">-</span>x_bar)<span class="op">^</span><span class="dv">2</span>) <span class="co"># Calculate variance</span>
    <span class="kw">return</span>(<span class="kw">c</span>(x_bar, sigma2))                  <span class="co"># Return the calculations</span>
  }
<span class="co"># Test the function</span>
  <span class="kw">norm.mom</span>(test_norm)</code></pre></div>
<pre><code>## [1] -0.04135066  0.98475546</code></pre>
<p><br></p>
<p>Because this one is so common, R has built-in estimators that rely on the exact solution provided by the formulas for the first two moments of the normal distribution:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  <span class="kw">mean</span>(test_norm)</code></pre></div>
<pre><code>## [1] -0.04135066</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  <span class="kw">var</span>(test_norm)</code></pre></div>
<pre><code>## [1] 0.9857412</code></pre>
<p><br></p>
<p>How do these compare to the answers returned by our function?</p>
<blockquote>
<p>Maximum likelihood estimator</p>
</blockquote>
<p>R has built-in maximum likelihood estimators that provide an exact solution to the first two moments of the normal distribution.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)
<span class="kw">fitdistr</span>(test_norm, <span class="st">&#39;normal&#39;</span>)</code></pre></div>
<pre><code>##       mean           sd     
##   -0.04135066    0.99234846 
##  ( 0.03138081) ( 0.02218959)</code></pre>
<p><br></p>
<p>Only one problem here: R doesn’t report the second moment! It reports the square root of the second moment: the standard deviation!! What the crap is that all about?</p>
<p>Finally, let’s write our own function and maximize the likelihood with the <code>optim()</code> function in R.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Define the function</span>
  normal.lik =<span class="st"> </span><span class="cf">function</span>(theta, y){
    <span class="co"># The starting value for mu that we provide</span>
    mu =<span class="st"> </span>theta[<span class="dv">1</span>]
    <span class="co"># The starting value for sigma2 that we provide</span>
    sigma2 =<span class="st"> </span>theta[<span class="dv">2</span>]
    <span class="co"># Number of observations in the data</span>
    n =<span class="st"> </span><span class="kw">nrow</span>(y)
    <span class="co"># Compute the log likelihood of the data (y) using the likelihood</span>
    <span class="co"># function for the normal distribution given the starting values for our</span>
    <span class="co"># parameters (contained in the vector &#39;theta&#39;)</span>
    logl =<span class="st"> </span><span class="op">-</span>.<span class="dv">5</span><span class="op">*</span>n<span class="op">*</span><span class="kw">log</span>(<span class="dv">2</span><span class="op">*</span>pi) <span class="op">-</span>.<span class="dv">5</span><span class="op">*</span>n<span class="op">*</span><span class="kw">log</span>(sigma2)<span class="op">-</span>(<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>sigma2))<span class="op">*</span><span class="kw">sum</span>((y<span class="op">-</span>mu)<span class="op">**</span><span class="dv">2</span>)
    <span class="kw">return</span>(<span class="op">-</span>logl)
  }</code></pre></div>
<p><br></p>
<p>Now, we use the <code>optim</code> function to maximize the likelihood of the data (technically by minimizing the -log likehood) given different values of our parameters (mu and sigma2).</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  <span class="kw">optim</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), normal.lik, <span class="dt">y=</span><span class="kw">data.frame</span>(test_norm))</code></pre></div>
<pre><code>## $par
## [1] -0.04154948  0.98491167
## 
## $value
## [1] 1411.258
## 
## $counts
## function gradient 
##       41       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL</code></pre>
<p><br></p>
<p>We can also make this into an object and call the parts by name:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make it into an object</span>
  est =<span class="st"> </span><span class="kw">optim</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), normal.lik, <span class="dt">y=</span><span class="kw">data.frame</span>(test_norm))        
<span class="co"># Look at the structure </span>
  <span class="kw">str</span>(est)   <span class="co"># I&#39;ll be damned, it&#39;s a list! Hey, we learned about those!</span></code></pre></div>
<pre><code>## List of 5
##  $ par        : num [1:2] -0.0415 0.9849
##  $ value      : num 1411
##  $ counts     : Named int [1:2] 41 NA
##   ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;function&quot; &quot;gradient&quot;
##  $ convergence: int 0
##  $ message    : NULL</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  est<span class="op">$</span>par    <span class="co"># Here are the estimates</span></code></pre></div>
<pre><code>## [1] -0.04154948  0.98491167</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  est<span class="op">$</span>par[<span class="dv">1</span>] <span class="co"># The mean</span></code></pre></div>
<pre><code>## [1] -0.04154948</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  est<span class="op">$</span>par[<span class="dv">2</span>] <span class="co"># The variance</span></code></pre></div>
<pre><code>## [1] 0.9849117</code></pre>
<p><br></p>
<p>That’s better: an exact solution!! Just in case you’d like to know exactly what <code>optim</code> is doing:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  ?optim</code></pre></div>
<p><br></p>
<blockquote>
<p>Quantiles and other descriptive statistics</p>
</blockquote>
<p>There are a number of other ways we might like to describe this this (or any) sampling distribution. Here are a few examples that we will work with this semester.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="kw">median</span>(test_norm) <span class="co"># Here is the median, or 50th percentile</span></code></pre></div>
<pre><code>## [1] -0.09427525</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="kw">quantile</span>(test_norm, <span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>)) <span class="co"># The 95% confidence limits</span></code></pre></div>
<pre><code>##      2.5%     97.5% 
## -1.902265  1.991910</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="kw">quantile</span>(test_norm, <span class="dt">probs =</span> <span class="kw">c</span>(<span class="fl">0.25</span>, <span class="fl">0.75</span>))   <span class="co"># Inner quartile range </span></code></pre></div>
<pre><code>##        25%        75% 
## -0.7390407  0.6393896</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    <span class="kw">range</span>(test_norm)                             <span class="co"># Range of sample</span></code></pre></div>
<pre><code>## [1] -3.578292  3.242863</code></pre>
<p><br></p>
<p><br></p>
</div>
</div>
</div>

<!DOCTYPE html>

<br>

<hr>

<p style="color:gray">This work is licensed under a <a href="https://creativecommons.org/licenses/by/4.0/legalcode">Creative Commons Attribution 4.0 International License.</a></p>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
