```{r child="../../_styles.Rmd"}
```

<h1 id="multi"> Generalized linear mixed models (GLMM) <h1>

<img src="../../images/matrixSmolt.jpg" alt="">

<h2 id="multi"> Introduction </h2>
 
This week in lecture, we introduced the generalized linear mixed model (GLMM). As we have discussed, the GLMM is to the linear mixed model as the GLM is to the general linear model (ANOVA, linear regression, ANCOVA, etc.). That is to say, the GLMM is just an LMM that assumes some error distribution other than normal. This week in lab, we will practice running GLMMs using restricted maximum likelihood estimation (REML) in the `lme4` package in R, and using Bayesian estimation in JAGS through the `R2jags` package in R. We will play with a couple of different data sets this week, and give you the choice of which to pursue further.

## Choose your own data adventure {.tabset .tabset-fade} 
<!-- .tabset-fade .tabset-pills} -->

This week we will work with two different data examples to demonstrate a couple different applications of GLMM to non-normal data. The binomial and Poisson distributions are commonly used to describe biological and ecological processes due to the nature of the data we collect. Therefore, one of the examples this week will use a Bernouli response (special case of the binomial where `N` = 1), and the other will use a count response to demonstrate the application of GLMMs in biology and ecology. Please realize that procedures outlined below are generally applicable within the broader context of GLMM, and they are easily generalized to include a wide variety of other sampling distributions not discussed in class.

For our assignments this week, you may choose to work with either one of these examples, and must only submit a write-up for one of them. You choose, although I encourage you to explore with both of them to investigate similarities and differences in how things work for the models.

### <span style="color:black"> Walleye phenology</span>

<h2 id="multi"><b> Walleye phenology </b></h2>

It is that magical time of year again. Birds are returning from their winter vacations, and all of the critters are twitterpaited. The salamanders and frogs are making there way to breeding pools in the soaked leaf litter, and even the fish are warming up for the spawn. There's only one problem: we don't quite know when those fish are going to get into the streams so we can catch them, clip their fins, put some tags in them, and study their every move (...muwahaha...). While the timing of daffodil emergence is often a reliable indicator, it would be nice to have a simple regression we could use to predict the timing of the fish spawn and maximize field sampling efficiency.

For this first example, we will attempt to predict counts of walleye, *Sander vitreus*, in spawning streams of Otsego Lake based on historical counts and climate data.

<h3 id="multi"><b> Walleye data </b></h3>

We begin by reading in the data set:

```{r, echo=TRUE, eval=FALSE}
# Read in the walleye data
  wae = read.csv('classData-master/otsegoRunCounts.csv', stringsAsFactors = FALSE)
```

```{r, echo=FALSE, eval=TRUE}
# Read in the walleye data
  wae = read.csv('../../data/otsegoRunCounts.csv', stringsAsFactors = FALSE)
```

Have a look at the first ten lines of the data set:
```{r}
  head(wae, 10)
```

And check out the data structure   

```{r}  
 str(wae)
```

These data are measurements of the length and mass of individual walleye at various reproductive stages that were captured in spawning tributaries of Otsego Lake during the 2009 and 2013 spawning season. o

We will use the data to predict number of walleye we expect to see each day in the spawning tribs during spring 2017 based on historical counts.

<h3 id="multi"><b> Climate Data </b></h3>

For this example, we are interested in predicting the timing of the walleye run. Generally speaking, phenology of spawning events in fishes and many other animals is driven by circannual rhythms entrained by photoperiod. However, temperature often acts as a trigger for releasing behavior related to spawning...**Translation**: if we want to predict timing of the spawning run, we need to have data for photoperiod and temperature, too!

I collected some of this information ahead of time for you. Temperature, precipitation, and snow depth data were downloaded from the following, surely reliable, [website](http://www.usclimatedata.com/climate/cooperstown/new-york/united-states/usny0326/2017/3).

Read in the data an have a look at the structure
```{r, echo=TRUE, eval=FALSE}
# Read in the climate data
  climate=read.csv('classData-master/tempdata.csv', stringsAsFactors = FALSE)

# Have a look at the data structure
  str(climate)
  
```

```{r, echo=FALSE, eval=TRUE}
# Read in the climate data
  climate=read.csv('../../data/tempdata.csv', stringsAsFactors = FALSE)

# Have a look at the data structure
  str(climate)
  
```


```{r}
# Have a look at the data structure
  str(climate)

```

<h3 ><b> Data management </b></h3>
<h4 id="multi"><b> Walleye counts </b></h4>

Okay, so now we have fish records by date and we have some climate data for those same dates and then some. Now, we need to smash all of those data together so we can use them.

Our first step will be to summarize the walleye counts by date. To do this, we will use the `plyr` package in R.

```{r, message=FALSE, warning=FALSE}
# Load the package
  library(plyr)
```

Tally up total counts of walleye in each stream on each day.
 
```{r}
# Make a column of ones that we can sum
weye = ddply(wae, c('date', 'site'), summarize, counts = length(Sex))
```

<h4><b> Climate data </b></h4>

Next, we will get the climate data in order. 

Because temperature fluctuates pretty widely in the spring, we often think of accumulated thermal units (ATU) or degree days as having more of an influence on the reproductive biology of many organisms rather than just absolute temperatures. You can think of degree days as the sum of all temperatures from some starting date until some ending date. In our case, we will add up the degree days from January 1 until date i in our dataframe to get degree days for each observation.

First, we will convert our temperatures to celcius like the rest of the world.

```{r}
# Convert from farenheit to celcius
  climate$high_c = (climate$high_f-32)*5/9
  climate$low_c = (climate$low_f-32)*5/9
```

Next, we will calculate degree days (in $^{\circ}$C) using both the high temp and the low temp. We will start by calculating the mean of highs and lows, and then add them up for the time period of interest. Ideally, we would be working with averages, but this will do for now.

```{r, message=FALSE, warning=FALSE}
# Calculate mean based on daily highs and lows
  climate$mean_c = apply(climate[ , 4:5], 1, mean)

# Exclude values less than zero from this calculation
  climate$ddPrep = climate$mean_c
  
  climate$ddPrep[climate$ddPrep < 0] = 0

# Change date to a date object from factor
  # Load lubridate package
    library(lubridate)
  # Convert to date
    climate$date = as.Date(as.character(climate$date),
                           format="%m/%d/%Y")
  # Get ordinal date
    climate$day = yday(climate$date)
  # Get year
    climate$year = year(climate$date)
  # Sort climate data by date and year
    climate = climate[with(climate, order(year, day)), ]    
    
# Add up the values to calculate degree
# days for each year
# Split the dataframe up into a list
# with a df for each year
    test = split(climate$ddPrep, climate$year)
  # Replace NA values of temperature with
  # arithmetic mean of 
  # preceding and following elements
    library(zoo)
    test = mapply(na.approx, test)
  # Calculate degree days for each year  
    dd = mapply(cumsum, test)
  # Unlist the result and add it to the climate data  
    climate$dd = unlist(dd)
```

<h4><b> Data merge </b></h4>

We can now add degree days to our fish data. What? Fish data? I forgot we had that!

To wrap up our climate analysis, we will quickly calculate day length at Otsego Lake based on lattitude for each day of our historical records using the `geosphere` package.

```{r, message=FALSE, warning=FALSE}
library(geosphere)
climate$daylight = daylength(lat = 42.76, doy = climate$day)
```

Our final job will be to add all of the climate data to our new dataframe containing walleye counts. This is relatively easy to do in R using the `merge()` function, like so:

```{r}
# First, format the date column in the weye df
  weye$date = as.Date(as.character(weye$date), format="%m/%d/%Y")  

# Merge the two dataframes
  eyes = merge(weye, climate)

# Finally, we are going to get rid of Leatherstocking 
# for now because there are few data points there
  eyes = eyes[eyes$site!='Leatherstocking Creek', ]
  
# Check to see how much data we have left
  nrow(eyes)
```

Wow, that's rough! We went from several hundred lines of data to just a handful pretty quickly!!

<h3><b> Modeling counts </b></h3>

After our data management triathalon, we can finally model walleye counts as a function of some explanatory variables of interest. As has become our practice during the last several lessons, we will do this in both frequentist and Bayesian frameworks.

<h4><b> REML estimation </b></h4>

We start by estimating a model using REML. Let's say for the sake of argument that we are simply interested in the lake-wide mean of our counts so that we know when students should, for example, be heading out to tributaries to look for walleyes in streams. 

For now, we will model walleye count as a function of photoperiod, with a random effect of site on the intercepts. This model assumes that there is variability in counts of spawning individuals between sites, but that the relationship between photoperiod and count is the same across all sites. In this case, we will specify a quadratic relationship between counts and dates because we expect the number of fish to increase to some point in the run before it decreases. We are not interested

In the `lme4` package, the model might look something like this:

```{r, warning=FALSE, message=FALSE}
# Load the package
  library(lme4)

# Make the model
  waeMod1 = glmer(counts~dd + (dd^2) + (1|site), data=eyes, family=poisson)
  
# Have a look-see at the results
  summary(waeMod1)
```

Crap! Our model failed to converge. It looks like this is probably because we have variables on really different scales, and because we have a lot of colinearity between them. So, let's try standardizing our covariates to see what we can do about that:

```{r, warning=FALSE, message=FALSE}
# Standardize photoperiod
  eyes$sdd = scale(eyes$dd) 

# Make the model
  waeMod2 = glmer(counts~sdd + I(sdd^2) + (1|site),
                  data=eyes,
                  family=negative.binomial(theta=1000))
  
# Have a look-see at the results
  summary(waeMod2)
```

Okay, looks like we are doing a lot better with this now.

As we look through these results, we can see that we have a significant effect of degree days on spawning behavior. What's more is that our count of spawning fish appears to increase during the year to a point before it starts to decrease.

Now, if we want, we can make a graph to show these predictions. Here, we make predictions for all years, and then we plot those predictions for a single site (`Shadow Brook`).

You will need to install the `merTools` package for this one before you can run the code below (`install.packages(merTolls)`). You will have to allow it to build from source.

```{r, message=FALSE, warning=FALSE}
# Load the merTools package
  library(merTools)
  
# Make a new dataframe for prediction
  sdd = seq(from = min(eyes$sdd), to = max(eyes$sdd), by = .1)
  site = sort(rep(unique(eyes$site), length(sdd)))
  sdd = rep(sdd, length(unique(eyes$site)))
  newdata = data.frame(sdd)
  
# Simulate predictions from the relationship stored in the model fit using
# our new data
  PI <- predictInterval(merMod = waeMod2, newdata = newdata, 
                        level = 0.95, n.sims = 1000,
                        stat = "median", type="linear.prediction",
                        include.resid.var = TRUE)
  PI = apply(PI, c(1, 2), exp)
  
# Plot the raw data but don't label the x-axis
# because we will want to add unstandardized labels
# even though our regression used standardized labels
  plot(eyes$sdd[eyes$site=='Shadow Brook'],
       eyes$counts[eyes$site=='Shadow Brook'],
       ylim = c(0,500), pch=21,
       bg=c('gray87', 'gray60','gray40', 'black')[as.factor(eyes$year)],
       cex=1.9, xlab='Degree days', ylab='Count',
       xaxt='n'
       )

# Add lines to the plot
  lines(newdata$sdd[site=='Shadow Brook'],
        PI[site=='Shadow Brook',1], lty=2, lwd=2, col='blue') # Mean
  lines(newdata$sdd[site=='Shadow Brook'],
        PI[site=='Shadow Brook',2], lty=2, lwd=2, col='red') # Lower
  lines(newdata$sdd[site=='Shadow Brook'],
        PI[site=='Shadow Brook',3], lty=2, lwd=2, col='red') # Upper
```

We could also do this using our global parameter estimates and some new data. We see that our mean predictions aren't terrible, but there is quite a bit of uncertainty here.

<h4><b> Bayesian estimation </b></h4>

We can fit the same model in the Bayesian framework, too. Here we specify it just as we have during the past couple of weeks.

Let's make sure to standardize covariates first:

```{r}
#eyes = eyes[eyes$year!=2019, ]
eyes$stemp = scale(eyes$mean_c)  # Temperature
eyes$stemp[is.na(eyes$stemp)] <- mean(eyes$stemp, na.rm = T)
eyes$sdd = scale(eyes$dd)        # Degree days
eyes$sdaylight  = scale(eyes$dd) # Daylight/photoperiod
eyes$doy = yday(eyes$date)
eyes$sdoy = scale(eyes$doy)
```

Now write out a model with random intercepts to predict count as a function of some covariate of interest (we'll start with degree day in the example that follows):

```{r, message=FALSE, warning=FALSE}
# Write model
modelstring="
  model {
  
    # Likelihood
    for(i in 1:n){
      count[i] ~ dnegbin(p[i], r)     # The random variable
      p[i] <- r/(r+lambda[i])
      log(lambda[i]) <- mu[i]
      mu[i] <- alpha[site[i]] + beta*X[i] + beta2*(X[i]^2) # Expectation
    }
    
    # Priors
    for (i in 1:ngroups){      
      alpha[i] ~ dnorm(mu.int, tau.int)   # Random intercepts
    }
    
    mu.int ~ dnorm(0, 0.1)       # Mean hyperparameter for random intercepts
    tau.int <- 1 / (sigma.int * sigma.int)
    sigma.int ~ dunif(0, 100)      # SD hyperparameter for random intercepts
    
    beta ~ dnorm(0, taub)          # Common slope
    taub <- 1 / ( sigmab * sigmab)    # Residual precision
    sigmab ~ dunif(0, 100)          # Residual standard deviation
    
    beta2 ~ dnorm(0, taub2)          # Common slope
    taub2 <- 1 / ( sigmab2 * sigmab2)    # Residual precision
    sigmab2 ~ dunif(0, 100)          # Residual standard deviation
    
    r ~ dgamma(0.001, 0.001)          # Site-specific overdispersion

  }
"

# Bundle data
  wae.data <- list(count=eyes$count,
                   site = as.numeric(as.factor(eyes$site)), 
                   X = as.vector(eyes$sdd), # replace covariates here
                   ngroups = length(unique(eyes$site)),
                   n = nrow(eyes)
                   )
  
# Inits function
inits <- function(){
  list(
    alpha = rnorm(length(unique(eyes$site)), 0, 1),
    r = rgamma(length(unique(eyes$site)), 0.1, 0.01),
    beta = rnorm(1, 0, 1),
    beta2 = rnorm(1, 0, 10),
    mu.int = rnorm(1, 0, 1),
    sigma.int = rlnorm(1),
    sigmab = rlnorm(1),
    sigmab2=rlnorm(1)
  )}

# Parameters to estimate
parameters <- c("alpha", "beta", "beta2","mu.int", "sigma.int", "sigmab",
                "sigmab2")

# MCMC settings
ni <-5000
nb <- 2500
nt <- 10
nc <- 3

# Load the package
  library(R2jags)

# Run the Gibbs sampler
  out <-jags(wae.data, inits=NULL, parameters,
             model.file = textConnection(modelstring),
             n.thin=nt, n.chains=nc, n.burnin=nb, n.iter=ni,
             progress.bar = 'text')
  
# Print the results  
  print(out, digits=3)
```

We notice that all parameters converge. Note that if we run this without standardizing our degree day covariate, convergence is not quite as clean. This is likely because of correlations between `dd` and `dd2`. 

One thing you might notice here is the shift in our upper 95% credible interval around the posterior prediction. This is pretty typical of phenology data, and is caused by the fact that we don't really sample much once we stop catching fish. Something to consider when designing your own studies...


<h3><b> Your mission </b></h3>

If you choose this homework option, I would like you to do the following:

\item 1. Re-run the code block above, replacing `sdd` in `wae.data` with each of the other standardized covariates from `eyes` (`stemp`, `sdaylight`, and `sdoy`). For each model, I want you to get the DIC and check convergence (do this one first).

\item 2. Then, using the best model DIC, please make predictions from the model coefficients. Below is an example using the degree day model above. You can use the predictions about walleye counts vs degree days to find out the approximate date of first spawn by looking up the date that corresponds to minimum degree day where predicted counts > 0 in the climate data (or `temp`, `daylight`, `doy`).

\item 3. Submit a brief writeup describing methods and results.

```{r}
# Make a new sequence of standardized degree days  
  sdd = seq(-3, 3, .1)

# Make prediction from the model parameters
  fit = exp(
    mean(out$BUGSoutput$sims.list$mu.int)+
    mean(out$BUGSoutput$sims.list$beta)*sdd+
    mean(out$BUGSoutput$sims.list$beta2)*(sdd^2))

  lcis = exp(
    quantile(out$BUGSoutput$sims.list$mu.int, probs=.025)+
    quantile(out$BUGSoutput$sims.list$beta, probs=.025)*sdd+
    quantile(out$BUGSoutput$sims.list$beta2, probs=.025)*(sdd^2))
  
  ucis = exp(
    quantile(out$BUGSoutput$sims.list$mu.int, probs=.975)+
    quantile(out$BUGSoutput$sims.list$beta, probs=.975)*sdd+
    quantile(out$BUGSoutput$sims.list$beta2, probs=.975)*(sdd^2))
  
# Plot the predictions
  par(mar=c(5,5,1,1))
  plot(eyes$sdd, eyes$counts,
       pch=21, bg=c('white', 'gray', 'black')[as.factor(eyes$site)], cex=1.2,
       ylim=c(0, max(eyes$counts)+50),
       xlim=c(-2.9, 3),
       xlab='Degree days (C)', 
       ylab='Count of spawners',
       xaxt='n', yaxt='n')
  lines(sdd, fit, lwd=2, lty=1, col='black')
  lines(sdd, lcis, lwd=2, lty=2, col='gray40')
  lines(sdd, ucis, lwd=2, lty=2, col='gray40')

# Add a new x-axis 
  axis(side=1,
         at=(seq(0,300,50)-mean(eyes$dd))/sd(eyes$dd),
         labels= 
           #Get dd on original scale from a sequence
           # of new standardized values
           seq(0,300,50)
         )
  
# Add a rotated y-axis
  axis(2, las=2)
```
 
### <span style="color:gray"><b> **Plant surveys** </b></span>

<h2 id="multi"><b> Big milfoil problems </b></h2>

For this example, we will look at increases or decreases in Eurasian watermilfoil (*Myriophylum spicatum*) following herbicide treatment at different doses. We will use a Bernoulli response (1 or 0) to test effects of turbidity on successful achievement of plant control across 30 sites.

<h3 id="multi"><b> Data </b></h3>

Start by reading in the data:

```{r, eval=FALSE, echo=TRUE}
# Read in the plant data
  plants = read.csv("classData-master/plants2.csv")
```

```{r, echo=FALSE, eval=TRUE}
# Read in the plant data
  plants = read.csv("../../data/plants2.csv")
```

This is a pretty straightforward data set compared to the walleye phenology set (hopefully that is not what led you here). Here, we have samples of Eurasian watermilfoil control from each of 30 experimental waterbodies treated with an herbicide at 5 different conductivities. Those data have been condensed to indicate whether the species increased or decreased at each site following herbicide application. Here, we will investigate the influence of conductivity (an index for turbidity where greater values indicate more turbid water) on this response, while accounting for random variation between sites.

<h3 id="multi"><b> Estimation with REML </b></h3>

To start with, we will fit the model using REML. Here, we need to remember to specify a site-specific random effect on the intercept and we need to give R the `family` for our link function.

```{r}
# Fit the model and store it to an object
  plantMod1=glmer(success~conductivity+(1|site),
                  data=plants, family=binomial)

```

Crud! The first thing we get smacked with is a warning about predictor (explanatory) variables being on very different scales. This is because the variable we are using (conductivity) varies across a huge range. That causes numerical problems during estimation with the algorithm being used.

Let's scale and center (standardize) the conductivity variable
```{r}
# Scale and center the variable
  plants$scond <- as.vector(scale(plants$conductivity))

# Fit the model and store it to an object
  plantMod1=glmer(success~scond+(1|site),
                  data=plants, family=binomial)
```

After a quick look at the results, we see that turbidity has a significant, negative effect on the probability of successful control following this herbicide application.

We could make a graph of these results as follows:

```{r}
# Get the fixed-effects coefficients from the model
  f.coefs <- summary(plantMod1)$coefficients

# Make a function to invert the logit
  invlogit=function(x){exp(x)/(1+exp(x))}

# Now we can make predictions with a sequence over 
# the range of observed values using y = mx + b
  newCond = seq(-3, 3, 0.10)
  preds = f.coefs[1,1] + f.coefs[2,1]*newCond
  
# Get upper and lower 95% CI for predictions, too!
  lwr = (f.coefs[1,1]-1.96*f.coefs[1,2]) + (f.coefs[2,1]-1.96*f.coefs[2,2])*newCond
  upr = (f.coefs[1,1]+1.96*f.coefs[1,2]) + (f.coefs[2,1]+1.96*f.coefs[2,2])*newCond
  
# Make a quick plot, inverting the logit function for 
# predictions on the fly
  plot(newCond, invlogit(preds), type='l', lwd=2, col='white',
       xlab = 'Standardized conductivity',
       ylab = 'Probability of successful control',
       yaxt = 'n', ylim = c(0, 1)
       )
  axis(side=2, las=2)
# Add lower and upper prediction of success by conductivity
# also inverting logit on the fly
  polygon(x = c(newCond, rev(newCond)), y = c(invlogit(lwr), rev(invlogit(upr))),
          col = rgb(0.8, 0.8, 0.8, 0.5), border = NA
          )
  lines(newCond, invlogit(preds), lty=1, lwd=1, col='black')

```

We can see that most of the uncertainty in this relationship occurs at higher `conductivity`, and the rest of the pattern is much what we would expect based on the value of `beta` from our model.

If we wanted to make predictions for one or more of the sites, we could do so using the `merTools` package as demonstrated in the walleye phenology example (check it out for the homework!).


<h3 id="multi"><b> Bayesian estimation </b></h3>

Now, we will estimate a model with random slopes using Bayesian methods.

```{r}
modelstring="
      model{

        # Priors
          for(i in 1:ngroups){
            alpha[i] ~ dnorm(mu.int, tau.int) # Random intercepts
          }

          mu.int ~ dnorm(0, 0.001)            # Mean hyperparameter for ran. intercepts
          tau.int <- 1/(sigma.int*sigma.int)  # Precision for random intercepts
          sigma.int ~ dunif(0, 100)           # SD hyperparameter for ran. intercepts

          beta ~ dnorm(0, 0.001)              # Common slope for beta

        # Likelihood
          for(i in 1:N){
            y[i] ~ dbern(mu[i])
            logit(mu[i]) <- alpha[site[i]] + beta*X[i]
          }
        }
"
```

> Make the data 

```{r}
# Package the data in a list for JAGS
    jags.data <- list(
      y = plants$success,
      site = plants$site,
      X = plants$scond,
      ngroups = length(unique(plants$site)),
      N = nrow(plants)
    )
```

> Define parameters for monitoring

```{r}
# Specify the parameters we want to monitor
    parameters = c("alpha", "beta", "mu.int", "sigma.int")
```

> Specify initial values

```{r}
# Make a function to declare some initial values.
    inits = function(){
      list(
        alpha = rnorm(length(unique(plants$site)), 0, 2),
        beta = rnorm(1,1,1),
        mu.int = rnorm(1,0,1),
        sigma.int = rlnorm(1)
      )
    }
```

> Define MCMC settings for Gibbs sampler

```{r}
# MCMC settings
  ni <- 15000
  nt <- 5
  nb <- 5000
  nc <- 3
```

> Run the model

```{r, eval=FALSE, message=FALSE, warning=FALSE}
# Load the R2jags package
  library(R2jags)

# Call JAGS from R and run the model
   plant_glmm <- jags(jags.data, inits, parameters,
     model.file = textConnection(modelstring),
     n.chains = nc, n.thin = nt, n.iter = ni,
     n.burnin = nb)
```

```{r, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R2jags package
  library(R2jags)

# Call JAGS from R and run the model
   plant_glmm <- jags(jags.data, inits, parameters,
     model.file = textConnection(modelstring),
     n.chains = nc, n.thin = nt, n.iter = ni,
     n.burnin = nb, progress.bar = 'none')
```

> Look at the results

```{r}
# Print the model
  print(plant_glmm, digits=3)
```

This output can be a bit daunting because of the `alpha` overload. Here, we have a unique `alpha` for each of our initial 30 sites. We have a single `beta` describing the change in probability of `success` as a function of `conductivity`.

If we just want the overall change in probability of success as related to conductivity, then will want to work with our global mean, `mu.int` for now. For this homework option, I will ask you to pick one of the sites to make predictions using one of the `alpha` parameters instead of `mu.int`.

```{r}
# Get the posteriors
posts = plant_glmm$BUGSoutput$sims.list

# Have a look at them
names(posts)
```

Get the regression coefficients:

```{r}
# Intercept estimates
mu.int <- posts$mu.int 

# Slope estimates
beta <- posts$beta 

```

Now that we have our coefficient estimates, we can make predictions using our good old friend $y = mx + b$, or `pred = mu.int + beta * scond`. We have to step up our coding game a little here to make one prediction for each pair of parameter estimates stored in our posteriors.

We start by making a new sequence of our `X` variable (`conductivity`)

```{r}
# Make up some new values of our
# standardized covariate (conductivity)
newcond = seq(-3, 3, .5)

```

Now, we will make an empty matrix with the same number of rows as our MCMC samples in `alpha` and `mu.int` and a number of columns corresponding to the number of elements in `newcond`. 

```{r}
# Make an empty matrix to hold predictions
preds = matrix(NA, nrow=length(mu.int), ncol=length(newcond))

```
Here comes the tricky part. We are going to loop over the rows and columns in the matrix, pulling new values of `alpha` and `mu.int` as well as new values of `newcond` in a nested, sequential way. Note that `alpha` and `mu.int` are indexed by `i` and `newcond` is indexed by a `t`.

```{r}
for(i in 1:nrow(preds)){
  for(t in 1:ncol(preds)){
    preds[i,t] <- mu.int[i] + beta[i]*newcond[t]
  }
}

```

Now, let's calculate some summary statistics from the joint posterior prediction. First, we will get the mean and and 95% CRI for probability of `success` for each new observation of `conductivity`.
```{r}
pred.mean <- invlogit(apply(preds, 2, mean))
pred.lwr <- invlogit(apply(preds, 2, quantile, 0.025))
pred.upr <- invlogit(apply(preds, 2, quantile, 0.975))

```

```{r}
# Make a quick plot, inverting the logit function for 
# predictions on the fly
  plot(newcond, pred.mean, type='l', lwd=2, col='white',
       xlab = 'Standardized conductivity',
       ylab = 'Probability of successful control',
       yaxt = 'n', ylim = c(0, 1)
       )
  axis(side=2, las=2)
# Add lower and upper prediction of success by conductivity
# also inverting logit on the fly
  polygon(x = c(newcond, rev(newcond)), y = c(pred.lwr, rev(pred.upr)),
          col = rgb(0.8, 0.8, 0.8, 0.5), border = NA
          )
  lines(newcond, pred.mean, lty=1, lwd=1, col='black')

```

Notice here that the variance around our predictions at high values of standardized `conductivity` is much better estimated than in our REML fit above. 


<h3 id="multi"><b> Your mission </b></h3>

For this option, I want you to do the following:

\item 1. Using either REML or Bayesian estimation, first run the model out to convergence. Make sure diagnostics idicate that estimates have converged and effective sample size (ESS, or `n.eff`) is sufficiently large if using Bayesian.

\item 2. Make predictions for at least one individual site using one set of `alpha` estimates from the model. If using REML, you can see an example of how to do this in the walleye phenology homework option.

\item 3. Submit a brief writeup describing methods and results.

<br>
