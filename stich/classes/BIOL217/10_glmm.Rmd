```{r child="../../_styles.Rmd"}
```

<h1 id="multi"> Linear mixed models (LMM) <h1>

<img src="../../images/yp.jpg" alt="">

<h2 id="multi"> Introduction </h2>

This week we will talk about extending linear models and generalized linear models to include "random effects" in the model, thus resulting in the "generalized linear mixed model" or GLMM. The GLMM is actually the most generalized formulation of the linear models that we have been discussing now for the past several weeks. All linear models (GLM, ANCOVA, ANOVA, regression, t-tests, etc.) are special cases of the GLMM. As such, we can think of the GLMM as the framework within which we have been working for weeks now! For this week, we will start with examples of the linear mixed model.

## Assumptions of linear models

OMG, why is this guy always talking about assumptions of linear models no matter what we do?!

Just as we discussed last week, linear models are just a special case of the GLMM. That is, the linear model assumes a certain error distribution (the normal) that helps things work smoothly and correctly. During the last two weeks, we discussed how we can use link functions to relax the assumption of linear models with respect to normality of residuals and homogeneity of variances, as well as assumptions about the linearity of relationships between explanatory variables and responses of interest by using data transformation. This week, we continue to relax the underlying assumptions of linear models to unleash the true power of  estimation in mixed effects models. This is essentially as far as the basic framework for linear modeling goes (with the exception of multivariate techniques), and all other cases (e.g. spatial and temporal autocorrelation regressions) are simply specialized instances of these models.

Let's take another look at the assumptions of linear models. We will repeat the same mantra from the past few weeks. Here are the three assumptions that we explicitly use when we use linear models (just in case you've forgotten them):

\item 1. Residuals are normally distributed with a mean of zero

\item 2. Independence of observations (residuals)

    +    Colinearity
    +    Auto correlation of errors (e.g., spatial & temporal)

\item 3. Homogeneity

\item 4. Linear relationship between X and Y

### Assumption 1: Normality of residuals

We've seen these before, but let's recap. For assumption 1, we are assuming a couple of implicit things: 1) The variable is *continuous* (and it must be if it's error structure is normal), and 2) The error in our model is normally distributed. 

In reality, this is probably the least important assumption of linear models, and really only matters if we are trying to make predictions from the models that we make, or when we are in gross violation of the assumption. Of course, we are often concerned with making predictions from the models that we make, so we can see why this might be important. However, more often we are in extreme violation of this assumption in some combination with assumption 4 above to such a degree that it actually does matter. For example, a response variable that is binomial (1 or zero) or multinomial in nature cannot possibly have normally distributed errors with respect to x unless there is absolutely no relationship between X and Y, right? So, if we wanted to predict the probability of patients dying from some medical treatment, or the presence/absence of species across a landscape then we can't use linear models. This is where the link functions that we have been discussing really come into play. The purpose of the link function is to place our decidedly non-normal error structures into an asymptotically normal probability space. The other key characteristic of the link function is that it must be invertible, that way we can get back to the parameter scale that we want to use for making predictions and visualizing the results of our models.

### Assumption 2: Independence of observations

This time we've broken assumption 2 in two components: Colinearity and autocorrelation of errors. Remember that the manifestation of these problems has primarily been in the precision of our coefficient estimates so far. This leads to the potential for change in the Type-I/II error rates in our models, causing us to draw false conclusions about which variables are important. As we discussed earlier in the course we expect to see some colinearity between observations, and we can deal with balancing this in our modeling through the use of model selection techniques to reduce Type-I and Type-II error. During the past couple of weeks, we examined tools that help us determine whether or not colinearity is actually causing problems in our models that go beyond minor nuisances. As for the second part, autocorrelation, we briefly touched on formulations of the GLM in our readings that included auto-regressive correlation matrices to relax this assumption of linear models and improve the precision of parameter estimates. This week, we will further extend this to include random effects so we can account for non-independence in the observations, and correlation in the residual errors of explanatory variables that could otherwise cause issues with accuracy and precision of our estimates. We will continue to use model selection as a method for determining tradeoffs between information gain and parameter redundancy that results from colinearity between explanatory variables, as well as for hypothesis testing.

### Assumption 3: Homogeneity of variances

In past weeks, we looked at ways to reduce this issue by introducing blocking (categorical) variables to our models. Last week, we noted that this could be further mitigated through the use of weighted least squares and MLE within the GLM framework, which can be applied to a wide range of regression methods from linear models to GLMs and GLMMs. This week we will examine how we can use various formulations of the GLMM to account for heteroscedasticity in residual errors directly by including the appropriate error terms in our models. This essentially means that we can start to account for things like repeated measures, nested effects, and various other violations through the use of one tool...**nifty**!!

### Assumption 4: Linearity and additivity

We've already looked at a couple of ways to deal with violations of these assumptions such as data transformation and/or polynomial formulations of the linear model. We will continue to apply these concepts this week as we begin to investigate the GLMM as robust framework for analysis.

## Introducing the GLMM

The first thing you should understand about GLMMs is that they are useful for analyzing data from a large number of distributions (basically, you can use them for any underlying error structure). But, when we use specific error structures, or make certain assumptions about the manner in which the heterogeneity of variances is structured with respect to specific factors, this model is often given specific names. For example, repeated measures ANOVA (or ANCOVA), nested ANOVA (or ANCOVA), factorial ANOVA (or ANCOVA), linear mixed models, linear mixed effects models, and generalized linear mixed effects models are all just different formulations of the GLMM with different names. It sounds confusing, but just remember this: any linear model with combinations of fixed and random effects is, at it's core, just another GLMM! If you can convince yourself of this, you will improve your ability to understand a wide range of experimental designs and accompanying statistical models by understanding this one model type.

The second thing you should understand to "get" GLMMs is what exactly is meant by a "random effect". So far in this course we have only dealt with "fixed" effects. The fixed effect is a categorical variable that is used to explain some variation in our response of interest. When we use a fixed effect in a statistical model, we are making the assumption that the categories for this effect are "fixed". In other words, we have assigned the the levels, or categories, based on some *a priori* knowledge that the levels themselves represent all possible groups that can be used to describe the data. Because of this definition, fixed effects are usually 1) things that we manipulate directly (like dosage or some other treatment), or 2) relatively simple grouping variables such as sex. By contrast, a "random effect" is an effect that we do not generally set ahead of time or manipulate, but rather one which is considered to be a sample from a population of potential categories that we cannot census or (often) control. Please note that there is not a single, widely accepted definition for either of these things in applied statistics and the definition can be context-specific. It becomes all the more confusing when we switch between maximum likelihood estimation and Bayesian inference. Don't take it from me, though. Ask one of the world's leading experts on the matter [here](http://andrewgelman.com/2005/01/25/why_i_dont_use/).

## Linear mixed models

We will start our explorations into GLMM by looking at the somewhat familiar case of "normal" data, whatever mythical meaning it may have. As with the relationship between ANOVA and GLM, we can say that the linear mixed model (LMM) is just a special case of the GLMM (hence the name), both of which belong to the group of multi-level or hierarchical models that house basically every kind of model we have looked at this semester.

So, what is a mixed model? This is a model that, generally speaking, assumes at least one parameter of interest is drawn from a population of potential sample sets. We usually use these when we are dealing with repeated samples for some group or individual, or if we wish to account for some latent variable beyond our control (e.g., lake or year). The use of random effects allows us to remove extraneous noise (variance) from the study system by explicitlty accounting for it. This can improve both the accuracy and the precision of estimates to make hypothesis testing on other explanatory variables more robust. It also allows us to generalize our conclusions to a broader scope (e.g. any lake instead of lakes X, Y, and Z). 

Beyond these mundane uses, a "multi-level" approach to modeling allows for a great deal of flexibility in assumptions we make about the effects and associated errors in our model. We might assume within our model that effects are different between populations by assigning random intercepts and/or slopes. We can specify whether we think the influence of a continuous covariate is correlated with the starting point (correlated random slopes and intercepts). There are even rare cases when we might wish to examine random slopes with shared intercepts or vice versa. In Bayesian inference we can use information at higher levels of organization, like the North American Range of a species, to inform parameter estimation at lower levels, such as individual populations. 

The point here is that random effects on a given parameter need not be a "nuisance" for which we wish to account:  it may be the very thing we wish to harness for inference, estimation, or prediction.

As with so many things, these tools are often best investigated through the use of a worked example. Generally speaking, we want the grouping variable we use to specify random effects to contain a relatively large number of potential levels (usually > 5, but often > 10) as this tends to result in more accurate, and more precise parameter estimates. We will look at a case to start in which we use fewer for the sake of demonstration.


## Worked example

Let's start by loading in a new data set. These data come from a preliminary study of rusty crayfish <a href="https://academic.oup.com/jcb/article/37/5/615/4060680"> *Faxonius*</a> *rusticus* density in various tributaries to the Susquehanna River. The data were collected as part of a summer research project by one of our former high-school interns at the SUNY Oneonta <a href="https://suny.oneonta.edu/biological-field-station"> Biological Field Station </a>.

```{r, echo=TRUE, eval=FALSE}
# Read in the data file
# We are reading it in with the optional
# argument because we don't want to deal
# with code relicts if we start
# dropping factor levels
  cray <-  read.csv("cray.csv", stringsAsFactors = FALSE)
```

```{r, echo=FALSE}
# Read in the data file
# We are reading it in with the optional
# argument because we don't want to deal
# with code relicts if we start
# dropping factor levels
  cray <-  read.csv("../../data/cray.csv", stringsAsFactors = FALSE)
```

```{r}
# Look at the data structure
  str(cray)
```  
  
```{r}  
# And, have a look at the first few lines of data
  head(cray)
```

We have a fairly straightforward data set this week. There are `r nrow(cray)` observations of 7 variables. Each of the observations (rows) corresponds to a rusty crayfish that was collected and measured (`length` in mm and `mass` in g) at one of several `site`s on a given `date`. The variable `catch` is the total number caught by electrobugging over a given `time` (minutes). To compare density between sites, `catch` was divided by (`time`/60) to calculate catch per unit effort (`cpue`) as number of crayfish per hour. Therefore, observations of `cpue`, `catch`, and `time` correspond to unique `date` and `site` combinations, but `length` and `mass` represent unique individuals within `site` and `date`.

Our primary objective in this study was to collect baseline data. But curiousity led us to explore variation in the condition of crayfish when we thought we were noticing skinnier crayfish in sites of higher density. Length-weight regressions are one tool that is commonly used to investigate changes in volumetric growth with increasing length. In the absence of a standardized condition metric such as that widely applied in fish populations, relative weight (W~r~), we thought this might offer some insight into variability in condition. 

What follows is a steroid-injected version of the analysis we started with.

Length-weight relationships in animals are generally parameterized by log-transforming length and mass. This is because the relationship between the two is exponential, or can be described using a power functino (ouch, think back to intro bio). For a given unit increase in length, we expect mass to increase as an approximately cubic function of length.

We can see this in our un-transformed data pretty cleanly:

```{r}
# Plot the raw data
plot(x = cray$length,
     y = cray$mass,
     pch = 21,
     bg = rgb(.8,.8,.8,.35),
     col = rgb(.8,.8,.8,.05),
     xlab = 'Length (mm)',
     ylab = 'Mass (g)'
     )
```

The relationship depicted above can be expressed mathematically as:

$$W = aL^b,$$

and statistically as:

$$W_i = a {L_i}^b e^{\epsilon_i},$$

where $W_i$ is mass (weight) of individual $_i$, $L_i$ is length, $a$ and $b$ are the coefficient and exponent describing change in mass as a power function of length, and $\epsilon_i$ is the multiplicative error term for each crayfish (residuals change with length). Here, $b$ also has interpretation relative to allometry in growth patterns, where values of 3.0 indicate isometry, values below 3 indicate negative allometry, and values above 3 indicate positive allometry in the length-weight relationship. This means that at values much above 3, we would expect individuals to get heavier faster relative to their length at large sizes.

Now, we can certainly estimate this kind of relationship using nonlinear regression. But, nonlinear regression can get a little unstable due to inherent correlations between parameters, and the multiplicative error described above. So, it can be easier to log-transform both sides of the equation to make the relationship linear and achieve a constant error across the range of x (homoscedasticity). As a result, we generally linearize relationships like this to improve modeling whenever we can, in this case by taking the natural logarithm of both sides of the equation:

$$log(W_i) = log(a) + b \cdot log(L_i) + \epsilon_i$$

Now, that should look a whole lot like the linear models we have been talking about all semester. In fact, by substitution, we could say that:

$$Y = \beta_0 + \beta_1 \cdot X_i + \epsilon_i$$

where $Y$ is `log(mass)`, and $X$ is `log(length)`. Then, we just need to remember that $\beta_0$ is estimated on the log scale.

We can take a look at how this looks in our data by plotting the transformed data.

Start by log-transforming length and mass

```{r}
# Log transform length and mass
  cray$loglength <- log(cray$length)
  cray$logmass <- log(cray$mass)
```

Plot the relationship. Note that only the names have been changed

```{r}
# Plot the raw data
plot(x = cray$loglength,
     y = cray$logmass,
     pch = 21,
     bg = rgb(.8,.8,.8,.35),
     col = rgb(.8,.8,.8,.05),
     xlab = expression(paste('log '[e],'Length (mm)')),
     ylab = expression(paste('log '[e],'Mass (g)'))
     )
```

If nothing else, this tells me we need to go after more small cray this summer. For now, let's get rid of all crayfish weighing less than 1 g because the data are sparse down there and a quick glance at our residuals withing groups will show that this results in non-normality in the residuals for the Butternut Creek site, specifically. Have a look:

```{r}
# Boxplot of mass by site
  boxplot(logmass~site, data=cray)
```

```{r}
# Drop the measurements for crayfish < 1 g
  cray <- cray[cray$mass > 1, ]
  boxplot(logmass~site, data=cray)
```

Wow, what a mess. Much better now, at least. Moving on with the analysis...

Next, we will take a look at a few different ways to analyze these data using maximum likelihood estimation and Bayesian inference. Our goal here is to estimate the relationship between length and mass while accounting for inherent variability between populations.

### Random-intercepts model

First, we will analyze the data assuming that the intercepts for our linear model can vary between populations, but the relationship between length and mass is the same across all populations. This is a very common approach in many ecological and biological applications, as it often is the case that we are just trying to account for sampling design when we do this kind of analysis.

This is really straightforward to do in R. First, we will load the `lme4` package.
```{r warning=FALSE, message=FALSE}
# Load the package
  library(lme4)
```

Next, we fit the model. Notice that now we have to add an argument to speficy a random effect of `site` on the intercept `1`.

```{r}
# Fit the model
  craymod <- lmer(logmass ~ loglength + (1|site),
               data=cray
               )
```

Being responsible individuals, we now have a look at the residuals to make sure we've met assumptions of normality and homoscedasticity:

```{r}
# Plot the residuals
  plot(craymod)
```

That's looking about as good as we could hope for, so now let's go ahead and crack open the model summary.

```{r}
# Print the summary
  summary(craymod)
```

For now, we will skip the usual walkthrough of all the wonderful tidbits that R has to offer and cut right to the chase. We can see from the table for our fixed effects that we have successfully detected the relationship between length and mass (p < 0.05), but this should come as no surprise based on the plot we saw.

We can see that the estimated sd for our intercept is fairly low, so we may not need to specify this as a random effect were we concerned about model complexity. Given that we are interested in this random effect, and that we (in this case) want to think of streams having been sampled from a broader population of streams, we will retain it in our model. From here, we could go on to make predictions across populations using our fixed intercept and slope, or we could use the population specific intercepts and the shared slope.

First, let's plot using the shared intercept from our fixed effects table:

```{r}
# Start by making a sequence of new lengths
# on the log scale
  lens <- seq(from = min(cray$loglength),
              to = max(cray$loglength),
              by = .1)

# Now, get the fixed effects
# coefficients from the model
  fcoeffs <- fixef(craymod)

# We can make the mean predictions
# fairly easily here
  pred <- fcoeffs[1] + fcoeffs[2] * lens
    
# And we could plot them on the log scale
# Plot the raw data
plot(x = cray$loglength,
     y = cray$logmass,
     pch = 21,
     bg = rgb(.8,.8,.8,.35),
     col = rgb(.8,.8,.8,.05),
     xlab = expression(paste('log'[e],'Length (mm)')),
     ylab = expression(paste('log '[e],'Mass (g)'))
     )
 lines(x=lens, y=pred, col='blue')
```

Of course, we could also plot these data on the real scale if we were concerned about actually describing the trend of interest, and at the population level, we can use the diagonal element of the variance-covariance matrix to represent uncertainty in our predictions:

```{r}
# We can make the mean predictions
# fairly easily here
  pred <- fcoeffs[1] + fcoeffs[2] * lens

# Now add the upper and lower confidence
# intervals for the relationship
  sds <- diag(vcov(craymod))
  lpred <- (fcoeffs[1]-1.96*sds[1]) + (fcoeffs[2]-1.96*sds[2]) * lens
  upred <- (fcoeffs[1]+1.96*sds[1]) + (fcoeffs[2]+1.96*sds[2]) * lens
    
# Plot the raw data
plot(x = cray$length,
     y = cray$mass,
     pch = 21,
     bg = rgb(.8,.8,.8,.35),
     col = rgb(.8,.8,.8,.05),
     xlab = 'Carapace length (mm)',
     ylab = 'Mass (g)'
     )
 lines(x = exp(lens), y = exp(pred), col = 'blue')
 lines(x = exp(lens), y = exp(lpred), col = 'red', lty = 2)
 lines(x = exp(lens), y = exp(upred), col='red', lty=2)
```

It is even simple enough for us to dig into the model and extract population-level intercepts here.

```{r}
# Get population-level parameters
  rcoeff <- coef(craymod)$site

# Have a look
  rcoeff
  
```

We can then go on to use each set of parameters shown here to make mean predictions for each stream. If want to plot the results for population 3, for example, our code would look like this:

```{r}
# Plot the raw data
  plot(cray$loglength[cray$site=="Colliersville"],
       cray$logmass[cray$site=="Colliersville"],
       ylim = c(0, 3), pch=21, bg='gray87',
       cex=1.25,
       xlab = expression(paste('log'[e],'Length (mm)')),
       ylab = expression(paste('log '[e],'Mass (g)'))
       )
# Make a sequence of new lengths
  lens = seq(min(cray$loglength), max(cray$loglength), by=.01)
# Predict new values for mass from lens
  masses = rcoeff[2,1] + rcoeff[2,2]*lens
# Add lines to the plot
  lines(lens, masses, lty=1, lwd=2, col='blue')
```

But, we still don't really have a great way of looking at differences between groups if we are interested in those. Why is this? We do not have the technology. Basically, computing group-specific variances is conceptually and programmatically challenging. But, we can use some simulation methods to do this, and some of these have been implemented in newer versions of the `lme4` package and related packages.

IMO, if you are going to go through simulations just to approximate confidence intervals, you are probably interested in the group-level estimates as well, and you should really be thinking about Bayesian approaches at this point.

The following method was released in December 2016 to meet the growing need for characterizing uncertainty in group-level predictions from mixed models. Let's take a look at how to use some of these tools.

```{r, warning=FALSE, message=FALSE}
library(merTools)

# Tell r which population we are 
# working with. We will store as a variable
# upfront so we can easily change populations
# by re-defining the variable, which I will
# call `i`. We will Colliersville to stay
# consistent with the example above
  i='Colliersville'
  
# Make a sequence of new lengths
  lens = seq(min(cray$loglength),
             max(cray$loglength),
             by=.1)
  
# Make a df that contains lengths
# and a label for population
  newd = data.frame(
    site = rep(i, length(lens)),
    loglength = lens
  )

# Simulate predictions from the relationship
# stored in the model fit using our new data
  PI <- predictInterval(merMod = craymod, newdata = newd, 
                        level = 0.95, n.sims = 10000,
                        stat = "median", type="linear.prediction",
                        include.resid.var = TRUE
                        )

# Plot the raw data for the population of interest, i
  plot(x = cray$loglength[cray$site==i], 
       y = cray$logmass[cray$site==i],
       ylim = c(0,3), pch=21, bg='gray87',
       cex=1.25,
       xlab = expression(paste('log'[e],'Length (mm)')),
       ylab = expression(paste('log '[e],'Mass (g)'))
       )

# Predict new values for mass from lens
  #masses = effs$pop[i,1] + effs$pop[i,2]*lens
# Add lines to the plot
  lines(newd$loglength, PI$fit, lty=1, lwd=2, col='blue') # Mean
  lines(newd$loglength, PI$upr, lty=1, lwd=2, col='red')  # Upper CI
  lines(newd$loglength, PI$lwr, lty=1, lwd=2, col='red')  # Lower CI
```
 
We could go through and do this for each site, and we would have a pretty nice (albeit slightly unwieldy) set of figures describing the relationship between length and mass. Alternatively, we could use the model coefficients to estimate the relationship across all populations, and to make predictions about unknown populations.

 
## Binary (logistic) regression

For our second example this week, we will use the `choice` data from a couple weeks ago that we used to demonstrate binomial logistic regression. This time, we will add in a random intercept term that will allow us to account for repeated observations within a year. This has two implications: 1) it accounts for the fact that the years in which we conducted this study are random samples from a larger, unobserved population, and 2) it accounts for the heterogeneity of variance that theoretically might occur as a result of taking multiple, and variable, numbers of measurements within a given year- thereby reducing the overall error of the model and our associated parameter estimates (in theory).

```{r, eval=FALSE, echo=TRUE}
# Let's read in the smolt data set that we used last time
  choice = read.csv('StillwaterChoiceData.csv')

# Look at the first few rows of data
  head(choice)
```

```{r, eval=TRUE, echo=FALSE}
# Let's read in the smolt data set that we used last time
  choice = read.csv('../../data/StillwaterChoiceData.csv')

# Look at the first few rows of data
  head(choice)
```

 
### Data Explanation

These data are from a study that examined factors affecting path choice by wild and hatchery-reared endangered Atlantic salmon smolts during seaward migration in the Penobscot River, Maine. State, local, and federal fishery managers were interested in understanding what factors affected migratory routing through the lower river because there were different numbers of dams, with different estimated smolt mortality rates, on either side of a large island hydropower project in this system. If managers could understand factors influencing migratory route, they might be able to manipulate flows, stocking dates, and dam operation to improve survival of these endangered fish. Furthermore, the results of the study were used to predict the effects of dam removal, and hydropower re-allocation in the lower river on population-level consequences for these fish. Please see the <08_glm_logisticRegression>logistic regression module</a> for a complete explanation of the data.

 
### Data analysis

We are going to use the 1/0 binary data to estimate the effects of a number of covariates of interest on the probability that an individual fish used the Stillwater Branch for migration in each year of this study using logistic regression. In order to do this, we will use the 'logit' link function, which can be defined as:
   
```{r}  
logit = function(x){
  log(x / (1-x))
}
```

The inverse of the logit function is:
 
```{r}
invlogit = function(x){
  exp(x) / (1 + exp(x))
}
```

Since we are not interested in the linear trend in the use of the Stillwater Branch through time, we need to convert year to factor. This is the same as if we wanted to use this as a fixed effect in the model.

```{r}
choice$year = as.factor(choice$year)
```

Next, define a set of models based on a priori combinations of explanatory variables.
```{r, warning=FALSE, message=FALSE}  
# First, make an empty list to hold the models
  mods=list()

# Now, fill the list with several a priori models
# Need to load the `lme4` package for the `glmer` function
  library(lme4)
# Here is the list
  mods[[1]]=glmer(path~(1|year)+hatchery+length+flow,family=binomial, data=choice)
  mods[[2]]=glmer(path~(1|year)+flow,family=binomial,data=choice)
  mods[[3]]=glmer(path~(1|year)+hatchery,family=binomial, data=choice)
  mods[[4]]=glmer(path~(1|year)+length,family=binomial, data=choice)
  mods[[5]]=glmer(path~(1|year)+length+hatchery,family=binomial,data=choice)
  mods[[6]]=glmer(path~(1|year)+length+flow,family=binomial,data=choice)
  mods[[7]]=glmer(path~(1|year)+hatchery+flow,family=binomial,data=choice)
```


Give the models some names using the formulas for each of the models. **Remember**: models are stored as list objects in R, and each of those list objects (models) has names. We can reference those names using the `$` notation:

```{r}
  for(i in 1: length(mods)){
    names(mods)[i] = as.character(summary(mods[[i]])$call$formula)[3]
  }
```

Now, we use the `AICcmodavg` package to make a model selection table like we did last week:

```{r, warning=FALSE, message=FALSE}  
# Load the package and make the table
  library(AICcmodavg)
  modtable = aictab( cand.set = mods, modnames = names(mods) )
```

Finally, we can use these models to make predictions about the relationships in our models the same way we have done previously with linear models and GLMs.

```{r}
# Start by making a new sequence of values from which to make predictions
  newFlow = seq(min(choice$flow), max(choice$flow), 1)

# Let's go ahead and use the best model to make some predictions, as we
# can see that flow is the most important variable here
  beta_0 = summary(mods[[2]])$coefficients[1,1]
  beta_1 = summary(mods[[2]])$coefficients[2,1]

# Now, we can make predictions from this model-averaged estimate
  preds = invlogit(beta_0 + beta_1*newFlow)

# Make a *quick* plot for data vis
  plot(newFlow, preds, type='l')
```


## Count models 

### Walleye in spring
We will wrap up our discussions about GLMM with a worked example of count models. For this one, we will attempt to predict counts of walleye, *Sander vitreus*, in spawning streams of Otsego Lake based on historical counts and climate data.

#### Data

We begin by reading in the data set:

```{r, echo=TRUE, eval=FALSE}
# Read in the walleye data
  eyes = read.csv('walleye.csv', stringsAsFactors = FALSE)
```

```{r, echo=FALSE, eval=TRUE}
# Read in the walleye data
  eyes = read.csv('../../data/walleye.csv', stringsAsFactors = FALSE)
```

Have a look at the first ten lines of the data set:

```{r, eval=FALSE}
  head(eyes, 10)
```

And check out the data structure:   

```{r}  
 str(eyes)
```

These data are counts of walleye that were captured in spawning tributaries of Otsego Lake during the 2009, 2013, 2017,and 2018 spawning season. These measurements are accompanied by various environmental indicators that include high and low flows, precipitation (rain and snow), high, low, mean temperatures (c) and degree days (dd), and photoperiod (daylight) on each day of the year.

We will use the data to predict number of walleye we expect to see each day in the spawning tribs based on historical counts and some explanatory variables of interest.

#### Estimation

We start by estimating a model using REML. Let's say for the sake of argument that we are simply interested in the lake-wide mean of our counts so that we know when students should, for example, be heading out to tributaries to look for walleyes in streams. 

For now, we will model walleye count as a function of photoperiod, with a random effect of site on the intercepts. This model assumes that there is variability in counts of spawning individuals between sites, but that the relationship between photoperiod and count is the same across all sites. In this case, we will specify a quadratic relationship between counts and dates because we expect the number of fish to increase to some point in the run before it decreases. We are not interested in the individual sites in this case, but need to account for repeated observations within spawning locations.

In the `lme4` package, the model might look something like this:

```{r, warning=FALSE, message=FALSE}
# Load the package
  library(lme4)

# Make the model
  waeMod1 = glmer(counts~dd + I(dd^2) + (1|site), data=eyes, family=poisson)
  
# Have a look-see at the results
  summary(waeMod1)
```

As we look through these results, we can see that we have a significant effect of degree days on spawning behavior. What's more is that our count of spawning fish appears to increase during the year to a point before it starts to decrease.

**But** we've got what appear to be some major issues related to convergence at the bottom of this output! [dun, dun, dun]

Luckily, we can fix all of these issues by simply standardizing the covariate (`dd`) as discussed previously. This helps keep things on a unit scale for model estimation, and prevents wacky estimates like negative variances (!).

```{r}
# Standardize the covariate
  eyes$sdd = as.vector(scale(eyes$dd))

# Make the model
  waeMod2 = glmer(counts~sdd + I(sdd^2) + (1|site),
                  data=eyes, family=poisson)
  
# Have a look-see at the results
  summary(waeMod2)
```

Now, if we want, we can make a graph to show these predictions. Here, we make predictions for all years, and then we plot those predictions for a single site (Shadow Brook).

```{r, message=FALSE, warning=FALSE}
# Load the merTools package
  library(merTools)
  
# Make a new dataframe for prediction
  sdd = seq(from = min(eyes$sdd), to = max(eyes$sdd), by = .1)
  site = sort(rep(unique(eyes$site), length(sdd)))
  sdd = rep(sdd, length(unique(eyes$site)))
  newdata = data.frame(sdd)
  
# Simulate predictions from the relationship stored in the model fit using
# our new data
  PI <- predictInterval(merMod = waeMod2, newdata = newdata, 
                        level = 0.95, n.sims = 1000,
                        stat = "median", type="linear.prediction",
                        include.resid.var = TRUE)
  PI = apply(PI, c(1, 2), exp)
# Plot the raw data for the population of interest, i
  plot(x = eyes$sdd, y = eyes$counts,
       ylim = c(0,300), pch=21,
       bg=c('gray87', 'gray60','gray40', 'black')[as.factor(eyes$year)],
       cex=1.25, xlab='Degree days', ylab='Count', xaxt='n')
  
# Add x-axis - this is the tricky part!!
  axis(side = 1,
       at = (seq(0,200,50)-mean(eyes$dd))/sd(eyes$dd), 
       labels = seq(0,200,50)
       )
# Add lines to the plot
  lines(newdata$sdd[site=='Shadow Brook'],
        PI[site=='Shadow Brook',1],
        lty=2, lwd=2, col='gray87')
  lines(newdata$sdd[site=='Shadow Brook'],
        PI[site=='Shadow Brook',2],
        lty=2, lwd=2, col='gray87')
  lines(newdata$sdd[site=='Shadow Brook'],
        PI[site=='Shadow Brook',3],
        lty=2, lwd=2, col='gray87') 
  box()
```

We could also do this using our global parameter estimates and some new data. We would see that our mean predictions aren't terrible, but there is quite a bit of uncertainty here, as above.
 