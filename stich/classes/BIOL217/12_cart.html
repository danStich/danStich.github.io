<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>12_cart.utf8</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<!DOCTYPE html>
<head>
<!-- Favicon for various operating systems -->
<link rel="apple-touch-icon" sizes="180x180" href="./favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="./favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="./favicon/favicon-16x16.png">
<!-- <link rel="manifest" href="./favicon/site.webmanifest"> -->
<link rel="mask-icon" href="./favicon/safari-pinned-tab.svg" color="#603cba">
<link rel="shortcut icon" href="./favicon/favicon.ico">
<meta name="msapplication-TileColor" content="#603cba">
<meta name="msapplication-config" content="./favicon/browserconfig.xml">
<meta name="theme-color" content="#382121">
</head>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">BIOL 217</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
  </a>
</li>
<li>
  <a href="software.html">Getting started</a>
</li>
<li>
  <a href="content.html">Course content</a>
</li>
<li>
  <a href="survival.html">Survival guide</a>
</li>
<li>
  <a href="resources.html">Additional resources</a>
</li>
<li>
  <a href="contact.html">Contact</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://danstich.github.io/worst-r/">worst-r</a>
</li>
<li>
  <a href="https://github.com/danStich">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<h1 id="multi">
Classification and regression trees (CART)
</h1>
<p><img src="../../images/water.jpg" alt=""></p>
<h2 id="multi">
Introduction
</h2>
<p>This week we will explore machine learning techniques for understanding biological and ecological data sets. We are moving on to a set of techniques known broadly as decision trees. Within this class of models, we will focus our discussion on a group of predictive tools known as “classification and regression trees” or <strong>CART</strong>. CART relies on ‘recursive partitioning’ to identify patterns in the variance of response variables with respect to explanatory variables of interest. If the response is categorical, then we use <strong>classification trees</strong>. If it is numeric, then we call them <strong>regression trees</strong>. Relatively recent work has led to the development of tools that also handle multiple responses: multivariate decision trees.</p>
<p>In a way, CART can be thought of as a tool that is complementary to some of the tools that we have already talked about this semester. We can look at differences between groups and distributions like many tools we discussed, but now we are flipping the process on its head. Instead of looking for differences in a response between a priori groups that we have defined as experimental treatments or presumably different biological populations, we will be using computer algorithms to search for unknown statistical populations within the structure of our response of interest with respect to our explanatory variables. Sometimes these are populations that we assume exist ahead of time, but very often they are populations that would be hidden from detection through the use of other quantitative tools.</p>
<div id="the-basics-of-cart-go-something-like-this" class="section level4">
<h4>The <strong>basics of CART</strong> go something like this:</h4>
<p>We partition the response into the two most homogenous groups possible based on our explanatory variables (predictors). If categorical, this means splitting into two categories. In the case of a binomial predictor, we end up with two groups- each containing only a single category. In the case of a multinomial predictor with k categories, we still end up with two groups, but each side of the partition might contain anywhere from 1 to K-1 groups. If the predictor is numeric, then we use a rank-order to determine at which rank the distributions of two groups are most homogeneous. (This is the same concept that we used when we looked at non-parametric alternatives to t-tests and ANOVA back in the first couple weeks of the semester.) This initial split is found by maximizing homogeneity of variance within groups in the partition based on all all possible splits for each of the explanatory variables.</p>
<p>Once we have made the first split, we continue to repeat the process over and over again until the tree becomes sufficiently large that we have over-fit the model. This process is called ‘growing’ the tree. We can then use some form of k-fold cross validation to ‘prune’ the tree back based on the predictive performance of the model.</p>
<p>For those looking for a little more (but not too much) depth and a big-picture overview of some of the math, you can find a nice worked example with some easy-to-understand diagrams <a href="http://stats.stackexchange.com/questions/44382/mathematics-behind-classification-and-regression-trees">in this thread from Stack Overflow</a>.</p>
</div>
<div id="classification-trees" class="section level2">
<h2>Classification trees</h2>
<p>We will work with the <code>iris</code> data for this example. The response in the <code>iris</code> data is a categorical variable with no particular order, <code>Species</code>. Because of this, the class of decision trees that we will be working with for the data is classification trees. In this case, our categorical variable has three levels (<code>setosa</code>, <code>versicolor</code>, and <code>virginica</code>), so we know that the initial split will include one branch that contains only one species and one branch that includes two species. The rest of our tree will be grown based on this initial structure, regardless of the variable that is used for the initial split.</p>
<p>First, let’s load the library and the data set that we will need to use for this example. We’ll start with the <code>rpart</code> package, but there are actually a few different packages that will do CART for us. Last, we set a seed for the random number generators in R so we all get the same results.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">library</span>(rpart)</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co"># Load the iris data that we&#39;ve been working with</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw">data</span>(iris)</span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co"># Set the random number seed so that the results</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co"># are the same each time we run the example (if we</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co"># didn&#39;t set this the results might differ a bit each time</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co"># because the subsampling process below is stochastic)</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="kw">set.seed</span>(<span class="dv">2568</span>)</span></code></pre></div>
<p>Next, we will split our data into <strong>training data</strong> that we use to fit the model, and <strong>test data</strong> we use to validate the model. To do this, we will randomly select rows from our dataframe and assign them to the training data. All unsampled rows will be assigned to the test data set.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># Define a variable to subsample rows of the</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="co"># iris dataframe</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>n &lt;-<span class="st"> </span><span class="kw">nrow</span>(iris)</span>
<span id="cb2-4"><a href="#cb2-4"></a></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="co"># Define a random subsample of the original data.</span></span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="co"># We will use these data as a training data set,</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="co"># that we will then use to fit the model. The object</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="co"># `train` is just a random sample of row numbers from</span></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="co"># the total number of rows ini the iris data.</span></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="co"># Here, we take half the data</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>train &lt;-<span class="st"> </span><span class="kw">sort</span>(<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>n, <span class="kw">floor</span>(n <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)))</span>
<span id="cb2-12"><a href="#cb2-12"></a></span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="co"># We can define separate data frames for the training</span></span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="co"># data and the test data using the indices contained</span></span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="co"># in `train`.</span></span>
<span id="cb2-16"><a href="#cb2-16"></a></span>
<span id="cb2-17"><a href="#cb2-17"></a><span class="co"># Training data</span></span>
<span id="cb2-18"><a href="#cb2-18"></a>iris.train &lt;-<span class="st"> </span>iris[train, ]</span>
<span id="cb2-19"><a href="#cb2-19"></a></span>
<span id="cb2-20"><a href="#cb2-20"></a><span class="co"># Test data</span></span>
<span id="cb2-21"><a href="#cb2-21"></a>iris.test &lt;-<span class="st"> </span>iris[<span class="op">-</span>train, ]</span></code></pre></div>
<p>Then, we can fit the classification tree to the training data.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># Fit the tree</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>iris_cart &lt;-<span class="st"> </span><span class="kw">rpart</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="co"># Formula: &#39;.&#39; means entire dataframe</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>  <span class="dt">data =</span> iris, <span class="co"># Using iris data</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>  <span class="dt">subset =</span> train <span class="co"># More specifically, the training data</span></span>
<span id="cb3-5"><a href="#cb3-5"></a>)</span></code></pre></div>
<p>We can print a summary of the tree to examine where the splits occurred and how these locations were chosen by the algorithm.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># Print the summary</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="kw">summary</span>(iris_cart)</span></code></pre></div>
<p>Finally, we can make some very rudimentary plots in base graphics for the tree and this can greatly facilitate their interpretation. We will also add some text to the plot to make it a little easier to understand.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># Make the plot</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="kw">plot</span>(iris_cart, <span class="dt">minbranch =</span> <span class="dv">10</span>, <span class="dt">compress =</span> <span class="ot">TRUE</span>, <span class="dt">margin =</span> <span class="fl">.15</span>, <span class="dt">nspace =</span> <span class="dv">5</span>)</span>
<span id="cb5-3"><a href="#cb5-3"></a></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="co"># Adding text to the plot makes the tree understandable</span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="kw">text</span>(iris_cart, <span class="dt">use.n =</span> <span class="ot">TRUE</span>, <span class="dt">all =</span> <span class="ot">FALSE</span>, <span class="dt">fancy =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>While this graphical representation is easier to understand than the summary output of the tree, it is not much nicer to look at than the tabular output. So, let’s see if we can do something about that!</p>
</div>
<div id="tree-visualization" class="section level1">
<h1>Tree visualization</h1>
<p>Let’s load a new package for making some nicer trees that we might actually want to use in a talk, on a poster, or even in a paper. One trade off that we need to consider when making decisions about information and data visualization is what we gain and loose with various representations of our trees. For example, the plots above are a mess and we almost certainly wouldn’t want to use them for data presentation in a paper, a talk, or poster. However, they do contain a lot of useful information that we are likely going to lose with nicer looking tools.</p>
<p>We will rely on the aptly named <code>rpart.plot</code> package to start looking at our trees. We will use the <code>prp()</code> function for a nicer looking tree.</p>
<p>You’ll notice right away that we lose the neat-o group membership numbers that we got out of the ugly plots. On the other hand, this is a lot nicer to look at! Plus, it is a lot clearer what the split for each variable represents.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Library load</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="kw">library</span>(rpart.plot)</span>
<span id="cb6-3"><a href="#cb6-3"></a></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co"># Plot the rpart object. Here, we add an</span></span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="co"># argument to make the variable names shorter</span></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="co"># so we can fit more in a single pane</span></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="kw">prp</span>(iris_cart, <span class="dt">varlen =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>But, wouldn’t it be nice if we could get a <em>little</em> more information out of this nicer layout? <strong>Glad you ask</strong>!</p>
<p>One simple way to do it is to pass values to the <code>extra</code> argument in the <code>prp()</code> function. With all of the options available in this plotting utility, it is hard to imagine one named <code>extra</code>, but let’s take a look at how a few of the possible values for <code>extra</code> change the plot (see <code>?prp</code> for a detailed list of possible display options.)</p>
<p>We can get counts in each category for each split</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">prp</span>(iris_cart, <span class="dt">varlen =</span> <span class="dv">3</span>, <span class="dt">extra =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Classification rate (this one is only for classification trees, not to be used for regression trees):</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">prp</span>(iris_cart, <span class="dt">varlen =</span> <span class="dv">3</span>, <span class="dt">extra =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Misclassification rates (also for classification trees only):</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">prp</span>(iris_cart, <span class="dt">varlen =</span> <span class="dv">3</span>, <span class="dt">extra =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>And, probability of membership in a node by class:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">prp</span>(iris_cart, <span class="dt">varlen =</span> <span class="dv">3</span>, <span class="dt">extra =</span> <span class="dv">4</span>)</span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<blockquote>
<p>Here is a <em>really</em> nice way of visualizing these trees, though.</p>
</blockquote>
<p><strong>Note</strong>: You may have let the package install a <code>.dll</code> file for <code>gtk+</code> in order to get the <code>RColorBrewer</code> package to work. <code>Gtk+</code> is the cross-platform <code>GIMP tool kit</code> for building graphical user interfaces (GUIs). This will not be required as part of the assignment this week because you will probably not be able to install this on the school PCs or on some personal laptops, but I want you to there is more to life (and machine learning) than ugly graphs. Now you have the code.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># Install and load the packages</span></span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="kw">library</span>(rattle) <span class="co"># We&#39;ll use this one to make some more functional plots</span></span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="kw">library</span>(RColorBrewer) <span class="co"># We&#39;ll use this one for colors</span></span>
<span id="cb11-4"><a href="#cb11-4"></a></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="co"># Now, make it fancy</span></span>
<span id="cb11-6"><a href="#cb11-6"></a><span class="kw">fancyRpartPlot</span>(iris_cart, <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sub =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Wicked awesome.</p>
</div>
<div id="pruning-the-tree" class="section level1">
<h1>Pruning the tree</h1>
<p>Now that we have looked at how to fit a tree and how to visualize a tree, we really should take a look at how to make sure the tree is not garbage. We can do this a few different ways. We have some tools available to us that we can use to determine the number of splits we should keep and the predictive power of the resultant trees. And we can even explore some statistical stopping rules that can eliminate the need for pruning, at least in theory.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># Let&#39;s start by fitting a tree to the data </span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="co"># that we assume is overfit</span></span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="kw">set.seed</span>(<span class="dv">5</span>)</span>
<span id="cb12-4"><a href="#cb12-4"></a>iris_full &lt;-<span class="st"> </span><span class="kw">rpart</span>(</span>
<span id="cb12-5"><a href="#cb12-5"></a>  Species <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb12-6"><a href="#cb12-6"></a>  <span class="dt">data =</span> iris,</span>
<span id="cb12-7"><a href="#cb12-7"></a>  <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb12-8"><a href="#cb12-8"></a>  <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;information&quot;</span>),</span>
<span id="cb12-9"><a href="#cb12-9"></a>  <span class="dt">cp =</span> <span class="fl">0.000001</span>,</span>
<span id="cb12-10"><a href="#cb12-10"></a>  <span class="dt">minsplit =</span> <span class="dv">1</span>,</span>
<span id="cb12-11"><a href="#cb12-11"></a>  <span class="dt">minbucket =</span> <span class="dv">1</span></span>
<span id="cb12-12"><a href="#cb12-12"></a>)</span>
<span id="cb12-13"><a href="#cb12-13"></a></span>
<span id="cb12-14"><a href="#cb12-14"></a><span class="co"># Take a quick look at the plot</span></span>
<span id="cb12-15"><a href="#cb12-15"></a><span class="kw">prp</span>(iris_full)</span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a><span class="kw">fancyRpartPlot</span>(iris_full, <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sub =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-13-2.png" width="672" /></p>
<p>Now we need to <strong>pick the tree size that minimizes misclassification rate (prediction error)</strong>. To do this, we are basically going to look at changes in three things:</p>
<ol style="list-style-type: decimal">
<li>Training error (error in predicting training data): <code>relerror</code></li>
<li>Crossvalidation error (predictive error in x-validation): <code>xerror</code></li>
<li>Complexity parameter: <code>cp</code> This one tells us how much information gain we get for additional splits. In general, we are looking for the cp at which cross validation error is minimized so we can use that to decide how far back we want to prune our trees.</li>
</ol>
<p>First, let’s look at how the cross-validation results change with respect to the size of our tree. This initial plot shows us how the relative error and the complexity parameter (<code>cp</code>) change with an increasing number of splits.</p>
<p>In this plot, it is clear that by the third split we have minimized the xerror and decreases in information loss are minimized through the addition of more splits, but let’s keep moving along.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a><span class="kw">plotcp</span>(iris_full)</span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Another, perhaps more familiar, way to look at this is to plot changes in the predictive R-squared value for the tree with respect to subsequent splits. Again, it is clear that by the time we get two splits out from the root that we have essentially explained as much variance as we are going to squeeze out of this tree.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="kw">rsq.rpart</span>(iris_full)</span>
<span id="cb15-3"><a href="#cb15-3"></a></span>
<span id="cb15-4"><a href="#cb15-4"></a>Classification tree<span class="op">:</span></span>
<span id="cb15-5"><a href="#cb15-5"></a><span class="kw">rpart</span>(<span class="dt">formula =</span> Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> iris, <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>, <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;information&quot;</span>), </span>
<span id="cb15-6"><a href="#cb15-6"></a>    <span class="dt">cp =</span> <span class="fl">1e-06</span>, <span class="dt">minsplit =</span> <span class="dv">1</span>, <span class="dt">minbucket =</span> <span class="dv">1</span>)</span>
<span id="cb15-7"><a href="#cb15-7"></a></span>
<span id="cb15-8"><a href="#cb15-8"></a>Variables actually used <span class="cf">in</span> tree construction<span class="op">:</span></span>
<span id="cb15-9"><a href="#cb15-9"></a>[<span class="dv">1</span>] Petal.Length Petal.Width  Sepal.Length</span>
<span id="cb15-10"><a href="#cb15-10"></a></span>
<span id="cb15-11"><a href="#cb15-11"></a>Root node error<span class="op">:</span><span class="st"> </span><span class="dv">100</span><span class="op">/</span><span class="dv">150</span> =<span class="st"> </span><span class="fl">0.66667</span></span>
<span id="cb15-12"><a href="#cb15-12"></a></span>
<span id="cb15-13"><a href="#cb15-13"></a>n=<span class="st"> </span><span class="dv">150</span> </span>
<span id="cb15-14"><a href="#cb15-14"></a></span>
<span id="cb15-15"><a href="#cb15-15"></a>       CP nsplit rel error xerror     xstd</span>
<span id="cb15-16"><a href="#cb15-16"></a><span class="dv">1</span> <span class="fl">5.0e-01</span>      <span class="dv">0</span>      <span class="fl">1.00</span>   <span class="fl">1.18</span> <span class="fl">0.050173</span></span>
<span id="cb15-17"><a href="#cb15-17"></a><span class="dv">2</span> <span class="fl">4.4e-01</span>      <span class="dv">1</span>      <span class="fl">0.50</span>   <span class="fl">0.67</span> <span class="fl">0.060888</span></span>
<span id="cb15-18"><a href="#cb15-18"></a><span class="dv">3</span> <span class="fl">2.0e-02</span>      <span class="dv">2</span>      <span class="fl">0.06</span>   <span class="fl">0.09</span> <span class="fl">0.029086</span></span>
<span id="cb15-19"><a href="#cb15-19"></a><span class="dv">4</span> <span class="fl">1.0e-02</span>      <span class="dv">3</span>      <span class="fl">0.04</span>   <span class="fl">0.08</span> <span class="fl">0.027520</span></span>
<span id="cb15-20"><a href="#cb15-20"></a><span class="dv">5</span> <span class="fl">5.0e-03</span>      <span class="dv">6</span>      <span class="fl">0.01</span>   <span class="fl">0.08</span> <span class="fl">0.027520</span></span>
<span id="cb15-21"><a href="#cb15-21"></a><span class="dv">6</span> <span class="fl">1.0e-06</span>      <span class="dv">8</span>      <span class="fl">0.00</span>   <span class="fl">0.09</span> <span class="fl">0.029086</span></span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>We can also print the results of the model fit to take a look at the numbers behind all of these graphs.</p>
<p>Some simple calculations are helpful for this:</p>
<ul>
<li>Prediction error rate in training data = <code>Root node error</code> * <code>rel error</code> * 100</li>
<li>Prediction error rate in cross-validation = <code>Root node error</code> * <code>xerror</code> * 100</li>
</ul>
<p>We want the cp value (with a simpler tree) that minimizes the <code>xerror</code>. The key here is that we want the first model to minimize <code>xerror</code>.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="kw">printcp</span>(iris_full)</span>
<span id="cb16-2"><a href="#cb16-2"></a></span>
<span id="cb16-3"><a href="#cb16-3"></a>Classification tree<span class="op">:</span></span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="kw">rpart</span>(<span class="dt">formula =</span> Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> iris, <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>, <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;information&quot;</span>), </span>
<span id="cb16-5"><a href="#cb16-5"></a>    <span class="dt">cp =</span> <span class="fl">1e-06</span>, <span class="dt">minsplit =</span> <span class="dv">1</span>, <span class="dt">minbucket =</span> <span class="dv">1</span>)</span>
<span id="cb16-6"><a href="#cb16-6"></a></span>
<span id="cb16-7"><a href="#cb16-7"></a>Variables actually used <span class="cf">in</span> tree construction<span class="op">:</span></span>
<span id="cb16-8"><a href="#cb16-8"></a>[<span class="dv">1</span>] Petal.Length Petal.Width  Sepal.Length</span>
<span id="cb16-9"><a href="#cb16-9"></a></span>
<span id="cb16-10"><a href="#cb16-10"></a>Root node error<span class="op">:</span><span class="st"> </span><span class="dv">100</span><span class="op">/</span><span class="dv">150</span> =<span class="st"> </span><span class="fl">0.66667</span></span>
<span id="cb16-11"><a href="#cb16-11"></a></span>
<span id="cb16-12"><a href="#cb16-12"></a>n=<span class="st"> </span><span class="dv">150</span> </span>
<span id="cb16-13"><a href="#cb16-13"></a></span>
<span id="cb16-14"><a href="#cb16-14"></a>       CP nsplit rel error xerror     xstd</span>
<span id="cb16-15"><a href="#cb16-15"></a><span class="dv">1</span> <span class="fl">5.0e-01</span>      <span class="dv">0</span>      <span class="fl">1.00</span>   <span class="fl">1.18</span> <span class="fl">0.050173</span></span>
<span id="cb16-16"><a href="#cb16-16"></a><span class="dv">2</span> <span class="fl">4.4e-01</span>      <span class="dv">1</span>      <span class="fl">0.50</span>   <span class="fl">0.67</span> <span class="fl">0.060888</span></span>
<span id="cb16-17"><a href="#cb16-17"></a><span class="dv">3</span> <span class="fl">2.0e-02</span>      <span class="dv">2</span>      <span class="fl">0.06</span>   <span class="fl">0.09</span> <span class="fl">0.029086</span></span>
<span id="cb16-18"><a href="#cb16-18"></a><span class="dv">4</span> <span class="fl">1.0e-02</span>      <span class="dv">3</span>      <span class="fl">0.04</span>   <span class="fl">0.08</span> <span class="fl">0.027520</span></span>
<span id="cb16-19"><a href="#cb16-19"></a><span class="dv">5</span> <span class="fl">5.0e-03</span>      <span class="dv">6</span>      <span class="fl">0.01</span>   <span class="fl">0.08</span> <span class="fl">0.027520</span></span>
<span id="cb16-20"><a href="#cb16-20"></a><span class="dv">6</span> <span class="fl">1.0e-06</span>      <span class="dv">8</span>      <span class="fl">0.00</span>   <span class="fl">0.09</span> <span class="fl">0.029086</span></span></code></pre></div>
<p>Now we can get the cp for tree that is the first to reach the minimum xerror (with the fewest number of splits):</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a>bestcp &lt;-<span class="st"> </span>iris_full<span class="op">$</span>cptable[<span class="kw">which.min</span>(iris_full<span class="op">$</span>cptable[, <span class="st">&quot;xerror&quot;</span>]), <span class="st">&quot;CP&quot;</span>]</span>
<span id="cb17-2"><a href="#cb17-2"></a>bestcp</span>
<span id="cb17-3"><a href="#cb17-3"></a>[<span class="dv">1</span>] <span class="fl">0.01</span></span></code></pre></div>
<p>We can use the information we have to calculate the misclassification rate for our optimal tree based on the equation shown below. Here, we can see that our best tree will have a misclassification rate of 0.035. <strong>In other words</strong>, we have effectively explained 96.5% of the variance in our data with this regression tree. Aw snap!</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="fl">5e-3</span> <span class="op">*</span><span class="st"> </span><span class="fl">.07</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>[<span class="dv">1</span>] <span class="fl">0.035</span></span></code></pre></div>
<p>Now we can use that cp to prune our tree by re-fitting our model with a new cp argument, changing nothing else, to see where the splits are and how they can be interpretted.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a>iris_pruned &lt;-<span class="st"> </span><span class="kw">rpart</span>(Species <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb19-2"><a href="#cb19-2"></a>  <span class="dt">data =</span> iris,</span>
<span id="cb19-3"><a href="#cb19-3"></a>  <span class="dt">method =</span> <span class="st">&quot;class&quot;</span>,</span>
<span id="cb19-4"><a href="#cb19-4"></a>  <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">split =</span> <span class="st">&quot;information&quot;</span>),</span>
<span id="cb19-5"><a href="#cb19-5"></a>  <span class="dt">cp =</span> <span class="fl">0.01</span></span>
<span id="cb19-6"><a href="#cb19-6"></a>)</span></code></pre></div>
<p>We see that our plot is reduced compared to the original overfitted plot, and it actually is identical to the original, although this is not always the case.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="kw">prp</span>(iris_pruned)</span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a><span class="kw">fancyRpartPlot</span>(iris_pruned, <span class="dt">main =</span> <span class="st">&quot;&quot;</span>, <span class="dt">sub =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p><img src="12_cart_files/figure-html/unnamed-chunk-20-2.png" width="672" /></p>
<p>That should get you started fitting and pruning CART models. Although the examples that we used above are for classification trees, you should understand that these concepts work the same way for regression trees, although the interpretation varies slightly. In fact, it is more straightforward because we are just counting the number of records in each node instead of the number of records within each group in each node. We will explore regression trees further in the assignment for this week.</p>
<p><br></p>
</div>

<!DOCTYPE html>

<br>

<hr>

<p style="color:gray; text-align:center">This work is licensed under a <a href="https://creativecommons.org/licenses/by/4.0/legalcode">Creative Commons Attribution 4.0 International License</a>. Data are provided for educational purposes only unless otherwise noted.</p>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
