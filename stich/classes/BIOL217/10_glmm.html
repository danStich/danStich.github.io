<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<!DOCTYPE html>
<head>
<!-- Favicon for various operating systems -->
<link rel="apple-touch-icon" sizes="180x180" href="./favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="./favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="./favicon/favicon-16x16.png">
<!-- <link rel="manifest" href="./favicon/site.webmanifest"> -->
<link rel="mask-icon" href="./favicon/safari-pinned-tab.svg" color="#603cba">
<link rel="shortcut icon" href="./favicon/favicon.ico">
<meta name="msapplication-TileColor" content="#603cba">
<meta name="msapplication-config" content="./favicon/browserconfig.xml">
<meta name="theme-color" content="#382121">
</head>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">danStich</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../../index.html">Home</a>
</li>
<li>
  <a href="../../teaching.html">Teaching</a>
</li>
<li>
  <a href="../../research.html">Research</a>
</li>
<li>
  <a href="../../cv.html">Curriculum vitae</a>
</li>
<li>
  <a href="../../courseWebsites.html">Course websites</a>
</li>
<li>
  <a href="../../contact.html">Contact</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<style>
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}

.column {
    float: left;
    padding: 15px;
}

.clearfix::after {
    content: "";
    clear: both;
    display: table;
}

.content {
    width: 75%;
}

</style>
<p><br></p>
<div id="generalized-linear-mixed-models" class="section level1">
<h1>Generalized linear mixed models</h1>
<p><br></p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p><br></p>
<p>This week we will talk about extending linear models and generalized linear models to include “random effects” in the model, thus resulting in the “generalized linear mixed model” or GLMM. The GLMM is actually the most generalized formulation of our linear models that we have been discussing now for the past several weeks. All linear models (ANOVA, regression, t-tests, etc.) are simply special cases of the GLMM. As such, we can think of the GLMM as the framework within which we have been working for weeks now! For this week, we will start with examples of the linear mixed model.</p>
<p><br></p>
</div>
<div id="assumptions-of-linear-models" class="section level2">
<h2>Assumptions of linear models</h2>
<p><br></p>
<p>OMG, why is this guy always talking about assumptions of linear models no matter what we do?!</p>
<p>Just as we discussed last week, linear models are just a special case of the GLMM. That is, the linear model assumes a certain error distribution (the normal) that helps things work smoothly and correctly. During the last two weeks, we discussed how we can use link functions to relax the assumption of linear models with respect to normality of residuals and homogeneity of variances, as well as assumptions about the linearity of relationships between explanatory variables and responses of interest by using data transformation. This week, we continue to relax the underlying assumptions of linear models to unleash the true power of estimation in mixed effects models. This is essentially as far as the basic framework for linear modeling goes (with the exception of multivariate techniques), and all other cases (e.g. spatial and temporal autocorrelation regressions) are simply specialized instances of these models.</p>
<p>Let’s take another look at the assumptions of linear models. Here, we will repeat the mantra from the past few weeks. Here are the three assumptions that we explicitly use when we use linear models, just in case you’ve forgotten them:</p>
<p><br></p>

<ol style="list-style-type: decimal">
<li>Residuals are normally distributed with a mean of zero</li>
</ol>

<ol start="2" style="list-style-type: decimal">
<li><p>Independence of observations (residuals)</p>
<ul>
<li>Colinearity</li>
<li>Auto correlation of errors (e.g., spatial &amp; temporal)</li>
</ul></li>
</ol>

<ol start="3" style="list-style-type: decimal">
<li>Homogeneity</li>
</ol>

<ol start="4" style="list-style-type: decimal">
<li>Linear relationship between X and Y</li>
</ol>
<p><br></p>
<div id="assumption-1-normality-of-residuals" class="section level3">
<h3>Assumption 1: Normality of residuals</h3>
<p><br></p>
<p>We’ve seen these before, but let’s recap. For assumption 1, we are assuming a couple of implicit things: 1. The variable is <em>continuous</em> (and it must be if it’s error structure is normal), and 2. The error in our model is normally distributed. In reality, this is probably the least important assumption of linear models, and really only matters if we are trying to make predictions from the models that we make. Of course, we are often concerned with making predictions from the models that we make, so we can see why this might be important. However, more often we are in extreme violation of this assumption in some combination with assumption 4 above to such a degree that it actually does matter. For example, a response variable that is binomial (1 or zero) or multinomial in nature cannot possibly have normally distributed errors with respect to x unless there is absolutely no relationship between X and Y, right? So, if we wanted to predict the probability of patients dying from some medical treatment, or the presence/absence of species across a landscape then we can’t use linear models. This is where the link functions that we have been discussing really come into play. The purpose of the link function is to place our decidedly non-normal error structures into an asymptotically normal probability space. The other key characteristic of the link function is that it must be invertible, that way we can get back to the parameter scale that we want to use for making predictions and visualizing the results of our models.</p>
<p><br></p>
</div>
<div id="assumption-2-independence-of-observations" class="section level3">
<h3>Assumption 2: Independence of observations</h3>
<p><br></p>
<p>This time we’ve broken assumption 2 in two components: Colinearity and autocorrelation of errors. Remember that the manifestation of these problems is in the precision of our coefficient estimates, and have the potential to change the Type-I/II error rates in our models, causing us to draw false conclusions about which variables are important. As we discussed earlier in the course we expect to see some colinearity between observations, and we can deal with balancing this in our modeling through the use of model selection techniques to reduce Type-I and Type-II error. The past couple of weeks, we examined tools that help us determine whether or not colinearity is actually causing problems in our models that go beyond minor nuisances. As for the second part, autocorrelation, we looked at formulations of the GLM that used ‘generalized least squares’ to include auto-regressive correlation matrices in our analysis that will allowed us to relax this assumption of linear models and improve the precision of our parameter estimates. This week, we will further extend this to include random effects to account for non-independence in the observations, and correlation in the residual errors of explanatory variables that could otherwise cause issues with accuracy and precision of our estimates. We will continue to use model selection as a method for determining tradeoffs between information gain and parameter redundancy that results from colinearity between explanatory variables, as well as for hypothesis testing.</p>
<p><br></p>
</div>
<div id="assumption-3-homogeneity-of-variances" class="section level3">
<h3>Assumption 3: Homogeneity of variances</h3>
<p><br></p>
<p>In past weeks, we looked at ways to reduce this issue by introducing blocking (categorical) variables to our models. Last week, we noted that this could be further mitigated through the use of weighted least squares and MLE within the GLM framework, which can be applied to a wide range of regression methods from linear models to GLMs and GLMMs. This week we will examine how we can use various formulations of the GLMM to account for heteroscedasticity in residual errors directly by including the appropriate error terms in our models. This essentially means that we can start to account for things like repeated measures, nested effects, and various other violations through the use of one tool- nifty!</p>
<p><br></p>
</div>
<div id="assumption-4-linearity-and-additivity" class="section level3">
<h3>Assumption 4: Linearity and additivity</h3>
<p><br></p>
<p>We’ve already looked at a couple of ways to deal with violations of these assumptions such as data transformation and/or polynomial formulations of the linear model. We will continue to apply these concepts this week as we begin to investigate the GLMM as robust framework for analysis.</p>
<p><br></p>
</div>
</div>
<div id="introducing-the-glmm" class="section level2">
<h2>Introducing the GLMM</h2>
<p><br></p>
<p>The first thing you should understand about GLMMs is that they are useful for analyzing data from a large number of statistical probability distributions (basically, you can use them for any underlying error structure). But, when we use specific error structures, or make certain assumptions about the manner in which the heterogeneity of variances is structured with respect to specific factors, this model is often given specific names. For example, repeated measures ANOVA (or ANCOVA), nested ANOVA(or ANCOVA), factorial ANOVA (or ANCOVA), linear mixed models, linear mixed effects models, and generalized linear mixed effects models are all just different formulations of the GLMM with different names. It sounds confusing, but just remember this: any linear model with combinations of fixed and random effects is, at it’s core, just another GLMM! If you can convince yourself of this, you will improve your ability to understand a wide range of experimental designs and accompanying statistical models by understanding this one model type.</p>
<p>The second thing you should understand to “get” GLMMs is what exactly is meant by a “random effect”. So far in this course we have only dealt with “fixed” group effects. The fixed effect is a categorical variable that is used to explain some variation in our response of interest. When we use a fixed effect in a statistical model, we are making the assumption that the categories for this effect are “fixed”. In other words, we have assigned the the levels, or categories, based on some a priori knowledge that the levels themselves represent all possible groups that can be used to describe the data. Because of this definition, fixed effects are usually 1) things that we manipulate directly (like dosage or some other treatment), or 2) relatively simple grouping variables such as sex. By contrast, a “random effect” is an effect that we do not generally set ahead of time or manipulate, but rather one which is considered to be a sample from a population of potential categories that we cannot census or (often) control. Please note that there is not a single, widely accepted definition for either of these things in applied statistics and the definition can be context-specific. Don’t take it from me, though, ask Andrew Gelman, globally recognized as one of the world’s leading experts on applied statistics, including the development of many modern approaches <a href="http://andrewgelman.com/2005/01/25/why_i_dont_use/">here</a>.</p>
<p><br></p>
</div>
<div id="linear-mixed-models" class="section level2">
<h2>Linear mixed models</h2>
<p><br></p>
<p>We will start our explorations into GLMM by looking at the somewhat familiar case of normal data. As with the relationship between ANOVA and GLM, we can say that the linear mixed model (LMM) is just a special case of the GLMM (hence the name).</p>
<p>So, what is a mixed model? This is a model that assumes some parameter of interest is drawn from a random sample from a distribution of possible samples. We usually use these when we are dealing with repeated samples for some group or individual, or if we wish to account for some latent variable beyond our control (e.g. lake). Essentially, the use of random effects allows us to remove extraneous noise (variance) from the study system by accounting for it directly. This can improve both the accuracy and the precision of estimates to make hypothesis testing more robust. It also allows us to generalize our conclusions to a broader scope (e.g. any lake instead of lakes X, Y, and Z). Finally, the approach allows for a great deal of flexibility in assumptions we make about the effects and associated errors in our model. We might assume within our model that effects are randomized by assigning random intercepts, random slopes and intercepts that are independent, or random slopes or intercepts that are correlated across our random variable. There are even rare cases when we might wish to examine random slopes with shared intercepts. Generally speaking, we want our random variable to contain a relatively large number of potential levels (usually &gt; 5) as this tends to result in more accurate, and more precise parameter estimates.</p>
<p>As with so many things, these tools are often best investigated through the use of a worked example. For this section, we will simulated data about asps, and we’ll walk through maximum likelihood estimation of the LMM.</p>
<p><br></p>
<div id="worked-example" class="section level3">
<h3>Worked example</h3>
<p><br></p>
<p>We will start by generating the data used in the book chapter. These data are for the viper asps from Chapter 9, with a new twist. Here, we simulate length and mass data for 10 vipers from each of 56 different populations. I will not cover the following in detail here, but I encourage you to delve into it as you work through your text book.</p>
<p><br></p>
<div id="data-generation" class="section level4">
<h4>Data generation</h4>
<p><br></p>
<p>Here is the code for data simulation, shamelessly copy-and-pasted from Chapter 12 of:</p>
<blockquote>
<p>Kery, M. 2010. Introduction to WinBUGS for Ecologists. Academic Press, Upper Saddle River, NY.</p>
</blockquote>
<p><br></p>
<p>You do not need to know this for this class, but I wanted you to see what goes into the data simulation.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n.groups &lt;-<span class="st"> </span><span class="dv">56</span>              <span class="co"># Number of populations</span>
n.sample &lt;-<span class="st"> </span><span class="dv">10</span>              <span class="co"># Number of vipers in each pop</span>
n &lt;-<span class="st"> </span>n.groups <span class="op">*</span><span class="st"> </span>n.sample        <span class="co"># Total number of data points</span>
pop &lt;-<span class="st"> </span><span class="kw">gl</span>(<span class="dt">n =</span> n.groups, <span class="dt">k =</span> n.sample)   <span class="co"># Indicator for population</span>

<span class="co"># Body length (cm)</span>
original.length &lt;-<span class="st"> </span><span class="kw">runif</span>(n, <span class="dv">45</span>, <span class="dv">70</span>) 
mn &lt;-<span class="st"> </span><span class="kw">mean</span>(original.length)
sd &lt;-<span class="st"> </span><span class="kw">sd</span>(original.length)
<span class="kw">cat</span>(<span class="st">&quot;Mean and sd used to normalise.original length:&quot;</span>, mn, sd, <span class="st">&quot;</span><span class="ch">\n\n</span><span class="st">&quot;</span>)</code></pre></div>
<pre><code>## Mean and sd used to normalise.original length: 57.13128 7.36973</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">length &lt;-<span class="st"> </span>(original.length <span class="op">-</span><span class="st"> </span>mn) <span class="op">/</span><span class="st"> </span>sd
<span class="kw">hist</span>(length, <span class="dt">col =</span> <span class="st">&quot;grey&quot;</span>)</code></pre></div>
<p><img src="10_glmm_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Xmat &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>pop<span class="op">*</span>length<span class="op">-</span><span class="dv">1</span><span class="op">-</span>length)

intercept.mean &lt;-<span class="st"> </span><span class="dv">230</span>           <span class="co"># mu_alpha</span>
intercept.sd &lt;-<span class="st"> </span><span class="dv">20</span>              <span class="co"># sigma_alpha</span>
slope.mean &lt;-<span class="st"> </span><span class="dv">60</span>                <span class="co"># mu_beta</span>
slope.sd &lt;-<span class="st"> </span><span class="dv">30</span>                  <span class="co"># sigma_beta</span>

intercept.effects&lt;-<span class="kw">rnorm</span>(<span class="dt">n =</span> n.groups, <span class="dt">mean =</span> intercept.mean, <span class="dt">sd =</span> intercept.sd)
slope.effects &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n.groups, <span class="dt">mean =</span> slope.mean, <span class="dt">sd =</span> slope.sd)
all.effects &lt;-<span class="st"> </span><span class="kw">c</span>(intercept.effects, slope.effects) <span class="co"># Put them all together</span>

lin.pred &lt;-<span class="st"> </span>Xmat[,] <span class="op">%*%</span><span class="st"> </span>all.effects         <span class="co"># Value of lin.predictor</span>
eps &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">30</span>)      <span class="co"># residuals </span>
mass &lt;-<span class="st"> </span>lin.pred <span class="op">+</span><span class="st"> </span>eps                      <span class="co"># response = lin.pred + residual</span>

<span class="kw">hist</span>(mass, <span class="dt">col =</span> <span class="st">&quot;grey&quot;</span>)                    <span class="co"># Inspect what we&#39;ve created</span></code></pre></div>
<p><img src="10_glmm_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lattice)
<span class="kw">xyplot</span>(mass <span class="op">~</span><span class="st"> </span>length <span class="op">|</span><span class="st"> </span>pop)</code></pre></div>
<p><img src="10_glmm_files/figure-html/unnamed-chunk-2-3.png" width="672" /></p>
<p><br></p>
<p>So, as you can see we have successfully created 56 populations of critters, each of which has some pre-defined relationship between length and mass based on 10 samples from the population. Wow. What a mess. Next, we will take a look at a few different ways to analyze these data using maximum likelihood estimation. Ultimately, what we want to do here is estimate the relationship between length and mass while accounting for inherent (well, simulated) variability between populations.</p>
<p><br></p>
</div>
<div id="random-intercepts-model" class="section level4">
<h4>Random-intercepts model</h4>
<p><br></p>
<p>First, we will analyze the data assuming that the intercepts for our linear model can vary between populations, but the relationship between length and mass is the same across all populations. This is a very common approach in many ecological and biological applications, as it often is the case that we are just trying to account for sampling design when we do this kind of analysis.</p>
<p><br></p>
</div>
<div id="restricted-max-likelihood-estimation" class="section level4">
<h4>Restricted max likelihood estimation</h4>
<p><br></p>
<p>This is really straightforward to do in R. First, we will load the <code>lme4</code> package that we will use for all LMM and GLMM implementation in this class. Then, we fit the model and print the output.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the lme4 library</span>
<span class="kw">library</span>(lme4)

<span class="co"># Fit the model</span>
lme.fit1 &lt;-<span class="st"> </span><span class="kw">lmer</span>(mass <span class="op">~</span><span class="st"> </span>length <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">|</span><span class="st"> </span>pop), <span class="dt">REML =</span> <span class="ot">TRUE</span>)

<span class="co"># Have a look</span>
<span class="kw">summary</span>(lme.fit1)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: mass ~ length + (1 | pop)
## 
## REML criterion at convergence: 5744.1
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.9338 -0.6693 -0.0628  0.6927  3.2194 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  pop      (Intercept)  330.5   18.18   
##  Residual             1509.1   38.85   
## Number of obs: 560, groups:  pop, 56
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  229.370      2.932   78.23
## length        58.766      1.713   34.30
## 
## Correlation of Fixed Effects:
##        (Intr)
## length 0.000</code></pre>
<p><br></p>
<p>As we look throught the output, the first thing that you’ll probably notice is that we don’t have a p-value, which is probably already driving some people nuts. This is a bit of a philosophical sticking point, which is why it is not included in the default print method for the summary of these objects. Here, we can do statistical inference based on the overlap with our coefficient estimates with zero. What does this look like?</p>
<p>Let’s say we want to see if our explanatory variable is significantly related to the response of interest, but we don’t have a p-value. If we still wanted to test significance of a variable (at <span class="math inline">\(\alpha\)</span> = 0.05), we could estimate the 95% CI for the regression coefficient and determine whether or not the 95% CI includes zero, like so:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compute 95% CI on regression coefficient for length above</span>
<span class="co"># We add or subtract 1.96 times the standard error for the coefficient because</span>
<span class="co"># 95% of the data fall within +/- 1.96 standard deviations of the mean</span>

<span class="co"># First, get the coefficients</span>
  res =<span class="st"> </span><span class="kw">summary</span>(lme.fit1)<span class="op">$</span>coefficients

  lengthCI =<span class="st"> </span><span class="kw">c</span>(res[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">-</span>(<span class="fl">1.96</span><span class="op">*</span>res[<span class="dv">2</span>,<span class="dv">2</span>]), res[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">+</span>(<span class="fl">1.96</span><span class="op">*</span>res[<span class="dv">2</span>,<span class="dv">2</span>]))

<span class="co"># Print the CI to the console</span>
  lengthCI</code></pre></div>
<pre><code>## [1] 55.40804 62.12417</code></pre>
<p><br></p>
<p>Here, we can see that our 95% CI excludes zero, so we can say that length has a significant effect on mass in this species. If we wanted to show this relationship in a graph, we now have to do the math by hand, which is why I forced you to learn the math involved with this. Here is an example of how one might do this:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the raw data</span>
  <span class="kw">plot</span>(length, mass)
<span class="co"># Make a sequence of new lengths</span>
  lens =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(length), <span class="kw">max</span>(length), <span class="dt">by=</span><span class="dv">1</span>)
<span class="co"># Predict new values for mass from lens</span>
  masses =<span class="st"> </span>res[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>res[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">*</span>lens
<span class="co"># Add lines to the plot</span>
  <span class="kw">lines</span>(lens, masses, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;blue&#39;</span>)</code></pre></div>
<p><img src="10_glmm_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p><br></p>
<p>This gives us the overall estimate for the relationship between length and mass. What it does not get us are individual estimates of the intercepts like we might want. We can, however, get these out if we use the <code>ranef</code> function:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get random intercepts</span>
  effs =<span class="st"> </span><span class="kw">coef</span>(lme.fit1)

<span class="co"># Have a look at the first few</span>
  <span class="kw">head</span>(effs<span class="op">$</span>pop)</code></pre></div>
<pre><code>##   (Intercept)   length
## 1    240.9334 58.76611
## 2    203.0585 58.76611
## 3    222.8307 58.76611
## 4    211.8497 58.76611
## 5    219.4091 58.76611
## 6    239.6483 58.76611</code></pre>
<p><br></p>
<p>This gives us the random intercept for each population so we can make predictions about them individually if we want to! It only gives us one value for the length parameter because we specified this as a main effect. If want to plot the results for population 3, for example, our code would look like this:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the raw data</span>
  <span class="kw">plot</span>(length[pop<span class="op">==</span><span class="dv">3</span>], mass[pop<span class="op">==</span><span class="dv">3</span>],
       <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">500</span>), <span class="dt">pch=</span><span class="dv">21</span>, <span class="dt">bg=</span><span class="st">&#39;gray87&#39;</span>,
       <span class="dt">cex=</span><span class="fl">1.5</span>, <span class="dt">xlab=</span><span class="st">&#39;log(Length)&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Mass&#39;</span>
       )
<span class="co"># Make a sequence of new lengths</span>
  lens =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(length), <span class="kw">max</span>(length), <span class="dt">by=</span>.<span class="dv">5</span>)
<span class="co"># Predict new values for mass from lens</span>
  masses =<span class="st"> </span>effs<span class="op">$</span>pop[<span class="dv">3</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>effs<span class="op">$</span>pop[<span class="dv">3</span>,<span class="dv">2</span>]<span class="op">*</span>lens
<span class="co"># Add lines to the plot</span>
  <span class="kw">lines</span>(lens, masses, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;blue&#39;</span>)</code></pre></div>
<p><img src="10_glmm_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p><br></p>
<p>But, we still don’t really have a great way of looking at differences between groups if we are interested in those. Why is this? We do not have the technology. Basically, computing group-specific variances is too far ahead of our programming algorithms. But, we can use some simulation methods to do this.</p>
<p>The following method was released in December 2016 to meet the growing need for characterizing uncertainty in group-level predictions from mixed models. Let’s take a look at how to use some of these tools.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(merTools)

<span class="co"># Tell r which population we are </span>
<span class="co"># working with. We will store as a variable</span>
<span class="co"># upfront so we can easily change populations</span>
<span class="co"># by re-defining the variable, which I will</span>
<span class="co"># call `i`. We will use population 3 to stay</span>
<span class="co"># consistent with the example above</span>
  i=<span class="dv">3</span>
  
<span class="co"># Make a sequence of new lengths</span>
  lens =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(length),
             <span class="kw">max</span>(length),
             <span class="dt">by=</span>.<span class="dv">1</span>)
  
<span class="co"># Make a df that contains lengths</span>
<span class="co"># and a label for population</span>
  newd =<span class="st"> </span><span class="kw">data.frame</span>(
    <span class="dt">pop =</span> <span class="kw">rep</span>(i, <span class="kw">length</span>(lens)),
    <span class="dt">length =</span> lens
  )

<span class="co"># Simulate predictions from the relationship</span>
<span class="co"># stored in the model fit using our new data</span>
  PI &lt;-<span class="st"> </span><span class="kw">predictInterval</span>(<span class="dt">merMod =</span> lme.fit1, <span class="dt">newdata =</span> newd, 
                        <span class="dt">level =</span> <span class="fl">0.95</span>, <span class="dt">n.sims =</span> <span class="dv">10000</span>,
                        <span class="dt">stat =</span> <span class="st">&quot;median&quot;</span>, <span class="dt">type=</span><span class="st">&quot;linear.prediction&quot;</span>,
                        <span class="dt">include.resid.var =</span> <span class="ot">TRUE</span>
                        )

<span class="co"># Plot the raw data for the population of interest, i</span>
  <span class="kw">plot</span>(length[pop<span class="op">==</span>i], mass[pop<span class="op">==</span>i],
       <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">500</span>), <span class="dt">pch=</span><span class="dv">21</span>, <span class="dt">bg=</span><span class="st">&#39;gray87&#39;</span>,
       <span class="dt">cex=</span><span class="fl">1.9</span>, <span class="dt">xlab=</span><span class="st">&#39;log(Length)&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Mass&#39;</span>
       )

<span class="co"># Predict new values for mass from lens</span>
  masses =<span class="st"> </span>effs<span class="op">$</span>pop[i,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>effs<span class="op">$</span>pop[i,<span class="dv">2</span>]<span class="op">*</span>lens
<span class="co"># Add lines to the plot</span>
  <span class="kw">lines</span>(newd<span class="op">$</span>length, PI<span class="op">$</span>fit, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;blue&#39;</span>) <span class="co"># Mean</span>
  <span class="kw">lines</span>(newd<span class="op">$</span>length, PI<span class="op">$</span>upr, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;red&#39;</span>)  <span class="co"># Upper CI</span>
  <span class="kw">lines</span>(newd<span class="op">$</span>length, PI<span class="op">$</span>lwr, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;red&#39;</span>)  <span class="co"># Lower CI</span></code></pre></div>
<p><img src="10_glmm_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p><br></p>
<p>We could go through and do this for each population (56 total), and we would have a pretty nice (albeit slightly unwieldy) set of figures describing the relationship between length and mass. Alternatively, we could use the model coefficients to estimate the relationship across all populations, and to make predictions about unknown populations.</p>
<p><br></p>
</div>
</div>
</div>
<div id="binary-logistic-regression" class="section level2">
<h2>Binary (logistic) regression</h2>
<p><br></p>
<p>For our first example this week, we will use the same data from last week that we used to demonstrate binomial logistic regression. This time, we will add in a random intercept term that will allow us to account for repeated observations within a year. This has two implications: 1) it accounts for the fact that the years in which we conducted this study are random samples from a larger, unobserved population, and 2) it accounts for the heterogeneity of variance that theoretically might occur as a result of taking multiple, and variable, numbers of measurements within a given year- thereby reducing the overall error of the model and our associated parameter estimates (in theory).</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Let&#39;s read in the smolt data set that we used last time</span>
  choice =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;http://employees.oneonta.edu/stichds/data/StillwaterChoiceData.csv&#39;</span>)

<span class="co"># Look at the first few rows of data</span>
  <span class="kw">head</span>(choice)</code></pre></div>
<pre><code>##   path year hatchery length mass date flow
## 1    0 2010        1    176   57  118  345
## 2    0 2005        1    205  101  128 1093
## 3    0 2010        1    180   56  118  345
## 4    0 2010        1    193   74  118  345
## 5    0 2005        1    189   76  128 1093
## 6    0 2010        1    180   65  118  345</code></pre>
<p><br></p>
<div id="data-explanation--copied-from-previous" class="section level3">
<h3>Data Explanation- copied from previous</h3>
<p><br></p>
<p>These data are from a study that examined factors affecting path choice by wild and hatchery-reared endangered Atlantic salmon smolts during seaward migration in the Penobscot River, Maine. State, local, and federal fishery managers were interested in understanding what factors affected migratory routing through the lower river because there were different numbers of dams, with different estimated smolt mortality rates, on either side of a large island hydropower project in this system. If managers could understand factors influencing migratory route, they might be able to manipulate flows, stocking dates, and dam operation to improve survival of these endangered fish. Furthermore, the results of the study were used to predict the effects of dam removal, and hydropower re-allocation in the lower river on population-level consequences for these fish. These data were part of a larger analysis:</p>
<p><br></p>
<p>Stich, D. S., M. M. Bailey, and J. D. Zydlewski. 2014. Survival of Atlantic salmon (<em>Salmo salar</em>) smolts through a hydropower complex. Journal of Fish Biology 85:1074-1096.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The data consist of the following variables:</span>
<span class="co"># path:     The migratory route used by individual fish. The choices were</span>
<span class="co">#           main-stem of the river (0) or the Stillwater Branch (1) around the</span>
<span class="co">#           island.</span>
<span class="co"># year:     The year in which individual fish were tagged and relocated using</span>
<span class="co">#           acoustic telemetry.</span>
<span class="co"># hatchery: An indicator describing if fish were reared in the wild (0) or in</span>
<span class="co">#           the federal conservation hatchery (1)</span>
<span class="co"># length:   Fish length (in mm)</span>
<span class="co"># mass:     Fish mass (in grams)</span>
<span class="co"># date:     Ordinal date on which the fish entered the hydrocomplex determined</span>
<span class="co">#           from time-stamps on acoustic receivers</span>
<span class="co"># flow:     Discharge recorded at the USGS gauge in the headpond of the dam</span>
<span class="co">#           several kilometers upstream of the hydropower complex.</span></code></pre></div>
<p><br></p>
</div>
<div id="data-analysis" class="section level3">
<h3>Data analysis</h3>
<p><br></p>
<p>We are going to use the 1/0 binary data to estimate the effects of a number of covariates of interest on the probability that an individual fish used the Stillwater Branch for migration in each year of this study using logistic regression. In order to do this, we will use the ‘logit’ link function, which can be defined as:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    logit =<span class="st"> </span><span class="cf">function</span>(x){
      <span class="kw">log</span>(x <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>x))
    }</code></pre></div>
<p><br></p>
<p>The inverse of the logit function is:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    invlogit =<span class="st"> </span><span class="cf">function</span>(x){
      <span class="kw">exp</span>(x) <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(x))
    }</code></pre></div>
<p><br></p>
<p>Since we are not interested in the linear trend in the use of the Stillwater Branch through time, we need to convert year to factor. This is the same as if we wanted to use this as a fixed effect in the model.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">    choice<span class="op">$</span>year =<span class="st"> </span><span class="kw">as.factor</span>(choice<span class="op">$</span>year)</code></pre></div>
<p><br></p>
<p>Next, define a set of models based on a priori combinations of explanatory variables.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># First, make an empty list to hold the models</span>
  mods=<span class="kw">list</span>()

<span class="co"># Now, fill the list with several a priori models</span>
<span class="co"># Need to load the `lme4` package for the `glmer` function</span>
  <span class="kw">library</span>(lme4)
<span class="co"># Here is the list</span>
  mods[[<span class="dv">1</span>]]=<span class="kw">glmer</span>(path<span class="op">~</span>(<span class="dv">1</span><span class="op">|</span>year)<span class="op">+</span>hatchery<span class="op">+</span>length<span class="op">+</span>flow,<span class="dt">family=</span>binomial,
                  <span class="dt">data=</span>choice)
  mods[[<span class="dv">2</span>]]=<span class="kw">glmer</span>(path<span class="op">~</span>(<span class="dv">1</span><span class="op">|</span>year)<span class="op">+</span>flow,<span class="dt">family=</span>binomial,<span class="dt">data=</span>choice)
  mods[[<span class="dv">3</span>]]=<span class="kw">glmer</span>(path<span class="op">~</span>(<span class="dv">1</span><span class="op">|</span>year)<span class="op">+</span>hatchery,<span class="dt">family=</span>binomial, <span class="dt">data=</span>choice)
  mods[[<span class="dv">4</span>]]=<span class="kw">glmer</span>(path<span class="op">~</span>(<span class="dv">1</span><span class="op">|</span>year)<span class="op">+</span>length,<span class="dt">family=</span>binomial, <span class="dt">data=</span>choice)
  mods[[<span class="dv">5</span>]]=<span class="kw">glmer</span>(path<span class="op">~</span>(<span class="dv">1</span><span class="op">|</span>year)<span class="op">+</span>length<span class="op">+</span>hatchery,<span class="dt">family=</span>binomial,<span class="dt">data=</span>choice)
  mods[[<span class="dv">6</span>]]=<span class="kw">glmer</span>(path<span class="op">~</span>(<span class="dv">1</span><span class="op">|</span>year)<span class="op">+</span>length<span class="op">+</span>flow,<span class="dt">family=</span>binomial,<span class="dt">data=</span>choice)
  mods[[<span class="dv">7</span>]]=<span class="kw">glmer</span>(path<span class="op">~</span>(<span class="dv">1</span><span class="op">|</span>year)<span class="op">+</span>hatchery<span class="op">+</span>flow,<span class="dt">family=</span>binomial,<span class="dt">data=</span>choice)</code></pre></div>
<p><br></p>
<p>Give the models some names using the formulas for each of the models. <strong>Remember</strong>: models are stored as list objects in R, and each of those list objects (models) has names. We can reference those names using the <code>$</code> notation:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="st"> </span><span class="kw">length</span>(mods)){
    <span class="kw">names</span>(mods)[i] =<span class="st"> </span><span class="kw">as.character</span>(<span class="kw">summary</span>(mods[[i]])<span class="op">$</span>call<span class="op">$</span>formula)[<span class="dv">3</span>]
  }</code></pre></div>
<p><br></p>
<p>Now, we use the <code>AICcmodavg</code> package to make a model selection table like we did last week:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the package and make the table</span>
  <span class="kw">library</span>(AICcmodavg)
  modtable =<span class="st"> </span><span class="kw">aictab</span>( <span class="dt">cand.set =</span> mods, <span class="dt">modnames =</span> <span class="kw">names</span>(mods) )</code></pre></div>
<p><br></p>
<p>Finally, we can use these models to make predictions about the relationships in our models the same way we have done previously with linear models and GLMs.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Start by making a new sequence of values from which to make predictions</span>
  newFlow =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(choice<span class="op">$</span>flow), <span class="kw">max</span>(choice<span class="op">$</span>flow), <span class="dv">1</span>)

<span class="co"># Let&#39;s go ahead and use the best model to make some predictions, as we</span>
<span class="co"># can see that flow is the most important variable here</span>
  beta_<span class="dv">0</span> =<span class="st"> </span><span class="kw">summary</span>(mods[[<span class="dv">2</span>]])<span class="op">$</span>coefficients[<span class="dv">1</span>,<span class="dv">1</span>]
  beta_<span class="dv">1</span> =<span class="st"> </span><span class="kw">summary</span>(mods[[<span class="dv">2</span>]])<span class="op">$</span>coefficients[<span class="dv">2</span>,<span class="dv">1</span>]

<span class="co"># Now, we can make predictions from this model-averaged estimate</span>
  preds =<span class="st"> </span><span class="kw">invlogit</span>(beta_<span class="dv">0</span> <span class="op">+</span><span class="st"> </span>beta_<span class="dv">1</span><span class="op">*</span>newFlow)

<span class="co"># Make a *quick* plot for data vis</span>
  <span class="kw">plot</span>(newFlow, preds, <span class="dt">type=</span><span class="st">&#39;l&#39;</span>)</code></pre></div>
<p><img src="10_glmm_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p><br></p>
</div>
</div>
<div id="count-models" class="section level2">
<h2>Count models</h2>
<p><br></p>
<div id="walleye-in-spring" class="section level3">
<h3>Walleye in spring</h3>
<p><br></p>
<p>It is that magical time of year again. Birds are returning from their winter vacations, and all of the critters are twitterpaited. The salamanders and frogs are making there way to breeding pools in the soaked leaf litter, and even the fish are warming up for the spawn. There’s only one problem: we don’t quite know when those fish are going to get into the streams so we can catch them, clip their fins, put some tags in them, and study their every move (…muahaha…).</p>
<p><br></p>
<p>For this first example, we will attempt to predict counts of walleye, Sander vitreus, in spawning streams of Otsego Lake based on historical counts and climate data.</p>
<p><br></p>
<div id="data" class="section level4">
<h4>Data</h4>
<p><br></p>
<p>We begin by reading in the data set:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Read in the walleye data</span>
  eyes =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;http://employees.oneonta.edu/stichds/data/walleye.csv&#39;</span>, <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><br></p>
<p>Have a look at the first ten lines of the data set:</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">  <span class="kw">head</span>(eyes, <span class="dv">10</span>)</code></pre></div>
<p><br></p>
<p>And check out the data structure</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"> <span class="kw">str</span>(eyes)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    35 obs. of  18 variables:
##  $ date        : chr  &quot;2009-04-05&quot; &quot;2009-04-05&quot; &quot;2009-04-05&quot; &quot;2009-04-07&quot; ...
##  $ site        : chr  &quot;Cripple Creek&quot; &quot;Hayden Creek&quot; &quot;Shadow Brook&quot; &quot;Hayden Creek&quot; ...
##  $ counts      : int  4 61 17 50 10 1 35 25 1 29 ...
##  $ high_f      : num  52 52 52 NA NA 39 39 55 55 53.1 ...
##  $ low_f       : num  32 32 32 NA NA 28 28 32 32 28.9 ...
##  $ precip_in   : num  0.02 0.02 0.02 NA NA 0 0 0 0 0.4 ...
##  $ snow_in     : int  0 0 0 NA NA 0 0 0 0 0 ...
##  $ snowDepth_in: int  0 0 0 NA NA 0 0 0 0 0 ...
##  $ high_c      : num  11.1 11.1 11.1 NA NA ...
##  $ low_c       : num  0 0 0 NA NA ...
##  $ mean_c      : num  5.56 5.56 5.56 NA NA ...
##  $ ddPrep      : num  5.56 5.56 5.56 NA NA ...
##  $ day         : int  95 95 95 97 97 99 99 100 100 102 ...
##  $ year        : int  2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ...
##  $ dd          : num  109 109 109 118 118 ...
##  $ dd2         : num  11954 11954 11954 14031 14031 ...
##  $ daylight    : num  12.9 12.9 12.9 13 13 ...
##  $ daylight2   : num  166 166 166 168 168 ...</code></pre>
<p><br></p>
<p>These data are measurements of the length and mass of individual walleye at various reproductive stages that were captured in spawning tributaries of Otsego Lake during the 2009 and 2013 spawning season. These measurements are accompanied by various environmental indicators that include high and low flows, precipitation (rain and snow), high, low, mean temperatures (c) and degree days (dd), and photoperiod (daylight) on each day of the year.</p>
<p>We will use the data to predict number of walleye we expect to see each day in the spawning tribs during spring 2018 based on historical counts and some explanatory variables of interest.</p>
<p><br></p>
</div>
<div id="reml-estimation" class="section level4">
<h4>REML estimation</h4>
<p><br></p>
<p>We start by estimating a model using REML. Let’s say for the sake of argument that we are simply interested in the lake-wide mean of our counts so that we know when students should, for example, be heading out to tributaries to look for walleyes in streams.</p>
<p>For now, we will model walleye count as a function of photoperiod, with a random effect of site on the intercepts. This model assumes that there is variability in counts of spawning individuals between sites, but that the relationship between photoperiod and count is the same across all sites. In this case, we will specify a quadratic relationship between counts and dates because we expect the number of fish to increase to some point in the run before it decreases. We are not interested</p>
<p>In the <code>lme4</code> package, the model might look something like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the package</span>
  <span class="kw">library</span>(lme4)

<span class="co"># Make the model</span>
  waeMod1 =<span class="st"> </span><span class="kw">glmer</span>(counts<span class="op">~</span>dd <span class="op">+</span><span class="st"> </span>dd2 <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">|</span>site), <span class="dt">data=</span>eyes, <span class="dt">family=</span>poisson)
  
<span class="co"># Have a look-see at the results</span>
  <span class="kw">summary</span>(waeMod1)</code></pre></div>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: poisson  ( log )
## Formula: counts ~ dd + dd2 + (1 | site)
##    Data: eyes
## 
##      AIC      BIC   logLik deviance df.resid 
##    847.4    853.7   -419.7    839.4       31 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -6.4406 -3.0107 -0.7852  2.4903 11.4417 
## 
## Random effects:
##  Groups Name        Variance Std.Dev.
##  site   (Intercept) 0.6094   0.7807  
## Number of obs: 35, groups:  site, 4
## 
## Fixed effects:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -7.360e+00  6.772e-01  -10.87   &lt;2e-16 ***
## dd           1.396e-01  7.488e-03   18.64   &lt;2e-16 ***
## dd2         -4.575e-04  2.479e-05  -18.45   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Correlation of Fixed Effects:
##     (Intr) dd    
## dd  -0.793       
## dd2  0.749 -0.983
## fit warnings:
## Some predictor variables are on very different scales: consider rescaling
## convergence code: 0
## Model failed to converge with max|grad| = 9.29104 (tol = 0.001, component 1)
## Model is nearly unidentifiable: very large eigenvalue
##  - Rescale variables?
## Model is nearly unidentifiable: large eigenvalue ratio
##  - Rescale variables?</code></pre>
<p>As we look through these results, we can see that we have a significant effect of daylight on spawning behavior. What’s more is that our count of spawning fish appears to increase during the year to a point before it starts to decrease.</p>
<p>Now, if we want, we can make a graph to show these predictions. Here, we make predictions for all years, and then we plot those predictions for a single site (Shadow Brook).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the merTools package</span>
  <span class="kw">library</span>(merTools)
  
<span class="co"># Make a new dataframe for prediction</span>
  dd =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="kw">min</span>(eyes<span class="op">$</span>dd), <span class="dt">to =</span> <span class="kw">max</span>(eyes<span class="op">$</span>dd), <span class="dt">by =</span> .<span class="dv">1</span>)
  site =<span class="st"> </span><span class="kw">sort</span>(<span class="kw">rep</span>(<span class="kw">unique</span>(eyes<span class="op">$</span>site), <span class="kw">length</span>(dd)))
  dd =<span class="st"> </span><span class="kw">rep</span>(dd, <span class="kw">length</span>(<span class="kw">unique</span>(eyes<span class="op">$</span>site)))
  dd2 =<span class="st"> </span>dd<span class="op">^</span><span class="dv">2</span>
  newdata =<span class="st"> </span><span class="kw">data.frame</span>(dd, dd2)
  
<span class="co"># Simulate predictions from the relationship stored in the model fit using</span>
<span class="co"># our new data</span>
  PI &lt;-<span class="st"> </span><span class="kw">predictInterval</span>(<span class="dt">merMod =</span> waeMod1, <span class="dt">newdata =</span> newdata, 
                        <span class="dt">level =</span> <span class="fl">0.95</span>, <span class="dt">n.sims =</span> <span class="dv">1000</span>,
                        <span class="dt">stat =</span> <span class="st">&quot;median&quot;</span>, <span class="dt">type=</span><span class="st">&quot;linear.prediction&quot;</span>,
                        <span class="dt">include.resid.var =</span> <span class="ot">TRUE</span>)
  PI =<span class="st"> </span><span class="kw">apply</span>(PI, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), exp)
<span class="co"># Plot the raw data for the population of interest, i</span>
  <span class="kw">plot</span>(eyes<span class="op">$</span>dd, eyes<span class="op">$</span>counts, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">500</span>), <span class="dt">pch=</span><span class="dv">21</span>, <span class="dt">bg=</span><span class="kw">c</span>(<span class="st">&#39;gray87&#39;</span>, <span class="st">&#39;gray60&#39;</span>,<span class="st">&#39;gray40&#39;</span>, <span class="st">&#39;black&#39;</span>)[<span class="kw">as.factor</span>(eyes<span class="op">$</span>year)],
       <span class="dt">cex=</span><span class="fl">1.9</span>, <span class="dt">xlab=</span><span class="st">&#39;Degree days&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;Count&#39;</span>)

<span class="co"># Add lines to the plot</span>
  <span class="kw">lines</span>(newdata<span class="op">$</span>dd[site<span class="op">==</span><span class="st">&#39;Shadow Brook&#39;</span>], PI[site<span class="op">==</span><span class="st">&#39;Shadow Brook&#39;</span>,<span class="dv">1</span>], <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>) <span class="co"># Mean</span>
  <span class="kw">lines</span>(newdata<span class="op">$</span>dd[site<span class="op">==</span><span class="st">&#39;Shadow Brook&#39;</span>], PI[site<span class="op">==</span><span class="st">&#39;Shadow Brook&#39;</span>,<span class="dv">2</span>], <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>) <span class="co"># Lower</span>
  <span class="kw">lines</span>(newdata<span class="op">$</span>dd[<span class="dv">1</span><span class="op">:</span><span class="dv">1007</span>], PI[site<span class="op">==</span><span class="st">&#39;Shadow Brook&#39;</span>,<span class="dv">3</span>], <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>) <span class="co"># Upper </span></code></pre></div>
<p><img src="10_glmm_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p><br></p>
<p>We could also do this using our global parameter estimates and some new data. We see that our mean predictions aren’t terrible, but there is quite a bit of uncertainty here.</p>
<p><br></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dd =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">300</span>, <span class="dt">by =</span> <span class="dv">1</span>)
dd2 =<span class="st"> </span>dd<span class="op">^</span><span class="dv">2</span>
pred =<span class="kw">exp</span>(<span class="op">-</span><span class="fl">7.3601897</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.1395633</span><span class="op">*</span>dd <span class="op">-</span><span class="st"> </span><span class="fl">0.0004575</span><span class="op">*</span>dd2)
predlwr =<span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span><span class="fl">7.3601897</span><span class="op">-</span><span class="fl">1.96</span><span class="op">*</span><span class="fl">6.772e-1</span> <span class="op">+</span><span class="st"> </span>(<span class="fl">0.1395633</span><span class="op">-</span><span class="fl">1.96</span><span class="op">*</span><span class="fl">7.488e-3</span>)<span class="op">*</span>dd <span class="op">-</span><span class="st"> </span>(<span class="fl">0.0004575</span><span class="op">-</span><span class="fl">1.96</span><span class="op">*</span><span class="fl">2.479e-5</span>)<span class="op">*</span>dd2)
predupr =<span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span><span class="fl">7.3601897</span><span class="op">+</span><span class="fl">1.96</span><span class="op">*</span><span class="fl">6.772e-1</span> <span class="op">+</span><span class="st"> </span>(<span class="fl">0.1395633</span><span class="op">+</span><span class="fl">1.96</span><span class="op">*</span><span class="fl">7.488e-3</span>)<span class="op">*</span>dd <span class="op">-</span><span class="st"> </span>(<span class="fl">0.0004575</span><span class="op">+</span><span class="fl">1.96</span><span class="op">*</span><span class="fl">2.479e-5</span>)<span class="op">*</span>dd2)

<span class="kw">plot</span>(dd, pred, <span class="dt">type=</span><span class="st">&#39;l&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;blue&#39;</span>, <span class="dt">xlab=</span><span class="st">&#39;degree days&#39;</span>, <span class="dt">ylab=</span><span class="st">&#39;count&#39;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">50</span>))
<span class="kw">lines</span>(predlwr, <span class="dt">col=</span><span class="st">&#39;red&#39;</span>)
<span class="kw">lines</span>(predupr, <span class="dt">col=</span><span class="st">&#39;red&#39;</span>)</code></pre></div>
<p><img src="10_glmm_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p><br></p>
</div>
</div>
</div>
</div>

<!DOCTYPE html>
<p>Copyright &copy; 2017 Dan Stich. All rights reserved.</p>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
