<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Intermediate R</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<!DOCTYPE html>
<head>
<!-- Favicon for various operating systems -->
<link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
<link rel="manifest" href="favicon/site.webmanifest">
<link rel="mask-icon" href="favicon/safari-pinned-tab.svg" color="#603cba">
<link rel="shortcut icon" href="favicon/favicon.ico">
<meta name="msapplication-TileColor" content="#603cba">
<meta name="msapplication-config" content="favicon/browserconfig.xml">
<meta name="theme-color" content="#382121">
</head>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">R4NALMS</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="software.html">Software needs</a>
</li>
<li>
  <a href="morning.html">Morning session</a>
</li>
<li>
  <a href="afternoon.html">Afternoon session</a>
</li>
<li>
  <a href="instructors.html">Instructors</a>
</li>
<li>
  <a href="contact.html">Contact</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Intermediate R</h1>

</div>


<style>
pre code, pre, code {
  white-space: pre !important;
  overflow-x: scroll !important;
  word-break: keep-all !important;
  word-wrap: initial !important;
}

.column {
    float: left;
    padding: 15px;
}

.clearfix::after {
    content: "";
    clear: both;
    display: table;
}

.content {
    width: 75%;
}

</style>
<p><img src="images/roboshad.png" alt=""></p>
<h2 top-margin:10px;>
Introduction
</h2>
<p>Once we have a basic grasp on how R works, and how and where to find help, the learning process becomes a lot less painful, and we can start to build an appreciation for how convenient it is to have a script we can come back to again and again. To show off this convenience, and the power of R as a statistics program, we will spend the afternoon session walking through some applied analyses and spend a little more time with data visualization tools.</p>
<p>The plan for the afternoon is to introduce a subset of the <strong>Secchi Dip-In data</strong> to do some standard statistical analysis with individual data points from many lakes, and then switch back to data visualization tools to create some isopleths of physical parameters measured at multiple times and depths over time in a single lake using the Otsego Lake data from the morning session.</p>
<div id="data-overview-and-management" class="section level2">
<h2>Data overview and management</h2>
<p>Before we ever get into a statistical analysis, it is always good practice to have a good, long look at the data we are working with to make sure everything makes sense, and to note issues that may rear their heads down the road.</p>
<p>Let’s start by reading in the Secchi Dip-In data. This particular data set is a subset of the whole, and contains only those data that were collected in the state of Ohio.</p>
<p>We read the data in with the argument <code>stringsAsFactors = FALSE</code> because there are a lot of string variables in the data, and factors can add extra handling time to our workflow (see <a href="introductory.html"> morning session tutorial </a>).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ohio &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;ohio.csv&#39;</span>, <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p>So, what are we working with here? Let’s have a look.</p>
<p>Remember from the morning session that it might be useful to understand how R sees your data first and foremost. The most reliable method for doing this with dataframes is to look at the <strong>structure</strong> of your data using <code>str</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Like this:</span>
<span class="kw">str</span>(ohio)</code></pre></div>
<p><img src="images/ohio.PNG" alt=""></p>
<p>Now that we have an idea of what the data set looks like, let’s take a little closer look. First, there are a couple of things that we can do that will clean up our code down the road a little bit. Let’s have a quick look at our column names again.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(ohio)</code></pre></div>
<pre><code>##  [1] &quot;Country&quot;                  &quot;Latitude&quot;                
##  [3] &quot;Longitude&quot;                &quot;Year&quot;                    
##  [5] &quot;Month&quot;                    &quot;Day&quot;                     
##  [7] &quot;Secchi..m.&quot;               &quot;NLA..&quot;                   
##  [9] &quot;State&quot;                    &quot;Water.Body&quot;              
## [11] &quot;County..Borough.Parrish.&quot; &quot;GNIS.ID&quot;                 
## [13] &quot;GNIS.Feature.Name&quot;        &quot;GNIS.Class&quot;              
## [15] &quot;GNIS.County&quot;              &quot;GNIS.Latitude&quot;           
## [17] &quot;GNIS.Longitude&quot;           &quot;GNIS.Elevation..m.&quot;</code></pre>
<p>Most of these are nice and clean, but there are some things that happen when we read data into R from Excel files. One of the things that R does is to replace all spaces and special characters with periods (<code>.</code>). This can make things a little difficult to read when we write code. For example, the column that said <code>Secchi (m)</code> in our Excel spreadsheet now says <code>Secchi..m.</code>. This confusion is compounded by the fact that some programming languages and R functions rely on the <code>.</code> for special purposes.</p>
<p>We are not going to replace all of the names, because we are not going to work with all of the columns, but let’s replace a couple of these that we are definitely going to use.</p>
<p>For now, let’s change the names for <code>Secchi..m.</code> and for <code>County..Borough.Parrish.</code>. Remember that the result of <code>names(ohio)</code> is a vector, and that we can replace individual elements of that vector. We just need to know the index of the element we wish to replace. In this case, <code>Secchi..m.</code> is the 7th column of the dataframe <code>ohio</code>, and <code>County..Borough.Parrish.</code> is the 11th.</p>
<p>Here is how it works:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># First, we replace Secchi..m. with Secchi</span>
<span class="kw">names</span>(ohio)[<span class="dv">7</span>] &lt;-<span class="st"> &quot;Secchi&quot;</span>

<span class="co"># Next, we replace County..Borough.Parrish.</span>
<span class="co"># with just County because we are working in a </span>
<span class="co"># single state in this case, not becuase we are</span>
<span class="co"># County-centric :)</span>
<span class="kw">names</span>(ohio)[<span class="dv">11</span>] &lt;-<span class="st"> &quot;County&quot;</span></code></pre></div>
<p>Of course, we could always do this in one fell swoop:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Replace names of both at once</span>
<span class="co"># because that is cooler than one</span>
<span class="co"># at a time.</span>
<span class="kw">names</span>(ohio)[<span class="kw">c</span>(<span class="dv">7</span>,<span class="dv">11</span>)] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;Secchi&#39;</span>, <span class="st">&#39;County&#39;</span>)</code></pre></div>
<p>Have a quick look to make sure you are happy with the new names.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Print the names of the df</span>
<span class="co"># to the console so we can </span>
<span class="co"># see them.</span>
<span class="kw">names</span>(ohio)</code></pre></div>
<pre><code>##  [1] &quot;Country&quot;            &quot;Latitude&quot;           &quot;Longitude&quot;         
##  [4] &quot;Year&quot;               &quot;Month&quot;              &quot;Day&quot;               
##  [7] &quot;Secchi&quot;             &quot;NLA..&quot;              &quot;State&quot;             
## [10] &quot;Water.Body&quot;         &quot;County&quot;             &quot;GNIS.ID&quot;           
## [13] &quot;GNIS.Feature.Name&quot;  &quot;GNIS.Class&quot;         &quot;GNIS.County&quot;       
## [16] &quot;GNIS.Latitude&quot;      &quot;GNIS.Longitude&quot;     &quot;GNIS.Elevation..m.&quot;</code></pre>
<p>Notice that above, we need to put indices inside of a call to <code>c</code> (the concatenate function) because otherwise R tries to interpret these as row and column indices and the program will return an error.</p>
</div>
<div id="data-exploration" class="section level2">
<h2>Data exploration</h2>
<p>Now that we have had a quick look at our data, and we have made some changes for the sake of convenience, let’s dig a little deeper.</p>
<p>For this afternoon, we are going to use <code>Secchi</code> as our <strong>response</strong>, or <strong>independent</strong> variable to demonstrate some basic statistical techniques in R. Before we can do any actual statistics, though, it is good practice to scrutinize the data we intend to use.</p>
<p>To start with, let’s take a quick look at Secchi depth using a histogram.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make the histogram</span>
<span class="kw">hist</span>(ohio<span class="op">$</span>Secchi, 
     <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>,
     <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>,
     <span class="dt">xlab=</span><span class="st">&#39;Secchi depth (m)&#39;</span>,
     <span class="dt">main=</span><span class="st">&#39;&#39;</span>)
<span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">pos=</span><span class="dv">0</span>)
<span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">2</span>, <span class="dt">las=</span><span class="dv">2</span>, <span class="dt">pos=</span><span class="dv">0</span>)</code></pre></div>
<p><img src="intermediate_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>From this plot alone, there are a few things that should be obvious.</p>
<p>First, we can see that the distribution of our response is <strong>right-skewed</strong>, with many more observations near zero than near the maximum.</p>
<p>Second, perhaps more nuanced, is that there are no values less than zero. For anyone who has spent some time using a Secchi disk, the reason for this is probably obvious. We can’t have negative values for light penetration into the water column<sup>[citation needed]</sup>. This variable is <strong>positive definitive</strong>. This is actually common for a lot of parameters we measure in lake management because we frequently are interested in concentrations, depths, areas, and other non-negative quantities.</p>
<p>We will need to think about both of these characteristics as we move into statistical analyses. One of the central assumptions of modern regression tools relates to normality of residuals, which in the absence of heterogenous groups, can initially be approximated using a histogram of the response. To show this, we can plot the residuals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make the histogram, this time</span>
<span class="co"># subtracting the mean from each </span>
<span class="co"># value. Is the result normal?</span>
<span class="kw">hist</span>(ohio<span class="op">$</span>Secchi<span class="op">-</span><span class="kw">mean</span>(ohio<span class="op">$</span>Secchi),
     <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>, 
     <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>,
     <span class="dt">xlab=</span><span class="st">&#39;Error&#39;</span>,
     <span class="dt">main=</span><span class="st">&#39;&#39;</span>)
<span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">pos=</span><span class="dv">0</span>)
<span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">2</span>, <span class="dt">las=</span><span class="dv">2</span>, <span class="dt">pos=</span><span class="op">-</span><span class="dv">1</span>)</code></pre></div>
<p><img src="intermediate_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>This looks pretty much the same as the one above, but the x-axis has changed. We should know, at this point that the distribution above is decidedly <em>not</em> normal.</p>
</div>
<div id="transformations" class="section level2">
<h2>Transformations</h2>
<p>We can handle both of these problems by log-transforming our data like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ohio<span class="op">$</span>logSecchi &lt;-<span class="st"> </span><span class="kw">log</span>(ohio<span class="op">$</span>Secchi)</code></pre></div>
<p>We can plot histogram of the residuals again to see what it did:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make the histogram,</span>
<span class="co"># subtracting the mean from each </span>
<span class="co"># value. Is the result normal?</span>
<span class="kw">hist</span>(ohio<span class="op">$</span>logSecchi<span class="op">-</span><span class="kw">mean</span>(ohio<span class="op">$</span>logSecchi),
     <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>, 
     <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>,
     <span class="dt">xlab=</span><span class="st">&#39;Error&#39;</span>,
     <span class="dt">main=</span><span class="st">&#39;&#39;</span>)
<span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">pos=</span><span class="dv">0</span>)
<span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">2</span>, <span class="dt">las=</span><span class="dv">2</span>, <span class="dt">pos=</span><span class="op">-</span><span class="fl">3.5</span>)</code></pre></div>
<p><img src="intermediate_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Clear to see here that our residuals look much more like a normal distribution now.</p>
<p>What has this done to our data, though? Let’s have a look:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make the histogram.</span>
<span class="kw">hist</span>(ohio<span class="op">$</span>logSecchi,
     <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>, 
     <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>, <span class="dt">xaxt=</span><span class="st">&#39;n&#39;</span>,
     <span class="dt">xlab=</span><span class="st">&#39;Error&#39;</span>,
     <span class="dt">main=</span><span class="st">&#39;&#39;</span>)
<span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">1</span>, <span class="dt">pos=</span><span class="dv">0</span>)
<span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">2</span>, <span class="dt">las=</span><span class="dv">2</span>, <span class="dt">pos=</span><span class="op">-</span><span class="dv">4</span>)</code></pre></div>
<p><img src="intermediate_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>As you can see, our response variable is no longer constrained to be greater than zero on the log scale, so we don’t have to worry about how that influences normality in our residuals anymore, and we won’t get negative predictions from any of the statistical models that we make.</p>
<p>Now we can move forward with some statistics!</p>
</div>
<div id="introductory-statistics-in-r" class="section level2">
<h2>Introductory statistics in R</h2>
<p>Let’s start with the simple case of comparing <code>Secchi</code> between lakes and reservoirs. If you have a basic understanding of statistics, you might immediately realize that this is a comparison of a continuous variable between two groups. We have a couple of paths forward here. We could either set aside distributional assumpstions and use <strong>non-parametric</strong> methods, or we could assume some distribution for our error structure and proceed using <strong>parametric</strong> or <strong>semi-parametric</strong> statistics.</p>
<p>In either case, we are going to have to make sure we only have data for lakes and reservoirs first. (If you looked closely at the data you will have realized that there are also records for <code>Dam</code> in our dataset.)</p>
<p>To get rid of the observations related to <code>Dam</code> for now, let’s just drop that level from our <code>GNIS.Class</code> variable in the <code>ohio</code> data set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ohio &lt;-<span class="st"> </span>ohio[ohio<span class="op">$</span>GNIS.Class<span class="op">==</span><span class="st">&quot;Lake&quot;</span> <span class="op">|</span><span class="st"> </span>ohio<span class="op">$</span>GNIS.Class<span class="op">==</span><span class="st">&quot;Reservoir&quot;</span>, ]</code></pre></div>
<div id="wilcoxon-rank-sums-test" class="section level3">
<h3>Wilcoxon rank-sums test</h3>
<p>Let’s start by fitting a quick Wilcoxon rank-sums test (aka Mann-Whitney U-test). While our options for non-parametric statistics are limited in complexity, they do not require distributional assumptions, and they tend to be more <strong>conservative</strong> than parametric tests, especially under limited sample sizes (not the case here).</p>
<p>Here is how we do it:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Wilcox test to assess the null hypothesis</span>
<span class="co"># that there is no difference in Secchi between</span>
<span class="co"># lakes and reservoirs.</span>
<span class="kw">wilcox.test</span>(<span class="dt">x=</span>ohio<span class="op">$</span>Secchi[ohio<span class="op">$</span>GNIS.Class<span class="op">==</span><span class="st">&#39;Lake&#39;</span>],
            <span class="dt">y=</span>ohio<span class="op">$</span>Secchi[ohio<span class="op">$</span>GNIS.Class<span class="op">==</span><span class="st">&#39;Reservoir&#39;</span>])</code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  ohio$Secchi[ohio$GNIS.Class == &quot;Lake&quot;] and ohio$Secchi[ohio$GNIS.Class == &quot;Reservoir&quot;]
## W = 12057000, p-value &lt; 2.2e-16
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>Okay, if you have never done a Wilcox test by hand, or even in Excel, please appreciate how ridiculously easy this is. But, more importantly, what is going in the output here?</p>
<p>First, we can see that we have conducted a Wilcoxon rank sum test. Second, R gives us a friendly reminder of what our data were that went into this. Next, we have the actual statistics for the test, including the test statistic <code>W</code> and the <code>p-value</code> for the comparison. With the handy-dandy <code>alternative hypothesis</code> that R provides us, we can conclude that since p &lt; 0.05 we reject the null hypothesis that there is no difference between groups.</p>
<p>While this is useful for making inference, we don’t get a good sense of how the group means actually differed from this test (although we could graph it). For that, we need to start making some assumptions about our data and the shape of the error associated with those data.</p>
</div>
<div id="parametric-statistics-in-r" class="section level3">
<h3>Parametric statistics in R</h3>
<p>Parametric statistics rely on some basic <strong>assumptions</strong> about or data and the error structure around our observations. There are a number of fundamental assumptions that guide our forays into what is essentially “the rest of statistics”. How we deal with these assumptions can range from doing nothing, to data transormation, to use of alternative model structures that allow us to shift or relax them.</p>
<p>What are these assumptions? Glad you ask!</p>
</div>
<div id="assumptions-of-linear-models" class="section level3">
<h3>Assumptions of linear models</h3>
<p>Linear models you say? Yes. While we don’t have time to go in the math behind this (ahem, <strong>the beautiful, unifying math</strong> behind this), suffice to say that most of the statistics we use in aquatic research are special cases, or generalizations, of the linear model. This includes things like t-tests, linear regression, ANOVA, ANCOVA, GLM, GLMM, GLS, GEE, and even multivariate statistics. The same holds true whether we are working with classical frequentist tools relying on least-squares estimation and maximum likelihood, or Bayesian methods using MCMC estimation. That means we always need to have these assumptions in mind.</p>
<p>Briefly, these assumptions include:</p>
<p><strong>1.</strong> Independence of observations.</p>
<p><strong>2.</strong> Normality of residuals (with mean=0).</p>
<p><strong>3.</strong> Homogeneity of variances (i.e. homoscedasticity).</p>
<p>We will discuss quickly in the afternoon session, but a more detailed explanation of each can be found <a href="assumptions.html"> here </a>.</p>
</div>
<div id="the-t-test" class="section level3">
<h3>The t-test</h3>
<p>The t-test is our go-to tool for comparing two group means with parametric statistics. Even when we use analysis of variance (ANOVA) to test for differences in means between more than two groups, we still need to use t-tests to follow up and determine pair-wise differences (usually using a Tukey HSD or analagous test).</p>
<p>Using a t-test in R is pretty straightforward. For this example, we will use a t-test to test the null hypothesis that there is no difference in <code>Secchi</code> between <code>Lake</code> and <code>Reservoir</code> classes. To do this, we use the function <code>t.test</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># T-test to assess the null hypothesis</span>
<span class="co"># that there is no difference in Secchi</span>
<span class="co"># between lakes and reservoirs in Ohio.</span>

<span class="co"># We use logSecchi to meet assumptions</span>
<span class="co"># of normality.</span>

<span class="co"># We can specify this one using a formula.</span>
<span class="co"># To be conservative here, we will assume </span>
<span class="co"># that we have unequal variances using</span>
<span class="co"># one of the optional arguments. Note that</span>
<span class="co"># the default in R is to assume that variances</span>
<span class="co"># are not equal, and this defaults to </span>
<span class="co"># a Welch&#39;s t-test that uses a calculated df</span>
<span class="co"># to adjust the calculated test statistic.</span>
<span class="kw">t.test</span>(logSecchi<span class="op">~</span>GNIS.Class, <span class="dt">data=</span>ohio, <span class="dt">equal=</span><span class="ot">FALSE</span>)</code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  logSecchi by GNIS.Class
## t = 33.218, df = 1677.6, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.6144321 0.6915445
## sample estimates:
##      mean in group Lake mean in group Reservoir 
##               0.1375958              -0.5153925</code></pre>
<p>Yay, more output!</p>
<p>Similar to the <code>wilcox.test</code>, the output of our <code>t.test</code> gives us the test that was used, the data provided to the function, the test statistics (now with <code>df</code>), and the <code>alternative hypothesis</code> to be accepted if p &lt; 0.05.</p>
<p>But, in contrast to the Wilcox, we have a little more information to go on.</p>
<p>First of all, we get an actual confidence interval our estimate of the difference between groups. Here, we see that we are 95% confident that the true difference in <code>logSecchi</code> between <code>Lake</code> and <code>Reservoir</code> is between 0.61 and 0.69. Note that this difference is positive because we specified <code>x</code> as the <code>logSecchi</code> for <code>Lake</code> and <code>y</code> as the <code>logSecchi</code> for <code>Reservoir</code>. If we had reversed this, our 95% confidence interval on the estimated difference would have used negative values.</p>
<p>R also tells us that the mean of <code>logSecchi</code> is 0.14 for <code>Lake</code> and -0.52 for <code>Reservoir</code>. We can bring these back to the real scale using the function <code>exp</code> to say that the mean of <code>Secchi</code> is 1.15 for <code>Lake</code> and 0.6 for <code>Reservoir</code>.</p>
<p>We could go about showing these differences graphically using (for example) a boxplot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make a boxplot of Secchi by waterbody type</span>

<span class="co"># Specify notch=TRUE to get a visual</span>
<span class="co"># approximation of significance by comparing</span>
<span class="co"># the spread of the notches. In this case, we</span>
<span class="co"># have a ton of data, so the notches are barely</span>
<span class="co"># visible...something to think about when you</span>
<span class="co"># are doing hypothesis testing with tons</span>
<span class="co"># of data.</span>

<span class="co"># We make the boxes narrower because</span>
<span class="co"># flat, wide boxes look gross and </span>
<span class="co"># make people not want to use R for</span>
<span class="co"># graphing even though it is awesome.</span>
<span class="kw">boxplot</span>(logSecchi<span class="op">~</span>GNIS.Class,
        <span class="dt">data =</span> ohio,
        <span class="dt">notch=</span><span class="ot">TRUE</span>,
        <span class="dt">col=</span><span class="st">&#39;gray87&#39;</span>,
        <span class="dt">ylab=</span><span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&#39;log&#39;</span>[e],<span class="st">&#39;Secchi depth&#39;</span>)),
        <span class="dt">boxwex =</span> .<span class="dv">25</span>,
        <span class="dt">outline=</span><span class="ot">FALSE</span>,
        <span class="dt">yaxt=</span><span class="st">&#39;n&#39;</span>,
        <span class="co"># Other parameters to make</span>
        <span class="co"># it prettier</span>
        <span class="dt">pars =</span> <span class="kw">list</span>(
          <span class="dt">staplewex=</span><span class="dv">0</span>,
          <span class="dt">whisklty=</span><span class="dv">1</span>,
          <span class="dt">whisklwd=</span><span class="dv">2</span>,
          <span class="dt">whiskcol=</span><span class="st">&#39;gray40&#39;</span>,
          <span class="dt">boxlwd=</span><span class="dv">2</span>,
          <span class="dt">boxcol=</span><span class="st">&#39;gray40&#39;</span>
        )
        )
<span class="kw">axis</span>(<span class="dt">side=</span><span class="dv">2</span>, <span class="dt">las=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="intermediate_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
</div>
<div id="analysis-of-variance" class="section level3">
<h3>Analysis of variance</h3>
<p>It is only a small step to move from a t-test to analysis of variance (ANOVA) conceptually, and this also requires only a small change to code in R. In theory, ANOVA is usually used to compare means of a continuous response when we have three or more groups. In practice, it is mathematically equivelant to a t-test if we have only two groups (the F-statistic is, in fact, just the sqaure of the t-statistic).</p>
<p>To demonstrate this, and to keep ourselves from being swept afield with another data set, let’s demonstrate ANOVA using the same scenario.</p>
<p>It can be more convenient to store the results of ANOVA in an object than just dumping the output to the console, so let’s start there. First, we will fit our ANOVA with the <code>lm</code> function (stands for linear model) because this is the most general tool for us.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit an ANOVA to test for differences in</span>
<span class="co"># means between groups</span>
mod &lt;-<span class="st"> </span><span class="kw">lm</span>(logSecchi<span class="op">~</span>GNIS.Class, <span class="dt">data=</span>ohio)</code></pre></div>
<p>Now, before we get into looking at p-values here, it is <strong>important</strong> that <strong>we diagnose the model first</strong>.</p>
<blockquote>
<p>Residual plots for linear models</p>
</blockquote>
<blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># There are four plots that come out</span>
<span class="co"># of this call. To see them all at </span>
<span class="co"># once, you need to set up the plotting</span>
<span class="co"># window to accomodate that. Otherwise, </span>
<span class="co"># you have to hit ENTER to scroll</span>
<span class="co"># through them one at a time.</span>

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))

<span class="co"># The default plot function knows what</span>
<span class="co"># to do with lm objects.</span>

<span class="kw">plot</span>(mod)</code></pre></div>
<p><img src="intermediate_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</blockquote>
<blockquote>
<p><strong>Top left</strong></p>
</blockquote>
<p>We want to see two things here:</p>
<p><strong>1.</strong> Most of our data should be contained on the interval (-3,3), so it looks like we are good to go here.</p>
<p><strong>2.</strong> If the residuals are normally distributed, we should see what looks like random scatter in the plots. When we have groups like this, the scatter of points should be approximately the same between the two values. Loooks like we are good here, too.</p>
<blockquote>
<p><strong>Bottom left</strong></p>
</blockquote>
<p><strong>1.</strong> Most of our data should be less than 3 (this is the square root of standardized residual so they are all positive).</p>
<p><strong>2.</strong> We should see random scatter in these as well, and interpretation of homogeneity follows the same process.</p>
<blockquote>
<p><strong>Top right</strong></p>
</blockquote>
<p>This is the one we are most interested in for examining the normality of our residual errors. If our residuals are normally distributed, then the points on this plot should (approximately) follow the straight, dotted line here. Q-Q plots, like others, can also be useful for identifying outliers in our data. These are labeled.</p>
<blockquote>
<p><strong>Bottom right</strong></p>
</blockquote>
<p>This plot is useful for identifying points that might be exerting undo influence on the intercept and slope of the line that we are trying to fit here. In general we are looking for values of Cook’s D greater than <span class="math inline">\(\frac{4}{(N-k-1)}\)</span> where <span class="math inline">\(N\)</span> is sample size and <span class="math inline">\(k\)</span> is number of explanatory variables, if we are going to set a threshold: <a href="http://stats.stackexchange.com/questions/22161/how-to-read-cooks-distance-plots">check it out.</a>. Here, we see that most of our data are within this, but we might have a couple of data points worth looking into.</p>
<p>Now that we have verified that we are not in gross violation of our assumptions, let’s have a look at the results.</p>
<p>We can make an ANOVA summary table in R using the lowercase <code>anova</code> function to get overall factor significance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Get ANOVA summary for the model</span>
<span class="kw">anova</span>(mod)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: logSecchi
##               Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## GNIS.Class     1  493.6  493.64  783.48 &lt; 2.2e-16 ***
## Residuals  14156 8919.2    0.63                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Here, we can see that out factor <code>GNIS.Class</code> had a significant effect on <code>logSecchi</code>, and we can reject the null hypothesis that there is no difference between any of the group means.</p>
<p>To poke this a bit further, we can ask R to conduct a <strong>multiple-comparisons</strong> test to examine <strong>pair-wise differences</strong>. I usually use a <code>TukeyHSD</code> for this. To use this test, we actually need to go back and fit our model using the <code>aov</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">TukeyHSD</span>( <span class="kw">aov</span>(mod))</code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = mod)
## 
## $GNIS.Class
##                      diff        lwr        upr p adj
## Reservoir-Lake -0.6529883 -0.6987157 -0.6072609     0</code></pre>
<p>The output from this function produces a mean and CI of the difference for each pair-wise comparison, along with a p-value for the test. In this case, we see that we only have one comparison, but that <code>logSecchi</code> is significantly different between groups.</p>
<p>From there, we can go on to make group-specific boxplots of the response to make the difference clearer for the reader. One of the cool things about doing this in R is that the code for the boxplot above wouldn’t change, even if we had more than two groups in our data.</p>
</div>
<div id="linear-regression" class="section level3">
<h3>Linear Regression</h3>
<p>Non-parametric statistics, t-tests, and ANOVAs are great for testing hypotheses about differences between groups, but they don’t really allow us to examine changes that occur along continual gradients. In our fields, those kinds of changes are often the things we are actually interested in. For example, we may wish to understand how temperature or secchi change with year to quantify long-term trends. To do this, we will rely on linear regression.</p>
<p>Linear regression takes the familiar form of the equation for a line:</p>
<p><span class="math inline">\(y = mx + b\)</span></p>
<p>Where <span class="math inline">\(y\)</span> is the dependent variable, <span class="math inline">\(b\)</span> is the y-intercept, <span class="math inline">\(m\)</span> is the slope of the line, and <span class="math inline">\(X\)</span> is the independent variable.</p>
<p>In statistics, we write this as</p>
<p><span class="math inline">\(y_{i} = \beta_{0} + \beta_{j} \cdot X_{j,i} + \epsilon_{i}\)</span></p>
<p>Where <span class="math inline">\(y_{i}\)</span> is the i<sup>th</sup> value of the dependent variable, <span class="math inline">\(\beta_{0}\)</span> is the y-intercept, <span class="math inline">\(\beta_{j}\)</span> is the slope of the line, <span class="math inline">\(X_{j,i}\)</span> is an independent variable, and <span class="math inline">\(\epsilon_{i}\)</span> is our error (assumed to have a mean of zero so it can be dropped- that’s why it is important to check).</p>
<p>More generally, we could extend this to have an arbitrarily large number of slopes and independent variables (i.e., multiple regression):</p>
<p><span class="math inline">\(y_{i} = \beta_{0} + \sum_{\substack{j=0}}^k (\beta_{j} \cdot X_{j,i}, ..., \beta_{k} \cdot X_{k,i}) + \epsilon_{i}\)</span></p>
<p>What fun!</p>
<p>How about fitting this kind of model in R? It will take us less time than it did to write out the equations.</p>
<p>Here, we will fit a linear regression to test the effects of <code>Latitude</code> and <code>Year</code> on <code>logSecchi</code>. To do this, we can stick with the <code>lm</code> function. But first, we need to do some work to get <code>Year</code> into a format we can use.</p>
<p>Right now, <code>Year</code> is stored as a character string. But, we need to have it as a numeric. We can convert this to a numeric easily:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Change </span>
ohio<span class="op">$</span>Year &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(ohio<span class="op">$</span>Year, <span class="st">&#39;Year &#39;</span>)</code></pre></div>
<pre><code>## Warning: NAs introduced by coercion</code></pre>
<p><strong>Note</strong>: You will get a warning letting you know that <code>NA</code> values have been introduced by <code>coersion</code>. This means that R found some values that it didn’t know how to convert to <code>numeric</code>. In this case, it is obvservations that were reported as means across a range of dates (e.g., <code>1972-1975</code>). R will ignore these <code>NA</code> values in statistical analyses, and we have plenty of data from that range, so we will just move on with life for now.</p>
<p>Next, we will fit the model using the same <code>lm</code> function that we used for ANOVA above. The reason we can do this is that ANOVA, t-tests, and linear regressions are all just specific cases of the linear model.</p>
<p>First, we will fit a model testing the effect of <code>Latitude</code> on <code>logSecchi</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lmod &lt;-<span class="st"> </span><span class="kw">lm</span>(logSecchi<span class="op">~</span>Latitude, <span class="dt">data=</span>ohio)</code></pre></div>
<p>Don’t forget to look at the residuals!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Look at residuals for model</span>
<span class="co"># testing effect of latitude</span>
<span class="co"># on logSecchi</span>
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(lmod)</code></pre></div>
<p><img src="intermediate_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>From our residual diagnostics, we see that we have some minor issues with the residuals and leverage exerted by what appear to be our our most extreme Secchi readings, but that overall things look pretty good. We do have some points that might warrant further investigation if we were going to submit this for journal publication. For now, let’s move along with the demonstration.</p>
<p>We can get the regression coefficients for our model directly from a summary of a fitted model object.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(lmod)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = logSecchi ~ Latitude, data = ohio)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2381 -0.6839 -0.0286  0.5654  2.8122 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.971515   0.381840   7.782 7.64e-15 ***
## Latitude    -0.085314   0.009467  -9.012  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8075 on 13888 degrees of freedom
##   (268 observations deleted due to missingness)
## Multiple R-squared:  0.005814,   Adjusted R-squared:  0.005742 
## F-statistic: 81.21 on 1 and 13888 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Here, <code>(Intercept)</code> is the y-intercept for our model, and <code>Latitude</code> is our “slope”. In this case, we see that our y-intercept is significantly different from zero (p = 7.6e-15), and detected a significant effect of <code>Latitude</code> on <code>logSecchi</code> (p = 2.3e-19). The interpretation of the latter is that the slope is not equal to zero.</p>
<p>Looking a little closer at the output, we can also see that we have explained less than 1% of the variability in <code>logSecchi</code> using <code>Latitude</code> as an explanatory variable (R<sup>2</sup> = 0.006). But, let’s plot the result anyway.</p>
<p>In order to plot the result of our model, we need to make some predictions from it first. To do this, we will need some new data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make new values of Latitude</span>
<span class="co"># that we can use to predict </span>
<span class="co"># logSecchi. If we want to use</span>
<span class="co"># the default predict function,</span>
<span class="co"># this has to be a variable</span>
<span class="co"># named &quot;Latitude&quot; in a new </span>
<span class="co"># dataframe that we can give</span>
<span class="co"># to R.</span>

<span class="co"># To make these new values, we </span>
<span class="co"># use a sequence that goes from</span>
<span class="co"># the minimum to the maximum of</span>
<span class="co"># our observed values and avoid</span>
<span class="co"># making predictions outside the</span>
<span class="co"># observed range of Latitudes.</span>

<span class="co"># We use a sequence to ensure </span>
<span class="co"># that we only get one of each</span>
<span class="co"># value so we can make some nice</span>
<span class="co"># line graphs of our predictions.</span>

newd &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">Latitude =</span> <span class="kw">seq</span>(
    <span class="dt">from=</span><span class="kw">min</span>(ohio<span class="op">$</span>Latitude, <span class="dt">na.rm=</span>T),
    <span class="dt">to=</span><span class="kw">max</span>(ohio<span class="op">$</span>Latitude, <span class="dt">na.rm=</span>T),
    <span class="dt">by=</span>.<span class="dv">01</span>
    )
  )</code></pre></div>
<p>Now, we can use our new values to predict logSecchi from the parameters of our linear regression model. We use the default <code>predict</code> function in R for this.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Make predictions from the model using</span>
<span class="co"># the new data.</span>

<span class="co"># We specify interval=&#39;prediction&#39; to</span>
<span class="co"># get prediction intervals, but we could</span>
<span class="co"># also ask for &#39;confidence&#39; intervals.</span>
preds &lt;-<span class="st"> </span><span class="kw">predict</span>(<span class="dt">object =</span> lmod,
                 <span class="dt">newdata =</span> newd,
                 <span class="dt">interval =</span> <span class="st">&#39;prediction&#39;</span>
                 )</code></pre></div>
<p>Let’s have a look at what our predictions look like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Take a look at the first few</span>
<span class="co"># rows of the preds dataframe</span>
<span class="kw">head</span>(preds)</code></pre></div>
<pre><code>##          fit       lwr      upr
## 1 -0.3323568 -1.915490 1.250776
## 2 -0.3332100 -1.916340 1.249920
## 3 -0.3340631 -1.917189 1.249063
## 4 -0.3349163 -1.918039 1.248207
## 5 -0.3357694 -1.918889 1.247350
## 6 -0.3366225 -1.919739 1.246493</code></pre>
<p><strong>Whoa</strong>! Why are all of our predicted values negative? This is because they are on the log<sub>e</sub> scale. We can get them back on the real scale by exponentiating. Here, we do this using the <code>apply</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Apply the exponentiation </span>
<span class="co"># to each column in the </span>
<span class="co"># dataframe.</span>
preds &lt;-<span class="st"> </span><span class="kw">apply</span>(<span class="dt">X=</span>preds, <span class="dt">MARGIN=</span><span class="dv">2</span>, <span class="dt">FUN=</span>exp)

<span class="co"># Have a look at the first</span>
<span class="co"># few rows of the df</span>
<span class="kw">head</span>(preds)</code></pre></div>
<pre><code>##         fit       lwr      upr
## 1 0.7172314 0.1472697 3.493054
## 2 0.7166197 0.1471446 3.490063
## 3 0.7160086 0.1470196 3.487075
## 4 0.7153980 0.1468947 3.484089
## 5 0.7147879 0.1467700 3.481106
## 6 0.7141784 0.1466453 3.478125</code></pre>
<p>These look a little more reasonable. Now, we can just make a plot of the relationship. We start with the raw data, and add our predictions from there.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="dt">x =</span> ohio<span class="op">$</span>Latitude,
     <span class="dt">y =</span> ohio<span class="op">$</span>Secchi,
     <span class="dt">xlab =</span> <span class="st">&#39;Latitude (decimal min)&#39;</span>,
     <span class="dt">ylab =</span> <span class="st">&#39;Secchi depth (m)&#39;</span>,
     <span class="dt">pch=</span><span class="dv">21</span>,
     <span class="dt">col=</span><span class="kw">rgb</span>(<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="fl">0.10</span>),
     <span class="dt">bg=</span><span class="kw">rgb</span>(<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="fl">0.5</span>,<span class="fl">0.10</span>),
     <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">10</span>)
     )

<span class="co"># Now, add the lines for the mean, lower, and</span>
<span class="co"># upper CIs from the model that we used</span>
<span class="kw">lines</span>(newd<span class="op">$</span>Latitude, preds[,<span class="dv">1</span>], <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&#39;blue&#39;</span>)
<span class="kw">lines</span>(newd<span class="op">$</span>Latitude, preds[,<span class="dv">2</span>], <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;red&#39;</span>)
<span class="kw">lines</span>(newd<span class="op">$</span>Latitude, preds[,<span class="dv">3</span>], <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&#39;red&#39;</span>)</code></pre></div>
<p><img src="intermediate_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>Wow, this graph is almost as underwhelming as the R<sup>2</sup> for the model that was used to create it. <strong>This should highlight the importance of communicating biological effect sizes</strong>. If we had just reported p-values, this would have gone un-noticed and someone might have misused the result. This is why we chose to retain this analysis for the workshop, and is also the reason why you should ask to see this kind of result communicated as a responsible reviewer of peer-reviewed research.</p>
<p>[gets off stump]</p>
<p>For a more exciting example, you can have a look at <a href='thermoclines.html'> this </a> thermocline analysis. We may swing back to this if we have time.</p>
<p>Next, let’s fit a model for <code>Year</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit a linear regression model to test</span>
<span class="co"># effect of `year` on `logSecchi`</span>
ymod &lt;-<span class="st"> </span><span class="kw">lm</span>(logSecchi<span class="op">~</span>Year, <span class="dt">data=</span>ohio)</code></pre></div>
<p>We can summarize the model again like so:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(ymod)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = logSecchi ~ Year, data = ohio)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2193 -0.7285 -0.0383  0.5924  2.7728 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -1.9203066  2.3062698  -0.833    0.405
## Year         0.0007337  0.0011545   0.635    0.525
## 
## Residual standard error: 0.8168 on 13984 degrees of freedom
##   (172 observations deleted due to missingness)
## Multiple R-squared:  2.888e-05,  Adjusted R-squared:  -4.263e-05 
## F-statistic: 0.4039 on 1 and 13984 DF,  p-value: 0.5251</code></pre>
<p>We see that our intercept is not significantly different from zero (p = 0.4050586), and we failed to detect a significant effect of <code>Year</code> on <code>logSecchi</code> (p = 0.525116).</p>
<p>We can also see that the R<sup>2</sup> indicates we have explained virtually none of the variability in <code>logSecchi</code> using <code>Year</code> as an explanatory variable.</p>
<p><strong>But</strong>, what if there was some really cool way that we could look at differences in the effect of <code>Year</code> on <code>logSecchi</code> between <code>Lake</code> and <code>Reservoir</code>?</p>
<p>Another fantastic question!</p>
</div>
<div id="analysis-of-covariance" class="section level3">
<h3>Analysis of covariance</h3>
<p>The final statistical analysis that we will discuss today is the analysis of covariance, or ANCOVA. The ANCOVA (aka general linear model) is the most general form of the class of linear models to which the t-test, ANOVA, and linear regression all belong.</p>
<p>Here, we will specify an ANCOVA to simultaneously test the effects of waterbody type (<code>GNIS.Class</code>) and <code>Year</code> on <code>logSecchi</code>. We will allow the relationship between <code>Year</code> and <code>logSecchi</code> to vary between <code>Lake</code> and <code>Reservoir</code> by specifying an <strong>interaction</strong> term in our regression model. We will forgo plotting the results of this model, as that goes beyond the scope of an introductory R class, but we may swing back to this if time alots. More information about plotting the results of this kind of model can be found <a href="http://employees.oneonta.edu/stichds/classes/BIOL217/06_diagnosticsAndEffects.html"> here </a>.</p>
<p>To fit this type of model in R, it is a pretty easy change to the formulas we have been using. But, we need to use the <code>Anova</code> function from the <code>car</code> library to create our ANOVA summary because we need to use a Type-III sum of squares for our calculations now.</p>
<p>First, we can see that if we combine <code>GNIS.Class</code> and <code>Year</code> in an <strong>additive</strong> way (main-effects only), we get the same results as we did in our individual ANOVA and linear regression models. We see a significant effect of <code>GNIS.Class</code>, and fail to detect a main effect of <code>Year</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(car)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Fit the model and store it to an object</span>
mainmod &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="dt">formula=</span>logSecchi<span class="op">~</span>GNIS.Class <span class="op">+</span><span class="st"> </span>Year,
              <span class="dt">data=</span>ohio)

<span class="co"># Take a look at the summary of the model</span>
<span class="kw">library</span>(car, <span class="dt">lib.loc =</span> <span class="st">&#39;r_libs/&#39;</span>)
<span class="kw">Anova</span>(mainmod)</code></pre></div>
<pre><code>## Anova Table (Type II tests)
## 
## Response: logSecchi
##            Sum Sq    Df  F value Pr(&gt;F)    
## GNIS.Class  490.6     1 776.0626 &lt;2e-16 ***
## Year          0.0     1   0.0488 0.8252    
## Residuals  8840.0 13983                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Next, we can modify this to include an interaction term. At this point, the change is trivial, whether or not you feel like you know what you are doing at this point.</p>
<pre><code>## Anova Table (Type II tests)
## 
## Response: logSecchi
##                 Sum Sq    Df  F value    Pr(&gt;F)    
## GNIS.Class       490.6     1 777.0533 &lt; 2.2e-16 ***
## Year               0.0     1   0.0489    0.8251    
## GNIS.Class:Year   11.9     1  18.8511 1.423e-05 ***
## Residuals       8828.1 13982                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>In this case, we can see that the interaction between <code>GNIS.Class</code> and <code>Year</code> is significant, meaning that there are differences in the relationship between <code>logSecchi</code> and <code>Year</code> depending on what kind of waterbody we are looking at.</p>
<p>If we dig a little deeper into the model, we can see what these relationships look like.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(intmod)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = logSecchi ~ GNIS.Class * Year, data = ohio)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.16198 -0.66448 -0.03279  0.56154  2.71918 
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)               25.307554   6.323025   4.002 6.30e-05 ***
## GNIS.ClassReservoir      -30.015499   6.763159  -4.438 9.15e-06 ***
## Year                      -0.012598   0.003165  -3.981 6.91e-05 ***
## GNIS.ClassReservoir:Year   0.014698   0.003385   4.342 1.42e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7946 on 13982 degrees of freedom
##   (172 observations deleted due to missingness)
## Multiple R-squared:  0.05389,    Adjusted R-squared:  0.05368 
## F-statistic: 265.4 on 3 and 13982 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>This model summary shows us that, in general, <code>Reservoir</code> appears to have lower Secchi depths than <code>Lake</code> (Intercept). We can also see that there was a significant decrease in Secchi depth across <code>Year</code> because the coefficient for that variable is <strong>negative</strong>, and the 95% CI for the parameter (calculated as 1.96 * SE) does not overlap zero. But that decrease was significantly less in <code>Reservoir</code> because of the positive change in slope from -0.013 to -0.013 + 0.015 = 0.002. If we derived the 95% CI around this parameter estimate by propogating errors, we would find that it overlaps zero significantly, and would thus conclude that we failed to detect an effect of <code>Year</code> on <code>logSecchi</code> for <code>Reservoir</code>.</p>
</div>
</div>
<div id="response-surfaces-isopleths" class="section level2">
<h2>Response surfaces (isopleths)</h2>
<p>When we have more than one continuous variable in a regression model, it can be helpful to think about <strong>surfaces</strong> instead of <strong>lines</strong>. While it sounds difficult conceptually, these are actually fairly common tools in our everyday lives. Elevation contours are one such example of a response surface (think <code>formula=elevation~Latitude*Longitude</code> in the simplest sense).</p>
<p>In lake management, we frequently run across these kinds of applications when we look at <strong>bathymetric maps</strong> and <strong>isopleths</strong>.</p>
<p>Here, we will investigate some basic tools for visualizing response surfaces, and specifically, we will create some isopleths.</p>
<p>For this example, we will go back to the <code>otsego</code> data that we used in the morning session and start by making an isopleth of <code>temp</code> for a single year. In this crowd, the concept probably requires less explanation than any of the statistics we’ve used so far, but basically we want to come up with a 3-dimensional way to visualize changes in temperature across time (days) and space (depth).</p>
<p>Let’s read in the data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">otsego &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&#39;physical.csv&#39;</span>)</code></pre></div>
<p>One of the first things that we are going to have to deal with is the formatting of dates in this data set. Right now, R sees the column <code>date</code> as a <strong>factor</strong> with 539 <strong>levels</strong>. To do this, we will need to first convert <code>date</code> to <code>character</code>, and then to <code>Date</code> by specifying the format.</p>
<p>Converting from factor/character to Date is probably one of the most frustrating aspects of working in R early on, so take your time and make sure you understand what this code is doing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Data formatting &amp; extraction</span>

<span class="co"># First, we convert the date column</span>
<span class="co"># to a character string. We pass the</span>
<span class="co"># result directly to the as.Date </span>
<span class="co"># function, and along with that we</span>
<span class="co"># specify a format so R knows where it</span>
<span class="co"># is looking for specific elements of</span>
<span class="co"># the date info we are trying to pass.</span>
otsego<span class="op">$</span>date &lt;-<span class="st"> </span><span class="kw">as.Date</span>(
  <span class="kw">as.character</span>(otsego<span class="op">$</span>date),
  <span class="dt">format=</span><span class="st">&quot;%m/%d/%Y&quot;</span>
  )</code></pre></div>
<p>Next, we <em>could</em> optionally subset the data, retaining only those observations for a given year. But, we are better off just using all of the data, and then zooming in on our contour plot by setting the x-axis (date) to the time period we desire (see below).</p>
<p>Then, we will omit any of the <code>NA</code> values in our data to make the next part a little easier to do.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Remove NA values to make life easier</span>
lim &lt;-<span class="st"> </span><span class="kw">na.omit</span>(otsego)</code></pre></div>
<p>Finally, we just do a little math trick to reverse the order of depths for interpolation and plotting in order to make the surface of the water show up at the top of our graphs. We will undo this when we label the graph.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Multiply depth column by -1 so depth will</span>
<span class="co"># plot from top to bottom.</span>
lim<span class="op">$</span>depth =<span class="st"> </span><span class="op">-</span><span class="dv">1</span> <span class="op">*</span>lim<span class="op">$</span>depth</code></pre></div>
<p>Now, we are ready to do the math behind the isopleth and plot it.</p>
<p>To do the math, we need to load the <code>akima</code> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(akima, <span class="dt">lib.loc =</span> <span class="st">&#39;r_libs/&#39;</span>)</code></pre></div>
<p>Next, we interpolate <code>temp</code> across <code>date</code> and <code>depth</code>. The interpolation we are using is basically just a bunch of linear regresions to predict <code>temp</code> for values of <code>date</code> and <code>temp</code> across a regular grid.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a data frame containing the</span>
<span class="co"># x, y, and z variables of interest</span>
plotter =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>lim<span class="op">$</span>date, <span class="dt">y=</span>lim<span class="op">$</span>depth, <span class="dt">z=</span>lim<span class="op">$</span>temp)

<span class="co"># Sort it so we have ascending values of x and y</span>
plotter =<span class="st"> </span>plotter[<span class="kw">with</span>(plotter, <span class="kw">order</span>(x, y)), ]

<span class="co"># Make a regularly spaced x, y, z grid using</span>
<span class="co"># linear interpolation from the akima package</span>
im =<span class="st"> </span><span class="kw">with</span>(plotter,
          <span class="kw">interp</span>(x, y, z, <span class="dt">duplicate=</span><span class="st">&#39;mean&#39;</span>,
                 <span class="dt">nx=</span><span class="kw">length</span>(<span class="kw">unique</span>(lim<span class="op">$</span>date)),
                 <span class="dt">ny=</span><span class="kw">length</span>(<span class="kw">unique</span>(lim<span class="op">$</span>depth)))
          )</code></pre></div>
<p>To wrap it all up, we can make the plot. This looks like a lot of code, but it is mostly comments. For this plot, we will zoom in on year 2017 by limiting the range of the x-axis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the isopleth</span>
<span class="co"># filled.contour is the function that actually </span>
<span class="co"># makes the contour plot. This is the same function</span>
<span class="co"># that is used in the wtr.heat.map function in the</span>
<span class="co"># RLakeAnalyzer package, but it is executed from</span>
<span class="co"># within a convenience wrapper there, so it is </span>
<span class="co"># hard to customize. </span>

<span class="co"># I tend to work with the filled.contour</span>
<span class="co"># function from the graphics package (included</span>
<span class="co"># in base R and loaded by default). This is</span>
<span class="co"># just a preference driven by need for</span>
<span class="co"># more flexibility.</span>

<span class="co"># Set up plotting window margins</span>
<span class="kw">par</span>(<span class="dt">mar=</span><span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">8</span>)) 

<span class="co"># Make the graph</span>
<span class="kw">filled.contour</span>(
  im<span class="op">$</span>x, <span class="co"># Variable on x-axis (date)</span>
  im<span class="op">$</span>y, <span class="co"># Variable on y-axis (depth)</span>
  im<span class="op">$</span>z, <span class="co"># Response (wq parameter) </span>
  <span class="co"># Could also choose &#39;grey.colors&#39; or &#39;terrain.colors&#39;.</span>
  <span class="co"># If you want the ramp to go the other way,</span>
  <span class="co"># just delete the &#39;rev&#39;. Note that you will</span>
  <span class="co"># need to change the 26 in parentheses to match</span>
  <span class="co"># the number of levels that you actually have or</span>
  <span class="co"># want to display.</span>
  <span class="dt">col=</span><span class="kw">topo.colors</span>(<span class="dv">26</span>),  
  <span class="co"># I don&#39;t like in-figure titles.</span>
  <span class="co"># You can add one, though. You will, however,</span>
  <span class="co"># need to change the &#39;mar&#39; argument in the call</span>
  <span class="co"># to par above.</span>
  <span class="dt">main =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&#39;Temperature (&#39;</span>, degree, <span class="st">&#39;C)&#39;</span>)),
  <span class="co"># Specify y-axis limits.</span>
  <span class="dt">ylim=</span><span class="kw">c</span>(<span class="kw">min</span>(im<span class="op">$</span>y), <span class="kw">max</span>(im<span class="op">$</span>y)),
  <span class="co"># Specify x-axis limits. In</span>
  <span class="co"># this case, we are &quot;zooming in&quot;</span>
  <span class="co"># on year 2017</span>
  <span class="dt">xlim=</span><span class="kw">c</span>(<span class="kw">as.Date</span>(<span class="st">&#39;2017/05/01&#39;</span>), <span class="kw">max</span>(im<span class="op">$</span>x)), 
  <span class="co"># X-axis label</span>
  <span class="dt">xlab=</span><span class="st">&#39;Date&#39;</span>, 
  <span class="co"># Y-axis label</span>
  <span class="dt">ylab=</span><span class="st">&#39;Depth (m)&#39;</span>,
  <span class="co"># Axis options</span>
  <span class="dt">plot.axes =</span> {  
        <span class="co"># This is how we include</span>
        <span class="co"># countour lines</span>
        <span class="kw">contour</span>(                            
        im<span class="op">$</span>x,                             
        im<span class="op">$</span>y,                             
        im<span class="op">$</span>z,                             
        <span class="dt">nlevels =</span> <span class="dv">26</span>,                     
        <span class="dt">drawlabels =</span> <span class="ot">FALSE</span>,               
        <span class="dt">col =</span> <span class="kw">topo.colors</span>(<span class="dv">26</span>),
        <span class="dt">lwd =</span> <span class="dv">1</span>,                          
        <span class="dt">lty =</span> <span class="dv">2</span>,                          
        <span class="dt">add =</span> <span class="ot">TRUE</span>
        )
    <span class="co"># Y-axis</span>
    <span class="kw">axis</span>(<span class="dv">2</span>, <span class="dt">at=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="op">-</span><span class="dv">50</span>,<span class="op">-</span><span class="dv">10</span>),
         <span class="dt">labels=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">50</span>,<span class="dv">10</span>)
    )
    <span class="co"># X-axis</span>
    <span class="kw">axis</span>(<span class="dv">1</span>,
         <span class="dt">at=</span><span class="kw">seq</span>(<span class="kw">as.Date</span>(<span class="st">&quot;2017/05/01&quot;</span>),
                <span class="dt">by=</span><span class="st">&quot;2 months&quot;</span>,
                <span class="dt">length.out=</span><span class="dv">16</span>
                ), 
         <span class="dt">labels =</span> <span class="kw">format</span>(
           <span class="kw">seq</span>(<span class="kw">as.Date</span>(<span class="st">&quot;2017/05/01&quot;</span>),
               <span class="dt">by=</span><span class="st">&quot;2 months&quot;</span>,
               <span class="dt">length.out=</span><span class="dv">16</span>
               ),
           <span class="st">&quot;%b %Y&quot;</span>
           )
    )
  }                                     
)             </code></pre></div>
<p><img src="intermediate_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<p>One of the things we have to be careful about with these graphs is that they are models, and if we are missing data at critical time periods, then our models are only as good as our data allow. This can be seen in the plot above. There is a gap in info from late November 2017 when the lake starts to turnover and conditions got rough until early January, when safe ice had set up for winter sampling. The result, in this case, is the apparent loss of the 4 C layer of water in the lake.</p>
<p>Have a go at these isopleths again, substituting DO or pH. You should find that the only code that needs to be changed is the variable used for <code>z</code> when you create the <code>plotter</code> data above, and the title of the plot.</p>
<p>If you are struggling, have a look at these cool isopleths for <a href="pH.html"> pH </a> and <a href="oxygen.html"> dissolved oxygen </a> to get a better idea of the workflow.</p>
</div>

<!DOCTYPE html>
<p>Copyright &copy; 2018 Dan Stich. All rights reserved.</p>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
